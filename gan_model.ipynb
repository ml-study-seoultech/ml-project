{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] [Batch 1/7401] [D loss: 0.7038] [G loss: 0.8415]\n",
      "[Epoch 1/5] [Batch 2/7401] [D loss: 0.3768] [G loss: 2.6877]\n",
      "[Epoch 1/5] [Batch 3/7401] [D loss: 1.4631] [G loss: 0.0704]\n",
      "[Epoch 1/5] [Batch 4/7401] [D loss: 0.2719] [G loss: 14.4704]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 191\u001b[0m\n\u001b[1;32m    188\u001b[0m pred_fake \u001b[38;5;241m=\u001b[39m discriminator(gen_imgs)\n\u001b[1;32m    189\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m criterion(pred_fake, valid)\n\u001b[0;32m--> 191\u001b[0m \u001b[43mg_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m#  Train Discriminator\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4  # 큰 이미지를 처리하므로 배치 크기를 줄였습니다.\n",
    "lr = 0.0002\n",
    "num_epochs = 5\n",
    "img_size = 512  # 이미지 크기를 512로 설정\n",
    "channels = 1  # 흑백 이미지일 경우 1, 컬러 이미지일 경우 3\n",
    "img_shape = (channels, img_size, img_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((img_size, img_size)),  # 이미지 크기를 512x512로 조정\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_dir, gt_dir, transform=None):\n",
    "        self.input_dir = input_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.input_images = sorted(os.listdir(input_dir))\n",
    "        self.gt_images = sorted(os.listdir(gt_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = Image.open(\n",
    "            os.path.join(self.input_dir, self.input_images[idx])\n",
    "        ).convert(\"L\")\n",
    "        gt_image = Image.open(os.path.join(self.gt_dir, self.gt_images[idx])).convert(\n",
    "            \"L\"\n",
    "        )\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            gt_image = self.transform(gt_image)\n",
    "\n",
    "        return input_image, gt_image\n",
    "\n",
    "\n",
    "# Directories\n",
    "input_dir = \"./train_input\"\n",
    "gt_dir = \"./train_gt\"\n",
    "\n",
    "# Data loading\n",
    "custom_dataset = CustomDataset(input_dir, gt_dir, transform=transform)\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def down_block(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, normalize=True\n",
    "        ):\n",
    "            layers = [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            ]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        def up_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "            layers = [\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels, out_channels, kernel_size, stride, padding\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            return layers\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *down_block(\n",
    "                channels, 64, 4, 2, 1, normalize=False\n",
    "            ),  # [batch, 64, 256, 256]\n",
    "            *down_block(64, 128, 4, 2, 1),  # [batch, 128, 128, 128]\n",
    "            *down_block(128, 256, 4, 2, 1),  # [batch, 256, 64, 64]\n",
    "            *down_block(256, 512, 4, 2, 1),  # [batch, 512, 32, 32]\n",
    "            *down_block(512, 512, 4, 2, 1),  # [batch, 512, 16, 16]\n",
    "            *down_block(512, 512, 4, 2, 1),  # [batch, 512, 8, 8]\n",
    "            *down_block(512, 512, 4, 2, 1),  # [batch, 512, 4, 4]\n",
    "            *down_block(512, 512, 4, 2, 1),  # [batch, 512, 2, 2]\n",
    "            *down_block(512, 512, 4, 2, 1),  # [batch, 512, 1, 1]\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            *up_block(512, 512, 4, 2, 1),  # [batch, 512, 2, 2]\n",
    "            *up_block(512, 512, 4, 2, 1),  # [batch, 512, 4, 4]\n",
    "            *up_block(512, 512, 4, 2, 1),  # [batch, 512, 8, 8]\n",
    "            *up_block(512, 512, 4, 2, 1),  # [batch, 512, 16, 16]\n",
    "            *up_block(512, 512, 4, 2, 1),  # [batch, 512, 32, 32]\n",
    "            *up_block(512, 256, 4, 2, 1),  # [batch, 256, 64, 64]\n",
    "            *up_block(256, 128, 4, 2, 1),  # [batch, 128, 128, 128]\n",
    "            *up_block(128, 64, 4, 2, 1),  # [batch, 64, 256, 256]\n",
    "            nn.ConvTranspose2d(64, channels, 4, 2, 1),  # [batch, channels, 512, 512]\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def block(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, normalize=True\n",
    "        ):\n",
    "            layers = [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            ]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(channels, 64, 4, 2, 1, normalize=False),  # [batch, 64, 256, 256]\n",
    "            *block(64, 128, 4, 2, 1),  # [batch, 128, 128, 128]\n",
    "            *block(128, 256, 4, 2, 1),  # [batch, 256, 64, 64]\n",
    "            *block(256, 512, 4, 2, 1),  # [batch, 512, 32, 32]\n",
    "            *block(512, 512, 4, 2, 1),  # [batch, 512, 16, 16]\n",
    "            *block(512, 512, 4, 2, 1),  # [batch, 512, 8, 8]\n",
    "            *block(512, 512, 4, 2, 1),  # [batch, 512, 4, 4]\n",
    "            nn.Conv2d(512, 1, 4, 1, 0),  # [batch, 1, 1, 1]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(-1, 1)\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_imgs, gt_imgs) in enumerate(custom_dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(input_imgs.size(0), 1, requires_grad=False).to(device)\n",
    "        fake = torch.zeros(input_imgs.size(0), 1, requires_grad=False).to(device)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = gt_imgs.to(device)\n",
    "        input_imgs = input_imgs.to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(input_imgs)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        pred_fake = discriminator(gen_imgs)\n",
    "        g_loss = criterion(pred_fake, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real images\n",
    "        pred_real = discriminator(real_imgs)\n",
    "        real_loss = criterion(pred_real, valid)\n",
    "\n",
    "        # Fake images\n",
    "        pred_fake = discriminator(gen_imgs.detach())\n",
    "        fake_loss = criterion(pred_fake, fake)\n",
    "\n",
    "        # Total loss\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch+1}/{num_epochs}] [Batch {i+1}/{len(custom_dataloader)}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=1024, out_features=784, bias=True)\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Dropout(p=0.3, inplace=False)\n",
      "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Generator model\n",
    "class Generator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Generator, self).__init__()\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(100, 256),\n",
    "      nn.LeakyReLU(0.2),\n",
    "      nn.BatchNorm1d(256),\n",
    "      nn.Linear(256, 512),\n",
    "      nn.LeakyReLU(0.2),\n",
    "      nn.BatchNorm1d(512),\n",
    "      nn.Linear(512, 1024),\n",
    "      nn.LeakyReLU(0.2),\n",
    "      nn.BatchNorm1d(1024),\n",
    "      nn.Linear(1024, 784),\n",
    "      nn.Tanh()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "# Example usage\n",
    "generator = Generator()\n",
    "print(generator)\n",
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(784, 1024),\n",
    "      nn.LeakyReLU(0.2),\n",
    "      nn.Dropout(0.3),\n",
    "      nn.Linear(1024, 512),\n",
    "      nn.LeakyReLU(0.2),\n",
    "      nn.Dropout(0.3),\n",
    "      nn.Linear(512, 256),\n",
    "      nn.LeakyReLU(0.2),\n",
    "      nn.Dropout(0.3),\n",
    "      nn.Linear(256, 1),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "# Example usage\n",
    "discriminator = Discriminator()\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x4194304 and 3136x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m generator(input_imgs)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Loss measures generator's ability to fool the discriminator\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m criterion(\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_imgs\u001b[49m\u001b[43m)\u001b[49m, valid)\n\u001b[1;32m    125\u001b[0m g_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    126\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 88\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x4194304 and 3136x1)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "num_epochs = 5\n",
    "img_shape = (1, 28, 28)  # 이미지 크기에 맞게 수정 필요\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(),  # 이미지가 컬러인 경우 추가\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_dir, gt_dir, transform=None):\n",
    "        self.input_dir = input_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.input_images = sorted(os.listdir(input_dir))\n",
    "        self.gt_images = sorted(os.listdir(gt_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = Image.open(os.path.join(self.input_dir, self.input_images[idx]))\n",
    "        gt_image = Image.open(os.path.join(self.gt_dir, self.gt_images[idx]))\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            gt_image = self.transform(gt_image)\n",
    "\n",
    "        return input_image, gt_image\n",
    "\n",
    "\n",
    "# Directories\n",
    "input_dir = \"./train_input\"\n",
    "gt_dir = \"./train_gt\"\n",
    "\n",
    "# Data loading\n",
    "custom_dataset = CustomDataset(input_dir, gt_dir, transform=transform)\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 추가 레이어들...\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 추가 레이어들...\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_imgs, gt_imgs) in enumerate(custom_dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((input_imgs.size(0), 1), requires_grad=False).to(device)\n",
    "        fake = torch.zeros((input_imgs.size(0), 1), requires_grad=False).to(device)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = gt_imgs.to(device)\n",
    "        input_imgs = input_imgs.to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(input_imgs)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(custom_dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 0/938] [D loss: 0.4207763671875] [G loss: 0.9376246333122253]\n",
      "[Epoch 0/5] [Batch 1/938] [D loss: 1.0952956676483154] [G loss: 3.9182088375091553]\n",
      "[Epoch 0/5] [Batch 2/938] [D loss: 0.4105992913246155] [G loss: 1.275123119354248]\n",
      "[Epoch 0/5] [Batch 3/938] [D loss: 0.6782832145690918] [G loss: 0.3810550570487976]\n",
      "[Epoch 0/5] [Batch 4/938] [D loss: 0.4412800073623657] [G loss: 0.9696307182312012]\n",
      "[Epoch 0/5] [Batch 5/938] [D loss: 0.5038331151008606] [G loss: 1.936354398727417]\n",
      "[Epoch 0/5] [Batch 6/938] [D loss: 0.4852534830570221] [G loss: 1.2654916048049927]\n",
      "[Epoch 0/5] [Batch 7/938] [D loss: 0.5434651374816895] [G loss: 0.8343309760093689]\n",
      "[Epoch 0/5] [Batch 8/938] [D loss: 0.4930841624736786] [G loss: 1.0141769647598267]\n",
      "[Epoch 0/5] [Batch 9/938] [D loss: 0.48950207233428955] [G loss: 1.2621713876724243]\n",
      "[Epoch 0/5] [Batch 10/938] [D loss: 0.5283129215240479] [G loss: 0.919470489025116]\n",
      "[Epoch 0/5] [Batch 11/938] [D loss: 0.5259958505630493] [G loss: 1.0776848793029785]\n",
      "[Epoch 0/5] [Batch 12/938] [D loss: 0.5442789196968079] [G loss: 0.9913398623466492]\n",
      "[Epoch 0/5] [Batch 13/938] [D loss: 0.5235835313796997] [G loss: 1.0353939533233643]\n",
      "[Epoch 0/5] [Batch 14/938] [D loss: 0.5268354415893555] [G loss: 1.0405582189559937]\n",
      "[Epoch 0/5] [Batch 15/938] [D loss: 0.5008205771446228] [G loss: 1.1387557983398438]\n",
      "[Epoch 0/5] [Batch 16/938] [D loss: 0.5322349071502686] [G loss: 1.0610638856887817]\n",
      "[Epoch 0/5] [Batch 17/938] [D loss: 0.5179290771484375] [G loss: 0.9804415106773376]\n",
      "[Epoch 0/5] [Batch 18/938] [D loss: 0.5040760040283203] [G loss: 1.1233981847763062]\n",
      "[Epoch 0/5] [Batch 19/938] [D loss: 0.5024791359901428] [G loss: 1.0408942699432373]\n",
      "[Epoch 0/5] [Batch 20/938] [D loss: 0.5463576316833496] [G loss: 1.0964293479919434]\n",
      "[Epoch 0/5] [Batch 21/938] [D loss: 0.4915919303894043] [G loss: 1.1639951467514038]\n",
      "[Epoch 0/5] [Batch 22/938] [D loss: 0.5076063275337219] [G loss: 1.162106990814209]\n",
      "[Epoch 0/5] [Batch 23/938] [D loss: 0.4823681712150574] [G loss: 0.948022723197937]\n",
      "[Epoch 0/5] [Batch 24/938] [D loss: 0.5658929347991943] [G loss: 1.8697278499603271]\n",
      "[Epoch 0/5] [Batch 25/938] [D loss: 1.073341965675354] [G loss: 0.17197975516319275]\n",
      "[Epoch 0/5] [Batch 26/938] [D loss: 0.5344582796096802] [G loss: 1.7299538850784302]\n",
      "[Epoch 0/5] [Batch 27/938] [D loss: 0.4972292184829712] [G loss: 0.9345394372940063]\n",
      "[Epoch 0/5] [Batch 28/938] [D loss: 0.493965208530426] [G loss: 0.9957777261734009]\n",
      "[Epoch 0/5] [Batch 29/938] [D loss: 0.49461206793785095] [G loss: 1.2366890907287598]\n",
      "[Epoch 0/5] [Batch 30/938] [D loss: 0.4818192720413208] [G loss: 0.8916467428207397]\n",
      "[Epoch 0/5] [Batch 31/938] [D loss: 0.5051102042198181] [G loss: 1.378599762916565]\n",
      "[Epoch 0/5] [Batch 32/938] [D loss: 0.5191798210144043] [G loss: 0.762993335723877]\n",
      "[Epoch 0/5] [Batch 33/938] [D loss: 0.44688183069229126] [G loss: 1.4830776453018188]\n",
      "[Epoch 0/5] [Batch 34/938] [D loss: 0.45807135105133057] [G loss: 1.0502851009368896]\n",
      "[Epoch 0/5] [Batch 35/938] [D loss: 0.48130691051483154] [G loss: 1.2748076915740967]\n",
      "[Epoch 0/5] [Batch 36/938] [D loss: 0.5118386745452881] [G loss: 0.984505832195282]\n",
      "[Epoch 0/5] [Batch 37/938] [D loss: 0.5141596794128418] [G loss: 1.7132014036178589]\n",
      "[Epoch 0/5] [Batch 38/938] [D loss: 0.7074941396713257] [G loss: 0.4448213577270508]\n",
      "[Epoch 0/5] [Batch 39/938] [D loss: 0.8408788442611694] [G loss: 2.576420307159424]\n",
      "[Epoch 0/5] [Batch 40/938] [D loss: 0.8532892465591431] [G loss: 0.2550133168697357]\n",
      "[Epoch 0/5] [Batch 41/938] [D loss: 0.5715526938438416] [G loss: 0.6983987092971802]\n",
      "[Epoch 0/5] [Batch 42/938] [D loss: 0.7183858752250671] [G loss: 2.398008108139038]\n",
      "[Epoch 0/5] [Batch 43/938] [D loss: 0.5377673506736755] [G loss: 0.8708940148353577]\n",
      "[Epoch 0/5] [Batch 44/938] [D loss: 0.5465995073318481] [G loss: 0.663877546787262]\n",
      "[Epoch 0/5] [Batch 45/938] [D loss: 0.49444904923439026] [G loss: 1.1334631443023682]\n",
      "[Epoch 0/5] [Batch 46/938] [D loss: 0.5166323781013489] [G loss: 1.329205870628357]\n",
      "[Epoch 0/5] [Batch 47/938] [D loss: 0.4698421061038971] [G loss: 0.9723259210586548]\n",
      "[Epoch 0/5] [Batch 48/938] [D loss: 0.517722487449646] [G loss: 1.1114819049835205]\n",
      "[Epoch 0/5] [Batch 49/938] [D loss: 0.48945820331573486] [G loss: 1.0725183486938477]\n",
      "[Epoch 0/5] [Batch 50/938] [D loss: 0.44468042254447937] [G loss: 1.0293538570404053]\n",
      "[Epoch 0/5] [Batch 51/938] [D loss: 0.4690433740615845] [G loss: 1.4246186017990112]\n",
      "[Epoch 0/5] [Batch 52/938] [D loss: 0.44621211290359497] [G loss: 1.0631400346755981]\n",
      "[Epoch 0/5] [Batch 53/938] [D loss: 0.49364984035491943] [G loss: 1.3130983114242554]\n",
      "[Epoch 0/5] [Batch 54/938] [D loss: 0.5040178894996643] [G loss: 0.8441703915596008]\n",
      "[Epoch 0/5] [Batch 55/938] [D loss: 0.5541989803314209] [G loss: 1.8873684406280518]\n",
      "[Epoch 0/5] [Batch 56/938] [D loss: 0.6658701300621033] [G loss: 0.46621841192245483]\n",
      "[Epoch 0/5] [Batch 57/938] [D loss: 0.5107743740081787] [G loss: 1.4822778701782227]\n",
      "[Epoch 0/5] [Batch 58/938] [D loss: 0.46263688802719116] [G loss: 1.1034493446350098]\n",
      "[Epoch 0/5] [Batch 59/938] [D loss: 0.47707849740982056] [G loss: 1.1008589267730713]\n",
      "[Epoch 0/5] [Batch 60/938] [D loss: 0.5543336868286133] [G loss: 1.1955018043518066]\n",
      "[Epoch 0/5] [Batch 61/938] [D loss: 0.5021350979804993] [G loss: 0.8238389492034912]\n",
      "[Epoch 0/5] [Batch 62/938] [D loss: 0.5390552282333374] [G loss: 1.4616237878799438]\n",
      "[Epoch 0/5] [Batch 63/938] [D loss: 0.458525687456131] [G loss: 0.8738652467727661]\n",
      "[Epoch 0/5] [Batch 64/938] [D loss: 0.4417693614959717] [G loss: 1.3661437034606934]\n",
      "[Epoch 0/5] [Batch 65/938] [D loss: 0.4973633885383606] [G loss: 1.1443983316421509]\n",
      "[Epoch 0/5] [Batch 66/938] [D loss: 0.5269920825958252] [G loss: 1.2059215307235718]\n",
      "[Epoch 0/5] [Batch 67/938] [D loss: 0.5061405897140503] [G loss: 1.2156436443328857]\n",
      "[Epoch 0/5] [Batch 68/938] [D loss: 0.45959407091140747] [G loss: 1.2237194776535034]\n",
      "[Epoch 0/5] [Batch 69/938] [D loss: 0.519502580165863] [G loss: 1.4299192428588867]\n",
      "[Epoch 0/5] [Batch 70/938] [D loss: 0.5869154930114746] [G loss: 0.600342869758606]\n",
      "[Epoch 0/5] [Batch 71/938] [D loss: 0.7328964471817017] [G loss: 2.29113507270813]\n",
      "[Epoch 0/5] [Batch 72/938] [D loss: 0.8502240777015686] [G loss: 0.2783741056919098]\n",
      "[Epoch 0/5] [Batch 73/938] [D loss: 0.4968603849411011] [G loss: 1.0345563888549805]\n",
      "[Epoch 0/5] [Batch 74/938] [D loss: 0.6074543595314026] [G loss: 1.7584795951843262]\n",
      "[Epoch 0/5] [Batch 75/938] [D loss: 0.5800684690475464] [G loss: 0.6293596029281616]\n",
      "[Epoch 0/5] [Batch 76/938] [D loss: 0.5050367116928101] [G loss: 1.11616051197052]\n",
      "[Epoch 0/5] [Batch 77/938] [D loss: 0.5080287456512451] [G loss: 1.6065342426300049]\n",
      "[Epoch 0/5] [Batch 78/938] [D loss: 0.4773179888725281] [G loss: 0.9327595233917236]\n",
      "[Epoch 0/5] [Batch 79/938] [D loss: 0.44018733501434326] [G loss: 1.1620882749557495]\n",
      "[Epoch 0/5] [Batch 80/938] [D loss: 0.5222883224487305] [G loss: 1.2124136686325073]\n",
      "[Epoch 0/5] [Batch 81/938] [D loss: 0.49313750863075256] [G loss: 0.9307445883750916]\n",
      "[Epoch 0/5] [Batch 82/938] [D loss: 0.5597514510154724] [G loss: 1.2103667259216309]\n",
      "[Epoch 0/5] [Batch 83/938] [D loss: 0.5573289394378662] [G loss: 0.8715406656265259]\n",
      "[Epoch 0/5] [Batch 84/938] [D loss: 0.5117636322975159] [G loss: 1.2982031106948853]\n",
      "[Epoch 0/5] [Batch 85/938] [D loss: 0.48696190118789673] [G loss: 0.7775197625160217]\n",
      "[Epoch 0/5] [Batch 86/938] [D loss: 0.5272214412689209] [G loss: 1.5402491092681885]\n",
      "[Epoch 0/5] [Batch 87/938] [D loss: 0.5647549033164978] [G loss: 0.7076996564865112]\n",
      "[Epoch 0/5] [Batch 88/938] [D loss: 0.5908399224281311] [G loss: 1.8876312971115112]\n",
      "[Epoch 0/5] [Batch 89/938] [D loss: 0.6111338138580322] [G loss: 0.5274198055267334]\n",
      "[Epoch 0/5] [Batch 90/938] [D loss: 0.5049015283584595] [G loss: 1.6535630226135254]\n",
      "[Epoch 0/5] [Batch 91/938] [D loss: 0.4909684658050537] [G loss: 0.7844505906105042]\n",
      "[Epoch 0/5] [Batch 92/938] [D loss: 0.5257519483566284] [G loss: 1.386045217514038]\n",
      "[Epoch 0/5] [Batch 93/938] [D loss: 0.5133005380630493] [G loss: 0.8809557557106018]\n",
      "[Epoch 0/5] [Batch 94/938] [D loss: 0.5062492489814758] [G loss: 1.1240849494934082]\n",
      "[Epoch 0/5] [Batch 95/938] [D loss: 0.5378128886222839] [G loss: 1.159099817276001]\n",
      "[Epoch 0/5] [Batch 96/938] [D loss: 0.5263098478317261] [G loss: 0.9975951910018921]\n",
      "[Epoch 0/5] [Batch 97/938] [D loss: 0.5139177441596985] [G loss: 1.503485083580017]\n",
      "[Epoch 0/5] [Batch 98/938] [D loss: 0.5577260255813599] [G loss: 0.8075974583625793]\n",
      "[Epoch 0/5] [Batch 99/938] [D loss: 0.5221797227859497] [G loss: 1.6371667385101318]\n",
      "[Epoch 0/5] [Batch 100/938] [D loss: 0.5830408334732056] [G loss: 0.6009008884429932]\n",
      "[Epoch 0/5] [Batch 101/938] [D loss: 0.6358759999275208] [G loss: 2.175952196121216]\n",
      "[Epoch 0/5] [Batch 102/938] [D loss: 0.6896858811378479] [G loss: 0.38907790184020996]\n",
      "[Epoch 0/5] [Batch 103/938] [D loss: 0.5810350179672241] [G loss: 1.8516404628753662]\n",
      "[Epoch 0/5] [Batch 104/938] [D loss: 0.48747316002845764] [G loss: 0.9003933668136597]\n",
      "[Epoch 0/5] [Batch 105/938] [D loss: 0.49153363704681396] [G loss: 0.857547402381897]\n",
      "[Epoch 0/5] [Batch 106/938] [D loss: 0.5190296173095703] [G loss: 1.4117921590805054]\n",
      "[Epoch 0/5] [Batch 107/938] [D loss: 0.5091050863265991] [G loss: 1.0064101219177246]\n",
      "[Epoch 0/5] [Batch 108/938] [D loss: 0.5289900302886963] [G loss: 0.8589390516281128]\n",
      "[Epoch 0/5] [Batch 109/938] [D loss: 0.5216518044471741] [G loss: 1.4134904146194458]\n",
      "[Epoch 0/5] [Batch 110/938] [D loss: 0.5259684324264526] [G loss: 0.7912549376487732]\n",
      "[Epoch 0/5] [Batch 111/938] [D loss: 0.5324386954307556] [G loss: 1.6281521320343018]\n",
      "[Epoch 0/5] [Batch 112/938] [D loss: 0.5406143665313721] [G loss: 0.8437321186065674]\n",
      "[Epoch 0/5] [Batch 113/938] [D loss: 0.46873635053634644] [G loss: 1.259398341178894]\n",
      "[Epoch 0/5] [Batch 114/938] [D loss: 0.4555022120475769] [G loss: 1.413830041885376]\n",
      "[Epoch 0/5] [Batch 115/938] [D loss: 0.4759642779827118] [G loss: 0.9973597526550293]\n",
      "[Epoch 0/5] [Batch 116/938] [D loss: 0.49547678232192993] [G loss: 1.430658221244812]\n",
      "[Epoch 0/5] [Batch 117/938] [D loss: 0.5211409330368042] [G loss: 0.8176183700561523]\n",
      "[Epoch 0/5] [Batch 118/938] [D loss: 0.6277755498886108] [G loss: 1.9167885780334473]\n",
      "[Epoch 0/5] [Batch 119/938] [D loss: 1.0307502746582031] [G loss: 0.18084615468978882]\n",
      "[Epoch 0/5] [Batch 120/938] [D loss: 0.5442078709602356] [G loss: 1.7627975940704346]\n",
      "[Epoch 0/5] [Batch 121/938] [D loss: 0.493268758058548] [G loss: 1.145403504371643]\n",
      "[Epoch 0/5] [Batch 122/938] [D loss: 0.5044938325881958] [G loss: 0.9623112082481384]\n",
      "[Epoch 0/5] [Batch 123/938] [D loss: 0.5056599974632263] [G loss: 1.3122094869613647]\n",
      "[Epoch 0/5] [Batch 124/938] [D loss: 0.5123913288116455] [G loss: 0.8507550954818726]\n",
      "[Epoch 0/5] [Batch 125/938] [D loss: 0.5337185859680176] [G loss: 1.2913432121276855]\n",
      "[Epoch 0/5] [Batch 126/938] [D loss: 0.5425844788551331] [G loss: 0.7417274713516235]\n",
      "[Epoch 0/5] [Batch 127/938] [D loss: 0.5612406134605408] [G loss: 1.7538366317749023]\n",
      "[Epoch 0/5] [Batch 128/938] [D loss: 0.5706664323806763] [G loss: 0.6290197968482971]\n",
      "[Epoch 0/5] [Batch 129/938] [D loss: 0.4684816598892212] [G loss: 1.3956191539764404]\n",
      "[Epoch 0/5] [Batch 130/938] [D loss: 0.5143283605575562] [G loss: 1.0858421325683594]\n",
      "[Epoch 0/5] [Batch 131/938] [D loss: 0.5155789852142334] [G loss: 0.9084309935569763]\n",
      "[Epoch 0/5] [Batch 132/938] [D loss: 0.6437848806381226] [G loss: 1.9823191165924072]\n",
      "[Epoch 0/5] [Batch 133/938] [D loss: 0.790233850479126] [G loss: 0.36617082357406616]\n",
      "[Epoch 0/5] [Batch 134/938] [D loss: 0.5498612523078918] [G loss: 1.6893234252929688]\n",
      "[Epoch 0/5] [Batch 135/938] [D loss: 0.49800634384155273] [G loss: 1.0755866765975952]\n",
      "[Epoch 0/5] [Batch 136/938] [D loss: 0.4797596335411072] [G loss: 0.937812089920044]\n",
      "[Epoch 0/5] [Batch 137/938] [D loss: 0.5713900923728943] [G loss: 1.5450665950775146]\n",
      "[Epoch 0/5] [Batch 138/938] [D loss: 0.5512001514434814] [G loss: 0.700875461101532]\n",
      "[Epoch 0/5] [Batch 139/938] [D loss: 0.43851155042648315] [G loss: 1.156430959701538]\n",
      "[Epoch 0/5] [Batch 140/938] [D loss: 0.5531615018844604] [G loss: 1.2539454698562622]\n",
      "[Epoch 0/5] [Batch 141/938] [D loss: 0.5023701190948486] [G loss: 0.8556957244873047]\n",
      "[Epoch 0/5] [Batch 142/938] [D loss: 0.4816269278526306] [G loss: 1.2303980588912964]\n",
      "[Epoch 0/5] [Batch 143/938] [D loss: 0.47945165634155273] [G loss: 0.854078471660614]\n",
      "[Epoch 0/5] [Batch 144/938] [D loss: 0.5531448721885681] [G loss: 1.6139932870864868]\n",
      "[Epoch 0/5] [Batch 145/938] [D loss: 0.6607407927513123] [G loss: 0.5215757489204407]\n",
      "[Epoch 0/5] [Batch 146/938] [D loss: 0.6197924613952637] [G loss: 1.8796907663345337]\n",
      "[Epoch 0/5] [Batch 147/938] [D loss: 0.6053923964500427] [G loss: 0.5557249188423157]\n",
      "[Epoch 0/5] [Batch 148/938] [D loss: 0.5084119439125061] [G loss: 1.4178991317749023]\n",
      "[Epoch 0/5] [Batch 149/938] [D loss: 0.5066249370574951] [G loss: 1.1016544103622437]\n",
      "[Epoch 0/5] [Batch 150/938] [D loss: 0.49279481172561646] [G loss: 0.8823838233947754]\n",
      "[Epoch 0/5] [Batch 151/938] [D loss: 0.49282214045524597] [G loss: 1.5965497493743896]\n",
      "[Epoch 0/5] [Batch 152/938] [D loss: 0.4978731870651245] [G loss: 0.8550153374671936]\n",
      "[Epoch 0/5] [Batch 153/938] [D loss: 0.4962780773639679] [G loss: 1.2631218433380127]\n",
      "[Epoch 0/5] [Batch 154/938] [D loss: 0.4443665146827698] [G loss: 1.1197538375854492]\n",
      "[Epoch 0/5] [Batch 155/938] [D loss: 0.41810712218284607] [G loss: 1.4002763032913208]\n",
      "[Epoch 0/5] [Batch 156/938] [D loss: 0.5445938110351562] [G loss: 0.950711190700531]\n",
      "[Epoch 0/5] [Batch 157/938] [D loss: 0.5042305588722229] [G loss: 1.679466724395752]\n",
      "[Epoch 0/5] [Batch 158/938] [D loss: 0.632535457611084] [G loss: 0.6252508759498596]\n",
      "[Epoch 0/5] [Batch 159/938] [D loss: 0.9671698808670044] [G loss: 2.717282772064209]\n",
      "[Epoch 0/5] [Batch 160/938] [D loss: 0.953564465045929] [G loss: 0.21537698805332184]\n",
      "[Epoch 0/5] [Batch 161/938] [D loss: 0.5240148305892944] [G loss: 0.7287924289703369]\n",
      "[Epoch 0/5] [Batch 162/938] [D loss: 0.6684541702270508] [G loss: 1.9262149333953857]\n",
      "[Epoch 0/5] [Batch 163/938] [D loss: 0.4843524694442749] [G loss: 1.0026116371154785]\n",
      "[Epoch 0/5] [Batch 164/938] [D loss: 0.5608578324317932] [G loss: 0.6947289109230042]\n",
      "[Epoch 0/5] [Batch 165/938] [D loss: 0.5538602471351624] [G loss: 1.1870253086090088]\n",
      "[Epoch 0/5] [Batch 166/938] [D loss: 0.5045872926712036] [G loss: 1.1751842498779297]\n",
      "[Epoch 0/5] [Batch 167/938] [D loss: 0.4952755570411682] [G loss: 1.0442918539047241]\n",
      "[Epoch 0/5] [Batch 168/938] [D loss: 0.4743024706840515] [G loss: 1.0442190170288086]\n",
      "[Epoch 0/5] [Batch 169/938] [D loss: 0.4817977845668793] [G loss: 1.3894062042236328]\n",
      "[Epoch 0/5] [Batch 170/938] [D loss: 0.4947161078453064] [G loss: 1.1094385385513306]\n",
      "[Epoch 0/5] [Batch 171/938] [D loss: 0.5214928388595581] [G loss: 0.9343141913414001]\n",
      "[Epoch 0/5] [Batch 172/938] [D loss: 0.4434438645839691] [G loss: 1.2570465803146362]\n",
      "[Epoch 0/5] [Batch 173/938] [D loss: 0.47402089834213257] [G loss: 1.2040953636169434]\n",
      "[Epoch 0/5] [Batch 174/938] [D loss: 0.49777108430862427] [G loss: 1.4088517427444458]\n",
      "[Epoch 0/5] [Batch 175/938] [D loss: 0.4778962731361389] [G loss: 0.7996217608451843]\n",
      "[Epoch 0/5] [Batch 176/938] [D loss: 0.7204978466033936] [G loss: 2.232006072998047]\n",
      "[Epoch 0/5] [Batch 177/938] [D loss: 0.8525464534759521] [G loss: 0.30451762676239014]\n",
      "[Epoch 0/5] [Batch 178/938] [D loss: 0.4952704906463623] [G loss: 1.3078075647354126]\n",
      "[Epoch 0/5] [Batch 179/938] [D loss: 0.5469024181365967] [G loss: 1.6790168285369873]\n",
      "[Epoch 0/5] [Batch 180/938] [D loss: 0.5996411442756653] [G loss: 0.6406965851783752]\n",
      "[Epoch 0/5] [Batch 181/938] [D loss: 0.503963828086853] [G loss: 0.9739783406257629]\n",
      "[Epoch 0/5] [Batch 182/938] [D loss: 0.5169932842254639] [G loss: 1.685193657875061]\n",
      "[Epoch 0/5] [Batch 183/938] [D loss: 0.5302479267120361] [G loss: 0.6927928924560547]\n",
      "[Epoch 0/5] [Batch 184/938] [D loss: 0.47198382019996643] [G loss: 1.022216796875]\n",
      "[Epoch 0/5] [Batch 185/938] [D loss: 0.5217783451080322] [G loss: 1.4347820281982422]\n",
      "[Epoch 0/5] [Batch 186/938] [D loss: 0.5241219401359558] [G loss: 0.9283650517463684]\n",
      "[Epoch 0/5] [Batch 187/938] [D loss: 0.5010894536972046] [G loss: 1.1948726177215576]\n",
      "[Epoch 0/5] [Batch 188/938] [D loss: 0.483630895614624] [G loss: 1.115671992301941]\n",
      "[Epoch 0/5] [Batch 189/938] [D loss: 0.48552244901657104] [G loss: 1.2887306213378906]\n",
      "[Epoch 0/5] [Batch 190/938] [D loss: 0.4514281749725342] [G loss: 1.0943785905838013]\n",
      "[Epoch 0/5] [Batch 191/938] [D loss: 0.4715965688228607] [G loss: 1.0687322616577148]\n",
      "[Epoch 0/5] [Batch 192/938] [D loss: 0.5078082084655762] [G loss: 1.801436185836792]\n",
      "[Epoch 0/5] [Batch 193/938] [D loss: 0.5966084599494934] [G loss: 0.5430235266685486]\n",
      "[Epoch 0/5] [Batch 194/938] [D loss: 0.513835072517395] [G loss: 1.7773748636245728]\n",
      "[Epoch 0/5] [Batch 195/938] [D loss: 0.5383064150810242] [G loss: 0.941612958908081]\n",
      "[Epoch 0/5] [Batch 196/938] [D loss: 0.44056856632232666] [G loss: 1.2564088106155396]\n",
      "[Epoch 0/5] [Batch 197/938] [D loss: 0.5036531090736389] [G loss: 1.6546975374221802]\n",
      "[Epoch 0/5] [Batch 198/938] [D loss: 0.6480600833892822] [G loss: 0.5175673365592957]\n",
      "[Epoch 0/5] [Batch 199/938] [D loss: 0.550968587398529] [G loss: 1.790999412536621]\n",
      "[Epoch 0/5] [Batch 200/938] [D loss: 0.5322145819664001] [G loss: 0.8310987949371338]\n",
      "[Epoch 0/5] [Batch 201/938] [D loss: 0.4990949034690857] [G loss: 1.3550546169281006]\n",
      "[Epoch 0/5] [Batch 202/938] [D loss: 0.4816627502441406] [G loss: 1.1661473512649536]\n",
      "[Epoch 0/5] [Batch 203/938] [D loss: 0.47500890493392944] [G loss: 1.1499996185302734]\n",
      "[Epoch 0/5] [Batch 204/938] [D loss: 0.4823780059814453] [G loss: 1.1691093444824219]\n",
      "[Epoch 0/5] [Batch 205/938] [D loss: 0.4958072304725647] [G loss: 1.2986398935317993]\n",
      "[Epoch 0/5] [Batch 206/938] [D loss: 0.49204009771347046] [G loss: 0.9259164333343506]\n",
      "[Epoch 0/5] [Batch 207/938] [D loss: 0.5138765573501587] [G loss: 1.7660683393478394]\n",
      "[Epoch 0/5] [Batch 208/938] [D loss: 0.62845778465271] [G loss: 0.5774992108345032]\n",
      "[Epoch 0/5] [Batch 209/938] [D loss: 0.5489559769630432] [G loss: 2.0317184925079346]\n",
      "[Epoch 0/5] [Batch 210/938] [D loss: 0.5036108493804932] [G loss: 0.8064566254615784]\n",
      "[Epoch 0/5] [Batch 211/938] [D loss: 0.4345957636833191] [G loss: 1.35627019405365]\n",
      "[Epoch 0/5] [Batch 212/938] [D loss: 0.5917590260505676] [G loss: 1.7174241542816162]\n",
      "[Epoch 0/5] [Batch 213/938] [D loss: 0.5918514728546143] [G loss: 0.5199242234230042]\n",
      "[Epoch 0/5] [Batch 214/938] [D loss: 0.5030907392501831] [G loss: 1.7567492723464966]\n",
      "[Epoch 0/5] [Batch 215/938] [D loss: 0.4555184543132782] [G loss: 1.0702815055847168]\n",
      "[Epoch 0/5] [Batch 216/938] [D loss: 0.5252212882041931] [G loss: 0.9901511073112488]\n",
      "[Epoch 0/5] [Batch 217/938] [D loss: 0.537149965763092] [G loss: 1.519221305847168]\n",
      "[Epoch 0/5] [Batch 218/938] [D loss: 0.5619136095046997] [G loss: 0.6759315133094788]\n",
      "[Epoch 0/5] [Batch 219/938] [D loss: 0.4951513409614563] [G loss: 1.4200057983398438]\n",
      "[Epoch 0/5] [Batch 220/938] [D loss: 0.45548492670059204] [G loss: 1.1430692672729492]\n",
      "[Epoch 0/5] [Batch 221/938] [D loss: 0.44305098056793213] [G loss: 1.1242120265960693]\n",
      "[Epoch 0/5] [Batch 222/938] [D loss: 0.4662517309188843] [G loss: 1.5111900568008423]\n",
      "[Epoch 0/5] [Batch 223/938] [D loss: 0.46265703439712524] [G loss: 0.7852171659469604]\n",
      "[Epoch 0/5] [Batch 224/938] [D loss: 0.5457718372344971] [G loss: 1.6118018627166748]\n",
      "[Epoch 0/5] [Batch 225/938] [D loss: 0.614997148513794] [G loss: 0.6830905675888062]\n",
      "[Epoch 0/5] [Batch 226/938] [D loss: 0.6001542806625366] [G loss: 1.9385924339294434]\n",
      "[Epoch 0/5] [Batch 227/938] [D loss: 0.556318998336792] [G loss: 0.594409704208374]\n",
      "[Epoch 0/5] [Batch 228/938] [D loss: 0.5991880297660828] [G loss: 1.5206761360168457]\n",
      "[Epoch 0/5] [Batch 229/938] [D loss: 0.4935383200645447] [G loss: 0.9451308250427246]\n",
      "[Epoch 0/5] [Batch 230/938] [D loss: 0.5392904877662659] [G loss: 1.6978323459625244]\n",
      "[Epoch 0/5] [Batch 231/938] [D loss: 0.49464958906173706] [G loss: 0.7283574938774109]\n",
      "[Epoch 0/5] [Batch 232/938] [D loss: 0.5708261728286743] [G loss: 1.4854835271835327]\n",
      "[Epoch 0/5] [Batch 233/938] [D loss: 0.4930790662765503] [G loss: 0.9798641800880432]\n",
      "[Epoch 0/5] [Batch 234/938] [D loss: 0.457427442073822] [G loss: 1.0108281373977661]\n",
      "[Epoch 0/5] [Batch 235/938] [D loss: 0.4377967119216919] [G loss: 1.5598284006118774]\n",
      "[Epoch 0/5] [Batch 236/938] [D loss: 0.5588028430938721] [G loss: 0.8102735877037048]\n",
      "[Epoch 0/5] [Batch 237/938] [D loss: 0.5409075021743774] [G loss: 1.8227462768554688]\n",
      "[Epoch 0/5] [Batch 238/938] [D loss: 0.6158443093299866] [G loss: 0.6582882404327393]\n",
      "[Epoch 0/5] [Batch 239/938] [D loss: 0.562780499458313] [G loss: 1.7655377388000488]\n",
      "[Epoch 0/5] [Batch 240/938] [D loss: 0.6011170148849487] [G loss: 0.8320621848106384]\n",
      "[Epoch 0/5] [Batch 241/938] [D loss: 0.4892238974571228] [G loss: 1.2925832271575928]\n",
      "[Epoch 0/5] [Batch 242/938] [D loss: 0.5634170770645142] [G loss: 1.225860834121704]\n",
      "[Epoch 0/5] [Batch 243/938] [D loss: 0.5110573768615723] [G loss: 0.9370759725570679]\n",
      "[Epoch 0/5] [Batch 244/938] [D loss: 0.5302368998527527] [G loss: 1.085701823234558]\n",
      "[Epoch 0/5] [Batch 245/938] [D loss: 0.5271086692810059] [G loss: 1.5192878246307373]\n",
      "[Epoch 0/5] [Batch 246/938] [D loss: 0.5647480487823486] [G loss: 0.6899427771568298]\n",
      "[Epoch 0/5] [Batch 247/938] [D loss: 0.5247154831886292] [G loss: 1.6530488729476929]\n",
      "[Epoch 0/5] [Batch 248/938] [D loss: 0.5755809545516968] [G loss: 0.7192612886428833]\n",
      "[Epoch 0/5] [Batch 249/938] [D loss: 0.5709290504455566] [G loss: 2.003748655319214]\n",
      "[Epoch 0/5] [Batch 250/938] [D loss: 0.5525755286216736] [G loss: 0.6705323457717896]\n",
      "[Epoch 0/5] [Batch 251/938] [D loss: 0.46949195861816406] [G loss: 1.270666480064392]\n",
      "[Epoch 0/5] [Batch 252/938] [D loss: 0.5514309406280518] [G loss: 1.466015338897705]\n",
      "[Epoch 0/5] [Batch 253/938] [D loss: 0.5954055786132812] [G loss: 0.6577550172805786]\n",
      "[Epoch 0/5] [Batch 254/938] [D loss: 0.5342189073562622] [G loss: 1.4797790050506592]\n",
      "[Epoch 0/5] [Batch 255/938] [D loss: 0.417370080947876] [G loss: 1.1925368309020996]\n",
      "[Epoch 0/5] [Batch 256/938] [D loss: 0.5115288496017456] [G loss: 0.9940057992935181]\n",
      "[Epoch 0/5] [Batch 257/938] [D loss: 0.5057820081710815] [G loss: 1.176855444908142]\n",
      "[Epoch 0/5] [Batch 258/938] [D loss: 0.5050559043884277] [G loss: 1.4808835983276367]\n",
      "[Epoch 0/5] [Batch 259/938] [D loss: 0.5553359389305115] [G loss: 0.7808371782302856]\n",
      "[Epoch 0/5] [Batch 260/938] [D loss: 0.5989574790000916] [G loss: 1.9835060834884644]\n",
      "[Epoch 0/5] [Batch 261/938] [D loss: 0.6613790988922119] [G loss: 0.48577216267585754]\n",
      "[Epoch 0/5] [Batch 262/938] [D loss: 0.5161279439926147] [G loss: 1.7394015789031982]\n",
      "[Epoch 0/5] [Batch 263/938] [D loss: 0.4613199234008789] [G loss: 1.0433605909347534]\n",
      "[Epoch 0/5] [Batch 264/938] [D loss: 0.47804194688796997] [G loss: 1.1640925407409668]\n",
      "[Epoch 0/5] [Batch 265/938] [D loss: 0.524823784828186] [G loss: 1.0913714170455933]\n",
      "[Epoch 0/5] [Batch 266/938] [D loss: 0.5092306137084961] [G loss: 0.957150399684906]\n",
      "[Epoch 0/5] [Batch 267/938] [D loss: 0.463367760181427] [G loss: 1.2315397262573242]\n",
      "[Epoch 0/5] [Batch 268/938] [D loss: 0.45398539304733276] [G loss: 1.3443009853363037]\n",
      "[Epoch 0/5] [Batch 269/938] [D loss: 0.5618991255760193] [G loss: 0.865980327129364]\n",
      "[Epoch 0/5] [Batch 270/938] [D loss: 0.5265688896179199] [G loss: 1.371133804321289]\n",
      "[Epoch 0/5] [Batch 271/938] [D loss: 0.4464736878871918] [G loss: 1.240139365196228]\n",
      "[Epoch 0/5] [Batch 272/938] [D loss: 0.5281480550765991] [G loss: 1.2216312885284424]\n",
      "[Epoch 0/5] [Batch 273/938] [D loss: 0.46465736627578735] [G loss: 1.2767999172210693]\n",
      "[Epoch 0/5] [Batch 274/938] [D loss: 0.4833412170410156] [G loss: 0.9962140321731567]\n",
      "[Epoch 0/5] [Batch 275/938] [D loss: 0.5070289373397827] [G loss: 1.2588151693344116]\n",
      "[Epoch 0/5] [Batch 276/938] [D loss: 0.5146967172622681] [G loss: 1.0824165344238281]\n",
      "[Epoch 0/5] [Batch 277/938] [D loss: 0.526715099811554] [G loss: 1.6719975471496582]\n",
      "[Epoch 0/5] [Batch 278/938] [D loss: 0.7472784519195557] [G loss: 0.39404743909835815]\n",
      "[Epoch 0/5] [Batch 279/938] [D loss: 0.8332533836364746] [G loss: 2.7939932346343994]\n",
      "[Epoch 0/5] [Batch 280/938] [D loss: 0.7862932682037354] [G loss: 0.30736392736434937]\n",
      "[Epoch 0/5] [Batch 281/938] [D loss: 0.5233627557754517] [G loss: 1.0717380046844482]\n",
      "[Epoch 0/5] [Batch 282/938] [D loss: 0.5167012214660645] [G loss: 1.690171241760254]\n",
      "[Epoch 0/5] [Batch 283/938] [D loss: 0.5086770057678223] [G loss: 0.880175769329071]\n",
      "[Epoch 0/5] [Batch 284/938] [D loss: 0.5120359063148499] [G loss: 0.9690678119659424]\n",
      "[Epoch 0/5] [Batch 285/938] [D loss: 0.4891415536403656] [G loss: 1.2541362047195435]\n",
      "[Epoch 0/5] [Batch 286/938] [D loss: 0.5002850294113159] [G loss: 1.184052586555481]\n",
      "[Epoch 0/5] [Batch 287/938] [D loss: 0.4258340001106262] [G loss: 1.1593310832977295]\n",
      "[Epoch 0/5] [Batch 288/938] [D loss: 0.5016359090805054] [G loss: 1.1469467878341675]\n",
      "[Epoch 0/5] [Batch 289/938] [D loss: 0.46155670285224915] [G loss: 1.191070556640625]\n",
      "[Epoch 0/5] [Batch 290/938] [D loss: 0.4436957836151123] [G loss: 1.3046932220458984]\n",
      "[Epoch 0/5] [Batch 291/938] [D loss: 0.46491575241088867] [G loss: 1.1599154472351074]\n",
      "[Epoch 0/5] [Batch 292/938] [D loss: 0.47587454319000244] [G loss: 1.4021059274673462]\n",
      "[Epoch 0/5] [Batch 293/938] [D loss: 0.5067591667175293] [G loss: 0.8039522767066956]\n",
      "[Epoch 0/5] [Batch 294/938] [D loss: 0.5427165627479553] [G loss: 2.052550792694092]\n",
      "[Epoch 0/5] [Batch 295/938] [D loss: 0.6009840369224548] [G loss: 0.5010578632354736]\n",
      "[Epoch 0/5] [Batch 296/938] [D loss: 0.5308375358581543] [G loss: 1.858028769493103]\n",
      "[Epoch 0/5] [Batch 297/938] [D loss: 0.6238366961479187] [G loss: 0.7517004609107971]\n",
      "[Epoch 0/5] [Batch 298/938] [D loss: 0.5507826209068298] [G loss: 1.4562804698944092]\n",
      "[Epoch 0/5] [Batch 299/938] [D loss: 0.4777616858482361] [G loss: 1.03377366065979]\n",
      "[Epoch 0/5] [Batch 300/938] [D loss: 0.5053536891937256] [G loss: 1.0238128900527954]\n",
      "[Epoch 0/5] [Batch 301/938] [D loss: 0.46396180987358093] [G loss: 1.1169806718826294]\n",
      "[Epoch 0/5] [Batch 302/938] [D loss: 0.5016376972198486] [G loss: 1.4362671375274658]\n",
      "[Epoch 0/5] [Batch 303/938] [D loss: 0.6401788592338562] [G loss: 0.5978063941001892]\n",
      "[Epoch 0/5] [Batch 304/938] [D loss: 0.735990583896637] [G loss: 2.4609057903289795]\n",
      "[Epoch 0/5] [Batch 305/938] [D loss: 0.6727023124694824] [G loss: 0.42572587728500366]\n",
      "[Epoch 0/5] [Batch 306/938] [D loss: 0.5320946574211121] [G loss: 1.2118666172027588]\n",
      "[Epoch 0/5] [Batch 307/938] [D loss: 0.5189492702484131] [G loss: 1.3784183263778687]\n",
      "[Epoch 0/5] [Batch 308/938] [D loss: 0.5004845857620239] [G loss: 1.0111931562423706]\n",
      "[Epoch 0/5] [Batch 309/938] [D loss: 0.5471879839897156] [G loss: 1.261759638786316]\n",
      "[Epoch 0/5] [Batch 310/938] [D loss: 0.4519256055355072] [G loss: 1.036105990409851]\n",
      "[Epoch 0/5] [Batch 311/938] [D loss: 0.5251580476760864] [G loss: 1.4134294986724854]\n",
      "[Epoch 0/5] [Batch 312/938] [D loss: 0.5076783299446106] [G loss: 1.0761733055114746]\n",
      "[Epoch 0/5] [Batch 313/938] [D loss: 0.47317421436309814] [G loss: 1.1091474294662476]\n",
      "[Epoch 0/5] [Batch 314/938] [D loss: 0.541526198387146] [G loss: 1.4077107906341553]\n",
      "[Epoch 0/5] [Batch 315/938] [D loss: 0.5422973036766052] [G loss: 0.6912077069282532]\n",
      "[Epoch 0/5] [Batch 316/938] [D loss: 0.5168920755386353] [G loss: 1.7360856533050537]\n",
      "[Epoch 0/5] [Batch 317/938] [D loss: 0.5363684296607971] [G loss: 0.876107931137085]\n",
      "[Epoch 0/5] [Batch 318/938] [D loss: 0.44514888525009155] [G loss: 1.5760307312011719]\n",
      "[Epoch 0/5] [Batch 319/938] [D loss: 0.4728204905986786] [G loss: 1.2053234577178955]\n",
      "[Epoch 0/5] [Batch 320/938] [D loss: 0.45471012592315674] [G loss: 1.096562385559082]\n",
      "[Epoch 0/5] [Batch 321/938] [D loss: 0.4793112277984619] [G loss: 1.44571852684021]\n",
      "[Epoch 0/5] [Batch 322/938] [D loss: 0.6443437933921814] [G loss: 0.6146058440208435]\n",
      "[Epoch 0/5] [Batch 323/938] [D loss: 0.8826452493667603] [G loss: 2.363698959350586]\n",
      "[Epoch 0/5] [Batch 324/938] [D loss: 0.7546834945678711] [G loss: 0.4229506254196167]\n",
      "[Epoch 0/5] [Batch 325/938] [D loss: 0.5420389175415039] [G loss: 1.108323335647583]\n",
      "[Epoch 0/5] [Batch 326/938] [D loss: 0.5433205962181091] [G loss: 1.558640480041504]\n",
      "[Epoch 0/5] [Batch 327/938] [D loss: 0.5047925710678101] [G loss: 0.9911471605300903]\n",
      "[Epoch 0/5] [Batch 328/938] [D loss: 0.5395673513412476] [G loss: 0.7936657071113586]\n",
      "[Epoch 0/5] [Batch 329/938] [D loss: 0.5454270839691162] [G loss: 1.4711101055145264]\n",
      "[Epoch 0/5] [Batch 330/938] [D loss: 0.5243959426879883] [G loss: 0.9651615619659424]\n",
      "[Epoch 0/5] [Batch 331/938] [D loss: 0.4621860980987549] [G loss: 1.2179625034332275]\n",
      "[Epoch 0/5] [Batch 332/938] [D loss: 0.48803719878196716] [G loss: 1.1747210025787354]\n",
      "[Epoch 0/5] [Batch 333/938] [D loss: 0.5240648984909058] [G loss: 1.0720698833465576]\n",
      "[Epoch 0/5] [Batch 334/938] [D loss: 0.4828870892524719] [G loss: 1.1079260110855103]\n",
      "[Epoch 0/5] [Batch 335/938] [D loss: 0.5710082054138184] [G loss: 1.3599393367767334]\n",
      "[Epoch 0/5] [Batch 336/938] [D loss: 0.538079023361206] [G loss: 0.8271191716194153]\n",
      "[Epoch 0/5] [Batch 337/938] [D loss: 0.5866603255271912] [G loss: 1.7320725917816162]\n",
      "[Epoch 0/5] [Batch 338/938] [D loss: 0.6249352097511292] [G loss: 0.6265960931777954]\n",
      "[Epoch 0/5] [Batch 339/938] [D loss: 0.5138255953788757] [G loss: 1.3516907691955566]\n",
      "[Epoch 0/5] [Batch 340/938] [D loss: 0.47963833808898926] [G loss: 1.200371265411377]\n",
      "[Epoch 0/5] [Batch 341/938] [D loss: 0.534052848815918] [G loss: 1.0064762830734253]\n",
      "[Epoch 0/5] [Batch 342/938] [D loss: 0.4829291105270386] [G loss: 1.2001656293869019]\n",
      "[Epoch 0/5] [Batch 343/938] [D loss: 0.5396907925605774] [G loss: 1.5927627086639404]\n",
      "[Epoch 0/5] [Batch 344/938] [D loss: 0.5992090106010437] [G loss: 0.603817343711853]\n",
      "[Epoch 0/5] [Batch 345/938] [D loss: 0.7181934118270874] [G loss: 2.266920566558838]\n",
      "[Epoch 0/5] [Batch 346/938] [D loss: 0.6688897013664246] [G loss: 0.5020126104354858]\n",
      "[Epoch 0/5] [Batch 347/938] [D loss: 0.5370466709136963] [G loss: 1.3417013883590698]\n",
      "[Epoch 0/5] [Batch 348/938] [D loss: 0.48198288679122925] [G loss: 1.4179197549819946]\n",
      "[Epoch 0/5] [Batch 349/938] [D loss: 0.5292749404907227] [G loss: 0.8592312932014465]\n",
      "[Epoch 0/5] [Batch 350/938] [D loss: 0.48689091205596924] [G loss: 1.153321623802185]\n",
      "[Epoch 0/5] [Batch 351/938] [D loss: 0.5061808824539185] [G loss: 1.321471929550171]\n",
      "[Epoch 0/5] [Batch 352/938] [D loss: 0.49647629261016846] [G loss: 0.9477364420890808]\n",
      "[Epoch 0/5] [Batch 353/938] [D loss: 0.4910787045955658] [G loss: 1.3611092567443848]\n",
      "[Epoch 0/5] [Batch 354/938] [D loss: 0.5579678416252136] [G loss: 1.187988519668579]\n",
      "[Epoch 0/5] [Batch 355/938] [D loss: 0.4908708930015564] [G loss: 0.8669918775558472]\n",
      "[Epoch 0/5] [Batch 356/938] [D loss: 0.5030547380447388] [G loss: 1.6362009048461914]\n",
      "[Epoch 0/5] [Batch 357/938] [D loss: 0.5534253716468811] [G loss: 0.758105456829071]\n",
      "[Epoch 0/5] [Batch 358/938] [D loss: 0.5119271278381348] [G loss: 1.2037276029586792]\n",
      "[Epoch 0/5] [Batch 359/938] [D loss: 0.5319767594337463] [G loss: 1.2615697383880615]\n",
      "[Epoch 0/5] [Batch 360/938] [D loss: 0.4792335331439972] [G loss: 0.9836739301681519]\n",
      "[Epoch 0/5] [Batch 361/938] [D loss: 0.601636528968811] [G loss: 1.7712267637252808]\n",
      "[Epoch 0/5] [Batch 362/938] [D loss: 0.728405237197876] [G loss: 0.4215586185455322]\n",
      "[Epoch 0/5] [Batch 363/938] [D loss: 0.6275656223297119] [G loss: 1.9860014915466309]\n",
      "[Epoch 0/5] [Batch 364/938] [D loss: 0.5520409941673279] [G loss: 0.8303738832473755]\n",
      "[Epoch 0/5] [Batch 365/938] [D loss: 0.5292354822158813] [G loss: 1.0850210189819336]\n",
      "[Epoch 0/5] [Batch 366/938] [D loss: 0.5223734378814697] [G loss: 1.6700973510742188]\n",
      "[Epoch 0/5] [Batch 367/938] [D loss: 0.5856088399887085] [G loss: 0.6823064684867859]\n",
      "[Epoch 0/5] [Batch 368/938] [D loss: 0.5736806988716125] [G loss: 1.7246556282043457]\n",
      "[Epoch 0/5] [Batch 369/938] [D loss: 0.611833930015564] [G loss: 0.6827444434165955]\n",
      "[Epoch 0/5] [Batch 370/938] [D loss: 0.4922204613685608] [G loss: 1.317779541015625]\n",
      "[Epoch 0/5] [Batch 371/938] [D loss: 0.5204417109489441] [G loss: 1.0150542259216309]\n",
      "[Epoch 0/5] [Batch 372/938] [D loss: 0.5003536343574524] [G loss: 1.0370831489562988]\n",
      "[Epoch 0/5] [Batch 373/938] [D loss: 0.4898267984390259] [G loss: 1.2565656900405884]\n",
      "[Epoch 0/5] [Batch 374/938] [D loss: 0.5069791078567505] [G loss: 1.0572768449783325]\n",
      "[Epoch 0/5] [Batch 375/938] [D loss: 0.4492427110671997] [G loss: 1.2392489910125732]\n",
      "[Epoch 0/5] [Batch 376/938] [D loss: 0.47166356444358826] [G loss: 1.3940421342849731]\n",
      "[Epoch 0/5] [Batch 377/938] [D loss: 0.49950557947158813] [G loss: 1.123108148574829]\n",
      "[Epoch 0/5] [Batch 378/938] [D loss: 0.5489477515220642] [G loss: 1.0339077711105347]\n",
      "[Epoch 0/5] [Batch 379/938] [D loss: 0.5479259490966797] [G loss: 1.2771745920181274]\n",
      "[Epoch 0/5] [Batch 380/938] [D loss: 0.5462356805801392] [G loss: 0.774921178817749]\n",
      "[Epoch 0/5] [Batch 381/938] [D loss: 0.9268659949302673] [G loss: 2.6010050773620605]\n",
      "[Epoch 0/5] [Batch 382/938] [D loss: 1.050235390663147] [G loss: 0.1367693394422531]\n",
      "[Epoch 0/5] [Batch 383/938] [D loss: 0.5238332748413086] [G loss: 0.7768344283103943]\n",
      "[Epoch 0/5] [Batch 384/938] [D loss: 0.6892293691635132] [G loss: 2.161931037902832]\n",
      "[Epoch 0/5] [Batch 385/938] [D loss: 0.5297771692276001] [G loss: 0.9234107136726379]\n",
      "[Epoch 0/5] [Batch 386/938] [D loss: 0.5791530609130859] [G loss: 0.7032561898231506]\n",
      "[Epoch 0/5] [Batch 387/938] [D loss: 0.5375356078147888] [G loss: 1.0923470258712769]\n",
      "[Epoch 0/5] [Batch 388/938] [D loss: 0.5037923455238342] [G loss: 1.1836035251617432]\n",
      "[Epoch 0/5] [Batch 389/938] [D loss: 0.4880106449127197] [G loss: 1.0702695846557617]\n",
      "[Epoch 0/5] [Batch 390/938] [D loss: 0.5099164247512817] [G loss: 1.0906319618225098]\n",
      "[Epoch 0/5] [Batch 391/938] [D loss: 0.5140440464019775] [G loss: 1.4030815362930298]\n",
      "[Epoch 0/5] [Batch 392/938] [D loss: 0.5014021396636963] [G loss: 0.9829003810882568]\n",
      "[Epoch 0/5] [Batch 393/938] [D loss: 0.5130690932273865] [G loss: 0.9927324056625366]\n",
      "[Epoch 0/5] [Batch 394/938] [D loss: 0.5214167833328247] [G loss: 1.1871010065078735]\n",
      "[Epoch 0/5] [Batch 395/938] [D loss: 0.5090462565422058] [G loss: 0.9593151807785034]\n",
      "[Epoch 0/5] [Batch 396/938] [D loss: 0.4958639144897461] [G loss: 1.167392373085022]\n",
      "[Epoch 0/5] [Batch 397/938] [D loss: 0.5040609836578369] [G loss: 1.6244813203811646]\n",
      "[Epoch 0/5] [Batch 398/938] [D loss: 0.6007190942764282] [G loss: 0.9398748874664307]\n",
      "[Epoch 0/5] [Batch 399/938] [D loss: 0.4950735569000244] [G loss: 1.2331459522247314]\n",
      "[Epoch 0/5] [Batch 400/938] [D loss: 0.5601550340652466] [G loss: 1.1536556482315063]\n",
      "[Epoch 0/5] [Batch 401/938] [D loss: 0.5587123036384583] [G loss: 0.6014872789382935]\n",
      "[Epoch 0/5] [Batch 402/938] [D loss: 0.6430482864379883] [G loss: 1.978294014930725]\n",
      "[Epoch 0/5] [Batch 403/938] [D loss: 0.6643232107162476] [G loss: 0.4634287357330322]\n",
      "[Epoch 0/5] [Batch 404/938] [D loss: 0.5152269601821899] [G loss: 1.4399930238723755]\n",
      "[Epoch 0/5] [Batch 405/938] [D loss: 0.5488301515579224] [G loss: 1.3535258769989014]\n",
      "[Epoch 0/5] [Batch 406/938] [D loss: 0.5775042772293091] [G loss: 0.8450879454612732]\n",
      "[Epoch 0/5] [Batch 407/938] [D loss: 0.4951016902923584] [G loss: 1.0032844543457031]\n",
      "[Epoch 0/5] [Batch 408/938] [D loss: 0.5424829125404358] [G loss: 1.5693479776382446]\n",
      "[Epoch 0/5] [Batch 409/938] [D loss: 0.6289838552474976] [G loss: 0.5786241292953491]\n",
      "[Epoch 0/5] [Batch 410/938] [D loss: 0.5544620156288147] [G loss: 1.717603087425232]\n",
      "[Epoch 0/5] [Batch 411/938] [D loss: 0.4942755103111267] [G loss: 0.8145313262939453]\n",
      "[Epoch 0/5] [Batch 412/938] [D loss: 0.5378981828689575] [G loss: 1.2440454959869385]\n",
      "[Epoch 0/5] [Batch 413/938] [D loss: 0.49474769830703735] [G loss: 1.1961894035339355]\n",
      "[Epoch 0/5] [Batch 414/938] [D loss: 0.45205366611480713] [G loss: 1.0679199695587158]\n",
      "[Epoch 0/5] [Batch 415/938] [D loss: 0.49226507544517517] [G loss: 1.0660932064056396]\n",
      "[Epoch 0/5] [Batch 416/938] [D loss: 0.5293021202087402] [G loss: 1.4697930812835693]\n",
      "[Epoch 0/5] [Batch 417/938] [D loss: 0.626334547996521] [G loss: 0.671325147151947]\n",
      "[Epoch 0/5] [Batch 418/938] [D loss: 0.5392131805419922] [G loss: 1.9613838195800781]\n",
      "[Epoch 0/5] [Batch 419/938] [D loss: 0.5473705530166626] [G loss: 0.9365096092224121]\n",
      "[Epoch 0/5] [Batch 420/938] [D loss: 0.5367172956466675] [G loss: 1.0134971141815186]\n",
      "[Epoch 0/5] [Batch 421/938] [D loss: 0.4952571392059326] [G loss: 1.4859158992767334]\n",
      "[Epoch 0/5] [Batch 422/938] [D loss: 0.5286259055137634] [G loss: 1.0658048391342163]\n",
      "[Epoch 0/5] [Batch 423/938] [D loss: 0.48435813188552856] [G loss: 1.46278715133667]\n",
      "[Epoch 0/5] [Batch 424/938] [D loss: 0.542674720287323] [G loss: 1.3251875638961792]\n",
      "[Epoch 0/5] [Batch 425/938] [D loss: 0.5331456661224365] [G loss: 0.8686701655387878]\n",
      "[Epoch 0/5] [Batch 426/938] [D loss: 0.5320993661880493] [G loss: 2.0227417945861816]\n",
      "[Epoch 0/5] [Batch 427/938] [D loss: 0.6492241024971008] [G loss: 0.6392702460289001]\n",
      "[Epoch 0/5] [Batch 428/938] [D loss: 0.5156545639038086] [G loss: 1.6657195091247559]\n",
      "[Epoch 0/5] [Batch 429/938] [D loss: 0.46651411056518555] [G loss: 1.093935251235962]\n",
      "[Epoch 0/5] [Batch 430/938] [D loss: 0.4653988480567932] [G loss: 1.111772060394287]\n",
      "[Epoch 0/5] [Batch 431/938] [D loss: 0.5060504078865051] [G loss: 1.5273377895355225]\n",
      "[Epoch 0/5] [Batch 432/938] [D loss: 0.5361182689666748] [G loss: 0.7323373556137085]\n",
      "[Epoch 0/5] [Batch 433/938] [D loss: 0.4672454595565796] [G loss: 1.4539703130722046]\n",
      "[Epoch 0/5] [Batch 434/938] [D loss: 0.4662124514579773] [G loss: 1.2239927053451538]\n",
      "[Epoch 0/5] [Batch 435/938] [D loss: 0.5381360054016113] [G loss: 0.8695372939109802]\n",
      "[Epoch 0/5] [Batch 436/938] [D loss: 0.606373131275177] [G loss: 2.3721554279327393]\n",
      "[Epoch 0/5] [Batch 437/938] [D loss: 0.6640488505363464] [G loss: 0.5149063467979431]\n",
      "[Epoch 0/5] [Batch 438/938] [D loss: 0.47651493549346924] [G loss: 1.7236201763153076]\n",
      "[Epoch 0/5] [Batch 439/938] [D loss: 0.48098960518836975] [G loss: 1.5371427536010742]\n",
      "[Epoch 0/5] [Batch 440/938] [D loss: 0.49997565150260925] [G loss: 0.8355702757835388]\n",
      "[Epoch 0/5] [Batch 441/938] [D loss: 0.48297107219696045] [G loss: 1.332183599472046]\n",
      "[Epoch 0/5] [Batch 442/938] [D loss: 0.514502227306366] [G loss: 1.2214850187301636]\n",
      "[Epoch 0/5] [Batch 443/938] [D loss: 0.46902555227279663] [G loss: 1.1597206592559814]\n",
      "[Epoch 0/5] [Batch 444/938] [D loss: 0.5162439942359924] [G loss: 1.1292269229888916]\n",
      "[Epoch 0/5] [Batch 445/938] [D loss: 0.5850313305854797] [G loss: 1.7459533214569092]\n",
      "[Epoch 0/5] [Batch 446/938] [D loss: 0.5345973968505859] [G loss: 0.7214508652687073]\n",
      "[Epoch 0/5] [Batch 447/938] [D loss: 0.5278553366661072] [G loss: 1.9511390924453735]\n",
      "[Epoch 0/5] [Batch 448/938] [D loss: 0.5863337516784668] [G loss: 0.6371184587478638]\n",
      "[Epoch 0/5] [Batch 449/938] [D loss: 0.45244067907333374] [G loss: 1.5864571332931519]\n",
      "[Epoch 0/5] [Batch 450/938] [D loss: 0.5078322887420654] [G loss: 1.2882992029190063]\n",
      "[Epoch 0/5] [Batch 451/938] [D loss: 0.44120487570762634] [G loss: 1.359979510307312]\n",
      "[Epoch 0/5] [Batch 452/938] [D loss: 0.4832702577114105] [G loss: 1.0404117107391357]\n",
      "[Epoch 0/5] [Batch 453/938] [D loss: 0.49069803953170776] [G loss: 1.5390944480895996]\n",
      "[Epoch 0/5] [Batch 454/938] [D loss: 0.49868327379226685] [G loss: 0.7685776352882385]\n",
      "[Epoch 0/5] [Batch 455/938] [D loss: 0.5629712343215942] [G loss: 1.751202940940857]\n",
      "[Epoch 0/5] [Batch 456/938] [D loss: 0.5841534733772278] [G loss: 0.6511351466178894]\n",
      "[Epoch 0/5] [Batch 457/938] [D loss: 0.521295428276062] [G loss: 1.5642127990722656]\n",
      "[Epoch 0/5] [Batch 458/938] [D loss: 0.47253307700157166] [G loss: 1.4167795181274414]\n",
      "[Epoch 0/5] [Batch 459/938] [D loss: 0.5557135939598083] [G loss: 0.7415499687194824]\n",
      "[Epoch 0/5] [Batch 460/938] [D loss: 0.5234553217887878] [G loss: 1.616600751876831]\n",
      "[Epoch 0/5] [Batch 461/938] [D loss: 0.5099905729293823] [G loss: 0.8190475702285767]\n",
      "[Epoch 0/5] [Batch 462/938] [D loss: 0.5411292314529419] [G loss: 1.6605912446975708]\n",
      "[Epoch 0/5] [Batch 463/938] [D loss: 0.5221251249313354] [G loss: 0.9818912148475647]\n",
      "[Epoch 0/5] [Batch 464/938] [D loss: 0.5081149339675903] [G loss: 1.1950706243515015]\n",
      "[Epoch 0/5] [Batch 465/938] [D loss: 0.5174906253814697] [G loss: 1.1757843494415283]\n",
      "[Epoch 0/5] [Batch 466/938] [D loss: 0.45751333236694336] [G loss: 1.2203474044799805]\n",
      "[Epoch 0/5] [Batch 467/938] [D loss: 0.49076443910598755] [G loss: 1.3718913793563843]\n",
      "[Epoch 0/5] [Batch 468/938] [D loss: 0.46932247281074524] [G loss: 1.3096976280212402]\n",
      "[Epoch 0/5] [Batch 469/938] [D loss: 0.45564138889312744] [G loss: 1.34541654586792]\n",
      "[Epoch 0/5] [Batch 470/938] [D loss: 0.4992605745792389] [G loss: 0.9756479859352112]\n",
      "[Epoch 0/5] [Batch 471/938] [D loss: 0.5547186136245728] [G loss: 1.6724964380264282]\n",
      "[Epoch 0/5] [Batch 472/938] [D loss: 0.6524429321289062] [G loss: 0.4989422857761383]\n",
      "[Epoch 0/5] [Batch 473/938] [D loss: 0.8199902176856995] [G loss: 2.7419865131378174]\n",
      "[Epoch 0/5] [Batch 474/938] [D loss: 0.7361841201782227] [G loss: 0.4067143499851227]\n",
      "[Epoch 0/5] [Batch 475/938] [D loss: 0.5123846530914307] [G loss: 1.2472296953201294]\n",
      "[Epoch 0/5] [Batch 476/938] [D loss: 0.5105922818183899] [G loss: 1.3067048788070679]\n",
      "[Epoch 0/5] [Batch 477/938] [D loss: 0.5590736269950867] [G loss: 1.0514397621154785]\n",
      "[Epoch 0/5] [Batch 478/938] [D loss: 0.562903881072998] [G loss: 1.1530241966247559]\n",
      "[Epoch 0/5] [Batch 479/938] [D loss: 0.4362030625343323] [G loss: 1.2102832794189453]\n",
      "[Epoch 0/5] [Batch 480/938] [D loss: 0.5265967845916748] [G loss: 1.3507003784179688]\n",
      "[Epoch 0/5] [Batch 481/938] [D loss: 0.4577660858631134] [G loss: 1.0408389568328857]\n",
      "[Epoch 0/5] [Batch 482/938] [D loss: 0.5374011397361755] [G loss: 1.7619478702545166]\n",
      "[Epoch 0/5] [Batch 483/938] [D loss: 0.5898153781890869] [G loss: 0.7180013060569763]\n",
      "[Epoch 0/5] [Batch 484/938] [D loss: 0.5559554100036621] [G loss: 1.539198398590088]\n",
      "[Epoch 0/5] [Batch 485/938] [D loss: 0.5545129776000977] [G loss: 0.8595845103263855]\n",
      "[Epoch 0/5] [Batch 486/938] [D loss: 0.570362389087677] [G loss: 1.2697356939315796]\n",
      "[Epoch 0/5] [Batch 487/938] [D loss: 0.6033692359924316] [G loss: 0.8194226026535034]\n",
      "[Epoch 0/5] [Batch 488/938] [D loss: 0.4691593647003174] [G loss: 1.3883720636367798]\n",
      "[Epoch 0/5] [Batch 489/938] [D loss: 0.5024608373641968] [G loss: 1.3070833683013916]\n",
      "[Epoch 0/5] [Batch 490/938] [D loss: 0.49151507019996643] [G loss: 1.0550270080566406]\n",
      "[Epoch 0/5] [Batch 491/938] [D loss: 0.5144325494766235] [G loss: 1.3788150548934937]\n",
      "[Epoch 0/5] [Batch 492/938] [D loss: 0.545635461807251] [G loss: 0.9147821068763733]\n",
      "[Epoch 0/5] [Batch 493/938] [D loss: 0.5581701397895813] [G loss: 1.825917363166809]\n",
      "[Epoch 0/5] [Batch 494/938] [D loss: 0.6026418209075928] [G loss: 0.6688327193260193]\n",
      "[Epoch 0/5] [Batch 495/938] [D loss: 0.5444900989532471] [G loss: 1.7919353246688843]\n",
      "[Epoch 0/5] [Batch 496/938] [D loss: 0.4829297661781311] [G loss: 0.8440571427345276]\n",
      "[Epoch 0/5] [Batch 497/938] [D loss: 0.5460386872291565] [G loss: 1.4887508153915405]\n",
      "[Epoch 0/5] [Batch 498/938] [D loss: 0.5363001823425293] [G loss: 0.8784981369972229]\n",
      "[Epoch 0/5] [Batch 499/938] [D loss: 0.5049501657485962] [G loss: 1.2567287683486938]\n",
      "[Epoch 0/5] [Batch 500/938] [D loss: 0.5195982456207275] [G loss: 1.3356826305389404]\n",
      "[Epoch 0/5] [Batch 501/938] [D loss: 0.5181241035461426] [G loss: 0.8917356729507446]\n",
      "[Epoch 0/5] [Batch 502/938] [D loss: 0.5498541593551636] [G loss: 1.737683892250061]\n",
      "[Epoch 0/5] [Batch 503/938] [D loss: 0.6302503347396851] [G loss: 0.62552809715271]\n",
      "[Epoch 0/5] [Batch 504/938] [D loss: 0.4965882897377014] [G loss: 1.6331616640090942]\n",
      "[Epoch 0/5] [Batch 505/938] [D loss: 0.5567253828048706] [G loss: 0.9041679501533508]\n",
      "[Epoch 0/5] [Batch 506/938] [D loss: 0.5424005389213562] [G loss: 1.5267794132232666]\n",
      "[Epoch 0/5] [Batch 507/938] [D loss: 0.5203113555908203] [G loss: 0.9936478137969971]\n",
      "[Epoch 0/5] [Batch 508/938] [D loss: 0.5752783417701721] [G loss: 1.6512211561203003]\n",
      "[Epoch 0/5] [Batch 509/938] [D loss: 0.6326127052307129] [G loss: 0.6284058690071106]\n",
      "[Epoch 0/5] [Batch 510/938] [D loss: 0.5284578800201416] [G loss: 1.6519776582717896]\n",
      "[Epoch 0/5] [Batch 511/938] [D loss: 0.5969803929328918] [G loss: 0.8562527894973755]\n",
      "[Epoch 0/5] [Batch 512/938] [D loss: 0.48325008153915405] [G loss: 1.6608561277389526]\n",
      "[Epoch 0/5] [Batch 513/938] [D loss: 0.4760981500148773] [G loss: 1.1189683675765991]\n",
      "[Epoch 0/5] [Batch 514/938] [D loss: 0.5374218225479126] [G loss: 1.0856128931045532]\n",
      "[Epoch 0/5] [Batch 515/938] [D loss: 0.5067048668861389] [G loss: 1.0467649698257446]\n",
      "[Epoch 0/5] [Batch 516/938] [D loss: 0.5167393088340759] [G loss: 1.484581708908081]\n",
      "[Epoch 0/5] [Batch 517/938] [D loss: 0.5060361623764038] [G loss: 0.8400968909263611]\n",
      "[Epoch 0/5] [Batch 518/938] [D loss: 0.6845610737800598] [G loss: 2.225368022918701]\n",
      "[Epoch 0/5] [Batch 519/938] [D loss: 0.8719977140426636] [G loss: 0.30402761697769165]\n",
      "[Epoch 0/5] [Batch 520/938] [D loss: 0.5464169383049011] [G loss: 1.2705005407333374]\n",
      "[Epoch 0/5] [Batch 521/938] [D loss: 0.5611008405685425] [G loss: 1.2880269289016724]\n",
      "[Epoch 0/5] [Batch 522/938] [D loss: 0.5916622281074524] [G loss: 0.7088468074798584]\n",
      "[Epoch 0/5] [Batch 523/938] [D loss: 0.5058363080024719] [G loss: 1.1972274780273438]\n",
      "[Epoch 0/5] [Batch 524/938] [D loss: 0.5048457980155945] [G loss: 1.3030929565429688]\n",
      "[Epoch 0/5] [Batch 525/938] [D loss: 0.4926278293132782] [G loss: 1.0670145750045776]\n",
      "[Epoch 0/5] [Batch 526/938] [D loss: 0.5029921531677246] [G loss: 1.180141568183899]\n",
      "[Epoch 0/5] [Batch 527/938] [D loss: 0.4826686978340149] [G loss: 1.1562778949737549]\n",
      "[Epoch 0/5] [Batch 528/938] [D loss: 0.46023643016815186] [G loss: 1.2649389505386353]\n",
      "[Epoch 0/5] [Batch 529/938] [D loss: 0.5201966762542725] [G loss: 1.0706003904342651]\n",
      "[Epoch 0/5] [Batch 530/938] [D loss: 0.46653854846954346] [G loss: 1.360477328300476]\n",
      "[Epoch 0/5] [Batch 531/938] [D loss: 0.4671536982059479] [G loss: 1.1490809917449951]\n",
      "[Epoch 0/5] [Batch 532/938] [D loss: 0.4313321113586426] [G loss: 1.4820924997329712]\n",
      "[Epoch 0/5] [Batch 533/938] [D loss: 0.503508985042572] [G loss: 1.2606139183044434]\n",
      "[Epoch 0/5] [Batch 534/938] [D loss: 0.48432040214538574] [G loss: 0.9589502811431885]\n",
      "[Epoch 0/5] [Batch 535/938] [D loss: 0.5896205902099609] [G loss: 1.632980465888977]\n",
      "[Epoch 0/5] [Batch 536/938] [D loss: 0.6886317729949951] [G loss: 0.5304290652275085]\n",
      "[Epoch 0/5] [Batch 537/938] [D loss: 0.7799798846244812] [G loss: 2.452918291091919]\n",
      "[Epoch 0/5] [Batch 538/938] [D loss: 0.6962022185325623] [G loss: 0.3920941948890686]\n",
      "[Epoch 0/5] [Batch 539/938] [D loss: 0.560236930847168] [G loss: 1.4378180503845215]\n",
      "[Epoch 0/5] [Batch 540/938] [D loss: 0.5201861262321472] [G loss: 1.308956265449524]\n",
      "[Epoch 0/5] [Batch 541/938] [D loss: 0.5702528953552246] [G loss: 0.8057922720909119]\n",
      "[Epoch 0/5] [Batch 542/938] [D loss: 0.4854758679866791] [G loss: 1.1869399547576904]\n",
      "[Epoch 0/5] [Batch 543/938] [D loss: 0.5425287485122681] [G loss: 1.3021259307861328]\n",
      "[Epoch 0/5] [Batch 544/938] [D loss: 0.47395211458206177] [G loss: 0.9369958639144897]\n",
      "[Epoch 0/5] [Batch 545/938] [D loss: 0.5441450476646423] [G loss: 1.2434967756271362]\n",
      "[Epoch 0/5] [Batch 546/938] [D loss: 0.44026055932044983] [G loss: 1.0712370872497559]\n",
      "[Epoch 0/5] [Batch 547/938] [D loss: 0.5168768167495728] [G loss: 1.3245567083358765]\n",
      "[Epoch 0/5] [Batch 548/938] [D loss: 0.5005291700363159] [G loss: 1.0298150777816772]\n",
      "[Epoch 0/5] [Batch 549/938] [D loss: 0.5193986296653748] [G loss: 1.3637378215789795]\n",
      "[Epoch 0/5] [Batch 550/938] [D loss: 0.521114706993103] [G loss: 1.2183566093444824]\n",
      "[Epoch 0/5] [Batch 551/938] [D loss: 0.47901463508605957] [G loss: 1.3985686302185059]\n",
      "[Epoch 0/5] [Batch 552/938] [D loss: 0.5331956148147583] [G loss: 0.9223608374595642]\n",
      "[Epoch 0/5] [Batch 553/938] [D loss: 0.6057254672050476] [G loss: 1.8445314168930054]\n",
      "[Epoch 0/5] [Batch 554/938] [D loss: 0.6393114328384399] [G loss: 0.5587357878684998]\n",
      "[Epoch 0/5] [Batch 555/938] [D loss: 0.5888856649398804] [G loss: 1.7295197248458862]\n",
      "[Epoch 0/5] [Batch 556/938] [D loss: 0.5282002091407776] [G loss: 0.843234658241272]\n",
      "[Epoch 0/5] [Batch 557/938] [D loss: 0.49787646532058716] [G loss: 1.615389108657837]\n",
      "[Epoch 0/5] [Batch 558/938] [D loss: 0.47279542684555054] [G loss: 1.1489135026931763]\n",
      "[Epoch 0/5] [Batch 559/938] [D loss: 0.5314834713935852] [G loss: 1.2777093648910522]\n",
      "[Epoch 0/5] [Batch 560/938] [D loss: 0.4799022078514099] [G loss: 1.1077675819396973]\n",
      "[Epoch 0/5] [Batch 561/938] [D loss: 0.46784958243370056] [G loss: 1.4729690551757812]\n",
      "[Epoch 0/5] [Batch 562/938] [D loss: 0.49834442138671875] [G loss: 1.0161938667297363]\n",
      "[Epoch 0/5] [Batch 563/938] [D loss: 0.4990914463996887] [G loss: 1.6770451068878174]\n",
      "[Epoch 0/5] [Batch 564/938] [D loss: 0.5258906483650208] [G loss: 0.8545271158218384]\n",
      "[Epoch 0/5] [Batch 565/938] [D loss: 0.5503534078598022] [G loss: 1.8737175464630127]\n",
      "[Epoch 0/5] [Batch 566/938] [D loss: 0.625217080116272] [G loss: 0.6316582560539246]\n",
      "[Epoch 0/5] [Batch 567/938] [D loss: 0.4707001745700836] [G loss: 1.9730490446090698]\n",
      "[Epoch 0/5] [Batch 568/938] [D loss: 0.48081260919570923] [G loss: 0.96695476770401]\n",
      "[Epoch 0/5] [Batch 569/938] [D loss: 0.4610273241996765] [G loss: 1.1703753471374512]\n",
      "[Epoch 0/5] [Batch 570/938] [D loss: 0.5374757051467896] [G loss: 1.768534779548645]\n",
      "[Epoch 0/5] [Batch 571/938] [D loss: 0.5640872716903687] [G loss: 0.7630126476287842]\n",
      "[Epoch 0/5] [Batch 572/938] [D loss: 0.498365581035614] [G loss: 1.2858480215072632]\n",
      "[Epoch 0/5] [Batch 573/938] [D loss: 0.47346752882003784] [G loss: 1.3735394477844238]\n",
      "[Epoch 0/5] [Batch 574/938] [D loss: 0.522562563419342] [G loss: 0.9906097650527954]\n",
      "[Epoch 0/5] [Batch 575/938] [D loss: 0.52468341588974] [G loss: 1.7108347415924072]\n",
      "[Epoch 0/5] [Batch 576/938] [D loss: 0.5786653757095337] [G loss: 0.7421989440917969]\n",
      "[Epoch 0/5] [Batch 577/938] [D loss: 0.5682521462440491] [G loss: 1.9207302331924438]\n",
      "[Epoch 0/5] [Batch 578/938] [D loss: 0.6882479190826416] [G loss: 0.4679757356643677]\n",
      "[Epoch 0/5] [Batch 579/938] [D loss: 0.5131487846374512] [G loss: 1.66649329662323]\n",
      "[Epoch 0/5] [Batch 580/938] [D loss: 0.48547011613845825] [G loss: 1.2189037799835205]\n",
      "[Epoch 0/5] [Batch 581/938] [D loss: 0.4545615315437317] [G loss: 1.3888721466064453]\n",
      "[Epoch 0/5] [Batch 582/938] [D loss: 0.4702116549015045] [G loss: 1.1362569332122803]\n",
      "[Epoch 0/5] [Batch 583/938] [D loss: 0.4945279359817505] [G loss: 1.5469456911087036]\n",
      "[Epoch 0/5] [Batch 584/938] [D loss: 0.49062156677246094] [G loss: 0.77885901927948]\n",
      "[Epoch 0/5] [Batch 585/938] [D loss: 0.5109041929244995] [G loss: 2.01161789894104]\n",
      "[Epoch 0/5] [Batch 586/938] [D loss: 0.5337117910385132] [G loss: 0.8089744448661804]\n",
      "[Epoch 0/5] [Batch 587/938] [D loss: 0.5309520959854126] [G loss: 1.6230676174163818]\n",
      "[Epoch 0/5] [Batch 588/938] [D loss: 0.5636153817176819] [G loss: 0.7981085181236267]\n",
      "[Epoch 0/5] [Batch 589/938] [D loss: 0.5968561172485352] [G loss: 2.0475962162017822]\n",
      "[Epoch 0/5] [Batch 590/938] [D loss: 0.5339486002922058] [G loss: 0.6579920053482056]\n",
      "[Epoch 0/5] [Batch 591/938] [D loss: 0.5568393468856812] [G loss: 1.3268879652023315]\n",
      "[Epoch 0/5] [Batch 592/938] [D loss: 0.49057817459106445] [G loss: 0.9213132858276367]\n",
      "[Epoch 0/5] [Batch 593/938] [D loss: 0.5086817741394043] [G loss: 1.3522448539733887]\n",
      "[Epoch 0/5] [Batch 594/938] [D loss: 0.49079015851020813] [G loss: 1.0640294551849365]\n",
      "[Epoch 0/5] [Batch 595/938] [D loss: 0.5575853586196899] [G loss: 1.8056360483169556]\n",
      "[Epoch 0/5] [Batch 596/938] [D loss: 0.5901049971580505] [G loss: 0.7291094660758972]\n",
      "[Epoch 0/5] [Batch 597/938] [D loss: 0.4810844659805298] [G loss: 1.6822088956832886]\n",
      "[Epoch 0/5] [Batch 598/938] [D loss: 0.4275665879249573] [G loss: 1.205891728401184]\n",
      "[Epoch 0/5] [Batch 599/938] [D loss: 0.5054893493652344] [G loss: 0.9598097205162048]\n",
      "[Epoch 0/5] [Batch 600/938] [D loss: 0.5481666326522827] [G loss: 1.7836633920669556]\n",
      "[Epoch 0/5] [Batch 601/938] [D loss: 0.5503444671630859] [G loss: 0.8214026689529419]\n",
      "[Epoch 0/5] [Batch 602/938] [D loss: 0.4969504475593567] [G loss: 1.6476413011550903]\n",
      "[Epoch 0/5] [Batch 603/938] [D loss: 0.4935673475265503] [G loss: 0.9885799288749695]\n",
      "[Epoch 0/5] [Batch 604/938] [D loss: 0.44323861598968506] [G loss: 1.6419312953948975]\n",
      "[Epoch 0/5] [Batch 605/938] [D loss: 0.44818633794784546] [G loss: 1.0678530931472778]\n",
      "[Epoch 0/5] [Batch 606/938] [D loss: 0.4698360562324524] [G loss: 1.3304001092910767]\n",
      "[Epoch 0/5] [Batch 607/938] [D loss: 0.4860575795173645] [G loss: 1.1749591827392578]\n",
      "[Epoch 0/5] [Batch 608/938] [D loss: 0.4785609841346741] [G loss: 1.2997939586639404]\n",
      "[Epoch 0/5] [Batch 609/938] [D loss: 0.4758642315864563] [G loss: 0.9777754545211792]\n",
      "[Epoch 0/5] [Batch 610/938] [D loss: 0.6964330673217773] [G loss: 1.9711503982543945]\n",
      "[Epoch 0/5] [Batch 611/938] [D loss: 0.7907626628875732] [G loss: 0.4058151841163635]\n",
      "[Epoch 0/5] [Batch 612/938] [D loss: 0.4783231019973755] [G loss: 1.8467786312103271]\n",
      "[Epoch 0/5] [Batch 613/938] [D loss: 0.5058395266532898] [G loss: 1.334488868713379]\n",
      "[Epoch 0/5] [Batch 614/938] [D loss: 0.5155500173568726] [G loss: 0.8760703802108765]\n",
      "[Epoch 0/5] [Batch 615/938] [D loss: 0.5017195343971252] [G loss: 1.3498941659927368]\n",
      "[Epoch 0/5] [Batch 616/938] [D loss: 0.457511305809021] [G loss: 1.0614985227584839]\n",
      "[Epoch 0/5] [Batch 617/938] [D loss: 0.5225077867507935] [G loss: 1.246685266494751]\n",
      "[Epoch 0/5] [Batch 618/938] [D loss: 0.5123927593231201] [G loss: 1.6440142393112183]\n",
      "[Epoch 0/5] [Batch 619/938] [D loss: 0.4797128140926361] [G loss: 0.9235761165618896]\n",
      "[Epoch 0/5] [Batch 620/938] [D loss: 0.5067713260650635] [G loss: 1.5990253686904907]\n",
      "[Epoch 0/5] [Batch 621/938] [D loss: 0.4705827236175537] [G loss: 1.155608057975769]\n",
      "[Epoch 0/5] [Batch 622/938] [D loss: 0.47906559705734253] [G loss: 1.1594940423965454]\n",
      "[Epoch 0/5] [Batch 623/938] [D loss: 0.5092098116874695] [G loss: 1.1994609832763672]\n",
      "[Epoch 0/5] [Batch 624/938] [D loss: 0.4947200119495392] [G loss: 1.2604663372039795]\n",
      "[Epoch 0/5] [Batch 625/938] [D loss: 0.5023412704467773] [G loss: 1.0786125659942627]\n",
      "[Epoch 0/5] [Batch 626/938] [D loss: 0.5882201194763184] [G loss: 1.477690577507019]\n",
      "[Epoch 0/5] [Batch 627/938] [D loss: 0.5396694540977478] [G loss: 0.8504458069801331]\n",
      "[Epoch 0/5] [Batch 628/938] [D loss: 0.535528838634491] [G loss: 1.8738839626312256]\n",
      "[Epoch 0/5] [Batch 629/938] [D loss: 0.5372274518013] [G loss: 0.7152223587036133]\n",
      "[Epoch 0/5] [Batch 630/938] [D loss: 0.5399513244628906] [G loss: 1.8693625926971436]\n",
      "[Epoch 0/5] [Batch 631/938] [D loss: 0.5486652851104736] [G loss: 0.9453136324882507]\n",
      "[Epoch 0/5] [Batch 632/938] [D loss: 0.5270064473152161] [G loss: 1.6147702932357788]\n",
      "[Epoch 0/5] [Batch 633/938] [D loss: 0.5380879640579224] [G loss: 0.885757565498352]\n",
      "[Epoch 0/5] [Batch 634/938] [D loss: 0.5467051863670349] [G loss: 1.2577528953552246]\n",
      "[Epoch 0/5] [Batch 635/938] [D loss: 0.4586332440376282] [G loss: 1.1846861839294434]\n",
      "[Epoch 0/5] [Batch 636/938] [D loss: 0.5195361375808716] [G loss: 1.2797164916992188]\n",
      "[Epoch 0/5] [Batch 637/938] [D loss: 0.4830232858657837] [G loss: 0.972455620765686]\n",
      "[Epoch 0/5] [Batch 638/938] [D loss: 0.4566148519515991] [G loss: 1.4487577676773071]\n",
      "[Epoch 0/5] [Batch 639/938] [D loss: 0.45594799518585205] [G loss: 1.2962015867233276]\n",
      "[Epoch 0/5] [Batch 640/938] [D loss: 0.48916178941726685] [G loss: 1.1581659317016602]\n",
      "[Epoch 0/5] [Batch 641/938] [D loss: 0.5847991704940796] [G loss: 1.6365866661071777]\n",
      "[Epoch 0/5] [Batch 642/938] [D loss: 0.5242947936058044] [G loss: 0.7608869671821594]\n",
      "[Epoch 0/5] [Batch 643/938] [D loss: 0.5415065884590149] [G loss: 1.9692668914794922]\n",
      "[Epoch 0/5] [Batch 644/938] [D loss: 0.6203107833862305] [G loss: 0.46822166442871094]\n",
      "[Epoch 0/5] [Batch 645/938] [D loss: 0.7227537035942078] [G loss: 2.201665163040161]\n",
      "[Epoch 0/5] [Batch 646/938] [D loss: 0.5800772309303284] [G loss: 0.7172095775604248]\n",
      "[Epoch 0/5] [Batch 647/938] [D loss: 0.4692551791667938] [G loss: 1.1824008226394653]\n",
      "[Epoch 0/5] [Batch 648/938] [D loss: 0.6102480292320251] [G loss: 1.8614007234573364]\n",
      "[Epoch 0/5] [Batch 649/938] [D loss: 0.5335733294487] [G loss: 0.725081741809845]\n",
      "[Epoch 0/5] [Batch 650/938] [D loss: 0.4867865741252899] [G loss: 1.17979097366333]\n",
      "[Epoch 0/5] [Batch 651/938] [D loss: 0.49746274948120117] [G loss: 1.7972769737243652]\n",
      "[Epoch 0/5] [Batch 652/938] [D loss: 0.5392141938209534] [G loss: 0.9225530624389648]\n",
      "[Epoch 0/5] [Batch 653/938] [D loss: 0.4548422396183014] [G loss: 1.1716585159301758]\n",
      "[Epoch 0/5] [Batch 654/938] [D loss: 0.5097066164016724] [G loss: 1.3312371969223022]\n",
      "[Epoch 0/5] [Batch 655/938] [D loss: 0.4694872200489044] [G loss: 1.183279275894165]\n",
      "[Epoch 0/5] [Batch 656/938] [D loss: 0.50003981590271] [G loss: 1.2058545351028442]\n",
      "[Epoch 0/5] [Batch 657/938] [D loss: 0.4335253834724426] [G loss: 1.3948171138763428]\n",
      "[Epoch 0/5] [Batch 658/938] [D loss: 0.5677486062049866] [G loss: 1.329646348953247]\n",
      "[Epoch 0/5] [Batch 659/938] [D loss: 0.4900296628475189] [G loss: 0.9814450740814209]\n",
      "[Epoch 0/5] [Batch 660/938] [D loss: 0.5925323963165283] [G loss: 1.9145870208740234]\n",
      "[Epoch 0/5] [Batch 661/938] [D loss: 0.7457613945007324] [G loss: 0.430610716342926]\n",
      "[Epoch 0/5] [Batch 662/938] [D loss: 0.6320911645889282] [G loss: 1.927478551864624]\n",
      "[Epoch 0/5] [Batch 663/938] [D loss: 0.5009766817092896] [G loss: 1.114019751548767]\n",
      "[Epoch 0/5] [Batch 664/938] [D loss: 0.4971003234386444] [G loss: 0.9789029955863953]\n",
      "[Epoch 0/5] [Batch 665/938] [D loss: 0.5082802176475525] [G loss: 1.4587979316711426]\n",
      "[Epoch 0/5] [Batch 666/938] [D loss: 0.4453829824924469] [G loss: 1.0594409704208374]\n",
      "[Epoch 0/5] [Batch 667/938] [D loss: 0.42560380697250366] [G loss: 1.1647794246673584]\n",
      "[Epoch 0/5] [Batch 668/938] [D loss: 0.4827331006526947] [G loss: 1.6206108331680298]\n",
      "[Epoch 0/5] [Batch 669/938] [D loss: 0.4149792790412903] [G loss: 1.1554012298583984]\n",
      "[Epoch 0/5] [Batch 670/938] [D loss: 0.4346269965171814] [G loss: 1.3872815370559692]\n",
      "[Epoch 0/5] [Batch 671/938] [D loss: 0.5392855405807495] [G loss: 1.0285587310791016]\n",
      "[Epoch 0/5] [Batch 672/938] [D loss: 0.4922284781932831] [G loss: 1.267276406288147]\n",
      "[Epoch 0/5] [Batch 673/938] [D loss: 0.500003457069397] [G loss: 0.8864845633506775]\n",
      "[Epoch 0/5] [Batch 674/938] [D loss: 0.4585683345794678] [G loss: 1.7200751304626465]\n",
      "[Epoch 0/5] [Batch 675/938] [D loss: 0.5153608918190002] [G loss: 0.983862042427063]\n",
      "[Epoch 0/5] [Batch 676/938] [D loss: 0.46808841824531555] [G loss: 1.7273019552230835]\n",
      "[Epoch 0/5] [Batch 677/938] [D loss: 0.5211973190307617] [G loss: 1.1121400594711304]\n",
      "[Epoch 0/5] [Batch 678/938] [D loss: 0.5300066471099854] [G loss: 1.0761445760726929]\n",
      "[Epoch 0/5] [Batch 679/938] [D loss: 0.5092509984970093] [G loss: 1.4026875495910645]\n",
      "[Epoch 0/5] [Batch 680/938] [D loss: 0.45027270913124084] [G loss: 1.0619401931762695]\n",
      "[Epoch 0/5] [Batch 681/938] [D loss: 0.44373375177383423] [G loss: 1.349300503730774]\n",
      "[Epoch 0/5] [Batch 682/938] [D loss: 0.48861801624298096] [G loss: 1.1669604778289795]\n",
      "[Epoch 0/5] [Batch 683/938] [D loss: 0.549638032913208] [G loss: 0.9435238242149353]\n",
      "[Epoch 0/5] [Batch 684/938] [D loss: 0.5796186923980713] [G loss: 1.6656948328018188]\n",
      "[Epoch 0/5] [Batch 685/938] [D loss: 0.5897548198699951] [G loss: 0.8606727123260498]\n",
      "[Epoch 0/5] [Batch 686/938] [D loss: 0.5533694624900818] [G loss: 1.610020399093628]\n",
      "[Epoch 0/5] [Batch 687/938] [D loss: 0.5201160907745361] [G loss: 0.90647292137146]\n",
      "[Epoch 0/5] [Batch 688/938] [D loss: 0.5737563967704773] [G loss: 1.3856030702590942]\n",
      "[Epoch 0/5] [Batch 689/938] [D loss: 0.5740719437599182] [G loss: 0.7494872212409973]\n",
      "[Epoch 0/5] [Batch 690/938] [D loss: 0.6361487507820129] [G loss: 2.0667724609375]\n",
      "[Epoch 0/5] [Batch 691/938] [D loss: 0.6113157272338867] [G loss: 0.6715556979179382]\n",
      "[Epoch 0/5] [Batch 692/938] [D loss: 0.4616909921169281] [G loss: 1.4888628721237183]\n",
      "[Epoch 0/5] [Batch 693/938] [D loss: 0.4782882332801819] [G loss: 1.4240303039550781]\n",
      "[Epoch 0/5] [Batch 694/938] [D loss: 0.4730750322341919] [G loss: 0.8789440393447876]\n",
      "[Epoch 0/5] [Batch 695/938] [D loss: 0.5036904811859131] [G loss: 1.4851500988006592]\n",
      "[Epoch 0/5] [Batch 696/938] [D loss: 0.4893306493759155] [G loss: 0.9545929431915283]\n",
      "[Epoch 0/5] [Batch 697/938] [D loss: 0.4720582067966461] [G loss: 1.4917854070663452]\n",
      "[Epoch 0/5] [Batch 698/938] [D loss: 0.4813980758190155] [G loss: 1.0356143712997437]\n",
      "[Epoch 0/5] [Batch 699/938] [D loss: 0.4500674903392792] [G loss: 1.2615995407104492]\n",
      "[Epoch 0/5] [Batch 700/938] [D loss: 0.5079561471939087] [G loss: 1.7439626455307007]\n",
      "[Epoch 0/5] [Batch 701/938] [D loss: 0.5050220489501953] [G loss: 0.8673964738845825]\n",
      "[Epoch 0/5] [Batch 702/938] [D loss: 0.5194867849349976] [G loss: 1.7286624908447266]\n",
      "[Epoch 0/5] [Batch 703/938] [D loss: 0.4520299434661865] [G loss: 0.8918777704238892]\n",
      "[Epoch 0/5] [Batch 704/938] [D loss: 0.45959576964378357] [G loss: 1.525075912475586]\n",
      "[Epoch 0/5] [Batch 705/938] [D loss: 0.5039247274398804] [G loss: 1.3114019632339478]\n",
      "[Epoch 0/5] [Batch 706/938] [D loss: 0.4656229019165039] [G loss: 1.0766948461532593]\n",
      "[Epoch 0/5] [Batch 707/938] [D loss: 0.45638859272003174] [G loss: 1.4261679649353027]\n",
      "[Epoch 0/5] [Batch 708/938] [D loss: 0.5731619596481323] [G loss: 1.0304359197616577]\n",
      "[Epoch 0/5] [Batch 709/938] [D loss: 0.4250631332397461] [G loss: 1.2106642723083496]\n",
      "[Epoch 0/5] [Batch 710/938] [D loss: 0.4627046585083008] [G loss: 1.0270086526870728]\n",
      "[Epoch 0/5] [Batch 711/938] [D loss: 0.45145756006240845] [G loss: 1.7649083137512207]\n",
      "[Epoch 0/5] [Batch 712/938] [D loss: 0.44828006625175476] [G loss: 0.9983476996421814]\n",
      "[Epoch 0/5] [Batch 713/938] [D loss: 0.618594229221344] [G loss: 1.9084515571594238]\n",
      "[Epoch 0/5] [Batch 714/938] [D loss: 0.6448891162872314] [G loss: 0.6590875387191772]\n",
      "[Epoch 0/5] [Batch 715/938] [D loss: 0.6695107221603394] [G loss: 2.335613489151001]\n",
      "[Epoch 0/5] [Batch 716/938] [D loss: 0.7178100347518921] [G loss: 0.5094441175460815]\n",
      "[Epoch 0/5] [Batch 717/938] [D loss: 0.48934006690979004] [G loss: 1.801623821258545]\n",
      "[Epoch 0/5] [Batch 718/938] [D loss: 0.517235279083252] [G loss: 1.2537944316864014]\n",
      "[Epoch 0/5] [Batch 719/938] [D loss: 0.5028263330459595] [G loss: 0.8402319550514221]\n",
      "[Epoch 0/5] [Batch 720/938] [D loss: 0.47140538692474365] [G loss: 1.9290482997894287]\n",
      "[Epoch 0/5] [Batch 721/938] [D loss: 0.5163595080375671] [G loss: 1.0773646831512451]\n",
      "[Epoch 0/5] [Batch 722/938] [D loss: 0.5711880922317505] [G loss: 1.202885627746582]\n",
      "[Epoch 0/5] [Batch 723/938] [D loss: 0.48246216773986816] [G loss: 1.4224457740783691]\n",
      "[Epoch 0/5] [Batch 724/938] [D loss: 0.4778120219707489] [G loss: 1.3777999877929688]\n",
      "[Epoch 0/5] [Batch 725/938] [D loss: 0.5008604526519775] [G loss: 0.9801363348960876]\n",
      "[Epoch 0/5] [Batch 726/938] [D loss: 0.4786679446697235] [G loss: 1.6940464973449707]\n",
      "[Epoch 0/5] [Batch 727/938] [D loss: 0.5405060648918152] [G loss: 0.7482640743255615]\n",
      "[Epoch 0/5] [Batch 728/938] [D loss: 0.5013751983642578] [G loss: 1.7010911703109741]\n",
      "[Epoch 0/5] [Batch 729/938] [D loss: 0.5703815817832947] [G loss: 0.9592130184173584]\n",
      "[Epoch 0/5] [Batch 730/938] [D loss: 0.4375186562538147] [G loss: 1.177626609802246]\n",
      "[Epoch 0/5] [Batch 731/938] [D loss: 0.42937955260276794] [G loss: 1.540107011795044]\n",
      "[Epoch 0/5] [Batch 732/938] [D loss: 0.4762863516807556] [G loss: 1.5865455865859985]\n",
      "[Epoch 0/5] [Batch 733/938] [D loss: 0.5361515283584595] [G loss: 1.0220756530761719]\n",
      "[Epoch 0/5] [Batch 734/938] [D loss: 0.5089668035507202] [G loss: 1.3228833675384521]\n",
      "[Epoch 0/5] [Batch 735/938] [D loss: 0.48358339071273804] [G loss: 1.4015415906906128]\n",
      "[Epoch 0/5] [Batch 736/938] [D loss: 0.4925510883331299] [G loss: 1.0178141593933105]\n",
      "[Epoch 0/5] [Batch 737/938] [D loss: 0.6136512160301208] [G loss: 1.8177552223205566]\n",
      "[Epoch 0/5] [Batch 738/938] [D loss: 0.6252325177192688] [G loss: 0.5697712898254395]\n",
      "[Epoch 0/5] [Batch 739/938] [D loss: 0.5131611824035645] [G loss: 1.8357737064361572]\n",
      "[Epoch 0/5] [Batch 740/938] [D loss: 0.45735692977905273] [G loss: 1.2702323198318481]\n",
      "[Epoch 0/5] [Batch 741/938] [D loss: 0.48972079157829285] [G loss: 1.0965197086334229]\n",
      "[Epoch 0/5] [Batch 742/938] [D loss: 0.5281477570533752] [G loss: 1.5820214748382568]\n",
      "[Epoch 0/5] [Batch 743/938] [D loss: 0.5462944507598877] [G loss: 0.8569888472557068]\n",
      "[Epoch 0/5] [Batch 744/938] [D loss: 0.43818604946136475] [G loss: 1.4863959550857544]\n",
      "[Epoch 0/5] [Batch 745/938] [D loss: 0.4448404312133789] [G loss: 1.2726539373397827]\n",
      "[Epoch 0/5] [Batch 746/938] [D loss: 0.5155629515647888] [G loss: 1.1636884212493896]\n",
      "[Epoch 0/5] [Batch 747/938] [D loss: 0.38936692476272583] [G loss: 1.3822699785232544]\n",
      "[Epoch 0/5] [Batch 748/938] [D loss: 0.536196231842041] [G loss: 1.4178502559661865]\n",
      "[Epoch 0/5] [Batch 749/938] [D loss: 0.5063366293907166] [G loss: 0.912126362323761]\n",
      "[Epoch 0/5] [Batch 750/938] [D loss: 0.5023552179336548] [G loss: 1.4614307880401611]\n",
      "[Epoch 0/5] [Batch 751/938] [D loss: 0.5419021844863892] [G loss: 0.9541486501693726]\n",
      "[Epoch 0/5] [Batch 752/938] [D loss: 0.5211302042007446] [G loss: 1.1718252897262573]\n",
      "[Epoch 0/5] [Batch 753/938] [D loss: 0.5018349885940552] [G loss: 1.6233558654785156]\n",
      "[Epoch 0/5] [Batch 754/938] [D loss: 0.544933021068573] [G loss: 0.843740701675415]\n",
      "[Epoch 0/5] [Batch 755/938] [D loss: 0.4996618628501892] [G loss: 2.1758313179016113]\n",
      "[Epoch 0/5] [Batch 756/938] [D loss: 0.5687941908836365] [G loss: 0.6865019798278809]\n",
      "[Epoch 0/5] [Batch 757/938] [D loss: 0.6774418354034424] [G loss: 2.258793830871582]\n",
      "[Epoch 0/5] [Batch 758/938] [D loss: 0.6243869662284851] [G loss: 0.6209996938705444]\n",
      "[Epoch 0/5] [Batch 759/938] [D loss: 0.47318798303604126] [G loss: 1.7437045574188232]\n",
      "[Epoch 0/5] [Batch 760/938] [D loss: 0.5056749582290649] [G loss: 1.3413209915161133]\n",
      "[Epoch 0/5] [Batch 761/938] [D loss: 0.5363037586212158] [G loss: 0.8670469522476196]\n",
      "[Epoch 0/5] [Batch 762/938] [D loss: 0.49848616123199463] [G loss: 1.3200429677963257]\n",
      "[Epoch 0/5] [Batch 763/938] [D loss: 0.508223831653595] [G loss: 1.0342532396316528]\n",
      "[Epoch 0/5] [Batch 764/938] [D loss: 0.5162313580513] [G loss: 1.271036148071289]\n",
      "[Epoch 0/5] [Batch 765/938] [D loss: 0.4506303668022156] [G loss: 1.0465099811553955]\n",
      "[Epoch 0/5] [Batch 766/938] [D loss: 0.4912075102329254] [G loss: 1.6989810466766357]\n",
      "[Epoch 0/5] [Batch 767/938] [D loss: 0.4656454920768738] [G loss: 0.9696648716926575]\n",
      "[Epoch 0/5] [Batch 768/938] [D loss: 0.5251573920249939] [G loss: 1.672257900238037]\n",
      "[Epoch 0/5] [Batch 769/938] [D loss: 0.58017897605896] [G loss: 0.6820107102394104]\n",
      "[Epoch 0/5] [Batch 770/938] [D loss: 0.6424161195755005] [G loss: 2.0043742656707764]\n",
      "[Epoch 0/5] [Batch 771/938] [D loss: 0.6857002973556519] [G loss: 0.5419574975967407]\n",
      "[Epoch 0/5] [Batch 772/938] [D loss: 0.556535005569458] [G loss: 1.6413183212280273]\n",
      "[Epoch 0/5] [Batch 773/938] [D loss: 0.4584566652774811] [G loss: 1.2224215269088745]\n",
      "[Epoch 0/5] [Batch 774/938] [D loss: 0.5961421132087708] [G loss: 0.5866363048553467]\n",
      "[Epoch 0/5] [Batch 775/938] [D loss: 0.5416046380996704] [G loss: 1.6379482746124268]\n",
      "[Epoch 0/5] [Batch 776/938] [D loss: 0.46684324741363525] [G loss: 1.2037417888641357]\n",
      "[Epoch 0/5] [Batch 777/938] [D loss: 0.510776162147522] [G loss: 0.8024290204048157]\n",
      "[Epoch 0/5] [Batch 778/938] [D loss: 0.55193030834198] [G loss: 1.9532746076583862]\n",
      "[Epoch 0/5] [Batch 779/938] [D loss: 0.5473498106002808] [G loss: 0.6992321610450745]\n",
      "[Epoch 0/5] [Batch 780/938] [D loss: 0.475383996963501] [G loss: 1.2630996704101562]\n",
      "[Epoch 0/5] [Batch 781/938] [D loss: 0.5206276178359985] [G loss: 1.5567091703414917]\n",
      "[Epoch 0/5] [Batch 782/938] [D loss: 0.4499281644821167] [G loss: 0.9997310042381287]\n",
      "[Epoch 0/5] [Batch 783/938] [D loss: 0.4668251872062683] [G loss: 1.1774909496307373]\n",
      "[Epoch 0/5] [Batch 784/938] [D loss: 0.45486587285995483] [G loss: 1.550050973892212]\n",
      "[Epoch 0/5] [Batch 785/938] [D loss: 0.42497095465660095] [G loss: 1.3481135368347168]\n",
      "[Epoch 0/5] [Batch 786/938] [D loss: 0.45578229427337646] [G loss: 1.040032982826233]\n",
      "[Epoch 0/5] [Batch 787/938] [D loss: 0.48373550176620483] [G loss: 1.4247182607650757]\n",
      "[Epoch 0/5] [Batch 788/938] [D loss: 0.5182024836540222] [G loss: 0.8512871265411377]\n",
      "[Epoch 0/5] [Batch 789/938] [D loss: 0.6083136200904846] [G loss: 2.235816717147827]\n",
      "[Epoch 0/5] [Batch 790/938] [D loss: 0.6461179256439209] [G loss: 0.6406387686729431]\n",
      "[Epoch 0/5] [Batch 791/938] [D loss: 0.5521951913833618] [G loss: 1.7082561254501343]\n",
      "[Epoch 0/5] [Batch 792/938] [D loss: 0.4504373073577881] [G loss: 1.3035290241241455]\n",
      "[Epoch 0/5] [Batch 793/938] [D loss: 0.48559218645095825] [G loss: 1.0750855207443237]\n",
      "[Epoch 0/5] [Batch 794/938] [D loss: 0.5881415605545044] [G loss: 1.7152178287506104]\n",
      "[Epoch 0/5] [Batch 795/938] [D loss: 0.46719133853912354] [G loss: 1.108229398727417]\n",
      "[Epoch 0/5] [Batch 796/938] [D loss: 0.4542366862297058] [G loss: 1.1279222965240479]\n",
      "[Epoch 0/5] [Batch 797/938] [D loss: 0.48798424005508423] [G loss: 1.7869212627410889]\n",
      "[Epoch 0/5] [Batch 798/938] [D loss: 0.47877758741378784] [G loss: 1.0062495470046997]\n",
      "[Epoch 0/5] [Batch 799/938] [D loss: 0.5192474126815796] [G loss: 1.4051895141601562]\n",
      "[Epoch 0/5] [Batch 800/938] [D loss: 0.4668760895729065] [G loss: 1.5105239152908325]\n",
      "[Epoch 0/5] [Batch 801/938] [D loss: 0.5071715116500854] [G loss: 1.242793321609497]\n",
      "[Epoch 0/5] [Batch 802/938] [D loss: 0.43538713455200195] [G loss: 1.370172381401062]\n",
      "[Epoch 0/5] [Batch 803/938] [D loss: 0.47929251194000244] [G loss: 1.1868114471435547]\n",
      "[Epoch 0/5] [Batch 804/938] [D loss: 0.5941994190216064] [G loss: 1.5378482341766357]\n",
      "[Epoch 0/5] [Batch 805/938] [D loss: 0.5235859751701355] [G loss: 0.8762024641036987]\n",
      "[Epoch 0/5] [Batch 806/938] [D loss: 0.5794017314910889] [G loss: 1.7605650424957275]\n",
      "[Epoch 0/5] [Batch 807/938] [D loss: 0.6007425785064697] [G loss: 0.6146125793457031]\n",
      "[Epoch 0/5] [Batch 808/938] [D loss: 0.6339624524116516] [G loss: 1.9573472738265991]\n",
      "[Epoch 0/5] [Batch 809/938] [D loss: 0.5483015775680542] [G loss: 0.8345766067504883]\n",
      "[Epoch 0/5] [Batch 810/938] [D loss: 0.5731997489929199] [G loss: 1.3303707838058472]\n",
      "[Epoch 0/5] [Batch 811/938] [D loss: 0.5385620594024658] [G loss: 1.295964241027832]\n",
      "[Epoch 0/5] [Batch 812/938] [D loss: 0.49786949157714844] [G loss: 1.061976432800293]\n",
      "[Epoch 0/5] [Batch 813/938] [D loss: 0.5113703012466431] [G loss: 1.3274422883987427]\n",
      "[Epoch 0/5] [Batch 814/938] [D loss: 0.475980281829834] [G loss: 1.0152384042739868]\n",
      "[Epoch 0/5] [Batch 815/938] [D loss: 0.6030675768852234] [G loss: 1.643393635749817]\n",
      "[Epoch 0/5] [Batch 816/938] [D loss: 0.643223762512207] [G loss: 0.5264251828193665]\n",
      "[Epoch 0/5] [Batch 817/938] [D loss: 0.6446201205253601] [G loss: 2.0747199058532715]\n",
      "[Epoch 0/5] [Batch 818/938] [D loss: 0.4897519052028656] [G loss: 0.8978495597839355]\n",
      "[Epoch 0/5] [Batch 819/938] [D loss: 0.46575623750686646] [G loss: 1.1183048486709595]\n",
      "[Epoch 0/5] [Batch 820/938] [D loss: 0.5530657768249512] [G loss: 1.6107351779937744]\n",
      "[Epoch 0/5] [Batch 821/938] [D loss: 0.4931202828884125] [G loss: 0.9512900710105896]\n",
      "[Epoch 0/5] [Batch 822/938] [D loss: 0.4665115177631378] [G loss: 1.1131564378738403]\n",
      "[Epoch 0/5] [Batch 823/938] [D loss: 0.4351085424423218] [G loss: 1.514838695526123]\n",
      "[Epoch 0/5] [Batch 824/938] [D loss: 0.4577391743659973] [G loss: 1.1005455255508423]\n",
      "[Epoch 0/5] [Batch 825/938] [D loss: 0.45217210054397583] [G loss: 1.450387954711914]\n",
      "[Epoch 0/5] [Batch 826/938] [D loss: 0.4350522756576538] [G loss: 1.2563570737838745]\n",
      "[Epoch 0/5] [Batch 827/938] [D loss: 0.4665268063545227] [G loss: 1.064110517501831]\n",
      "[Epoch 0/5] [Batch 828/938] [D loss: 0.461095929145813] [G loss: 1.236778974533081]\n",
      "[Epoch 0/5] [Batch 829/938] [D loss: 0.5240288972854614] [G loss: 1.1453272104263306]\n",
      "[Epoch 0/5] [Batch 830/938] [D loss: 0.43640977144241333] [G loss: 1.5076849460601807]\n",
      "[Epoch 0/5] [Batch 831/938] [D loss: 0.5433825254440308] [G loss: 0.7797603607177734]\n",
      "[Epoch 0/5] [Batch 832/938] [D loss: 0.7759197354316711] [G loss: 2.528172492980957]\n",
      "[Epoch 0/5] [Batch 833/938] [D loss: 0.9640538692474365] [G loss: 0.23085683584213257]\n",
      "[Epoch 0/5] [Batch 834/938] [D loss: 0.5322257876396179] [G loss: 1.5280649662017822]\n",
      "[Epoch 0/5] [Batch 835/938] [D loss: 0.5170083045959473] [G loss: 1.756568193435669]\n",
      "[Epoch 0/5] [Batch 836/938] [D loss: 0.5177866220474243] [G loss: 0.9348686933517456]\n",
      "[Epoch 0/5] [Batch 837/938] [D loss: 0.5833812355995178] [G loss: 0.8759781122207642]\n",
      "[Epoch 0/5] [Batch 838/938] [D loss: 0.518537163734436] [G loss: 1.3445779085159302]\n",
      "[Epoch 0/5] [Batch 839/938] [D loss: 0.45056024193763733] [G loss: 1.4414182901382446]\n",
      "[Epoch 0/5] [Batch 840/938] [D loss: 0.4794640839099884] [G loss: 0.9091813564300537]\n",
      "[Epoch 0/5] [Batch 841/938] [D loss: 0.5019651651382446] [G loss: 1.1901047229766846]\n",
      "[Epoch 0/5] [Batch 842/938] [D loss: 0.5502779483795166] [G loss: 1.6132049560546875]\n",
      "[Epoch 0/5] [Batch 843/938] [D loss: 0.5353837013244629] [G loss: 0.7365766167640686]\n",
      "[Epoch 0/5] [Batch 844/938] [D loss: 0.4977266192436218] [G loss: 1.349133849143982]\n",
      "[Epoch 0/5] [Batch 845/938] [D loss: 0.473420113325119] [G loss: 1.227142095565796]\n",
      "[Epoch 0/5] [Batch 846/938] [D loss: 0.47317028045654297] [G loss: 1.2983489036560059]\n",
      "[Epoch 0/5] [Batch 847/938] [D loss: 0.5480722188949585] [G loss: 1.157863736152649]\n",
      "[Epoch 0/5] [Batch 848/938] [D loss: 0.5606265068054199] [G loss: 1.0781211853027344]\n",
      "[Epoch 0/5] [Batch 849/938] [D loss: 0.43891477584838867] [G loss: 1.0450600385665894]\n",
      "[Epoch 0/5] [Batch 850/938] [D loss: 0.562264621257782] [G loss: 1.5740383863449097]\n",
      "[Epoch 0/5] [Batch 851/938] [D loss: 0.5486233830451965] [G loss: 0.7815361022949219]\n",
      "[Epoch 0/5] [Batch 852/938] [D loss: 0.5646324157714844] [G loss: 1.876428246498108]\n",
      "[Epoch 0/5] [Batch 853/938] [D loss: 0.5985821485519409] [G loss: 0.624093770980835]\n",
      "[Epoch 0/5] [Batch 854/938] [D loss: 0.5972762107849121] [G loss: 1.5748796463012695]\n",
      "[Epoch 0/5] [Batch 855/938] [D loss: 0.48760372400283813] [G loss: 0.9967124462127686]\n",
      "[Epoch 0/5] [Batch 856/938] [D loss: 0.5344032645225525] [G loss: 1.480175256729126]\n",
      "[Epoch 0/5] [Batch 857/938] [D loss: 0.45813804864883423] [G loss: 1.0813716650009155]\n",
      "[Epoch 0/5] [Batch 858/938] [D loss: 0.4881555438041687] [G loss: 1.2565507888793945]\n",
      "[Epoch 0/5] [Batch 859/938] [D loss: 0.5456523895263672] [G loss: 1.1266623735427856]\n",
      "[Epoch 0/5] [Batch 860/938] [D loss: 0.5512223243713379] [G loss: 1.048149824142456]\n",
      "[Epoch 0/5] [Batch 861/938] [D loss: 0.5439020991325378] [G loss: 1.2646690607070923]\n",
      "[Epoch 0/5] [Batch 862/938] [D loss: 0.5557706356048584] [G loss: 1.182229995727539]\n",
      "[Epoch 0/5] [Batch 863/938] [D loss: 0.5240681767463684] [G loss: 1.2684571743011475]\n",
      "[Epoch 0/5] [Batch 864/938] [D loss: 0.5078778862953186] [G loss: 1.2034777402877808]\n",
      "[Epoch 0/5] [Batch 865/938] [D loss: 0.48081174492836] [G loss: 1.3239266872406006]\n",
      "[Epoch 0/5] [Batch 866/938] [D loss: 0.5001205205917358] [G loss: 0.9457772970199585]\n",
      "[Epoch 0/5] [Batch 867/938] [D loss: 0.49941176176071167] [G loss: 1.2966175079345703]\n",
      "[Epoch 0/5] [Batch 868/938] [D loss: 0.5513094067573547] [G loss: 0.7693808078765869]\n",
      "[Epoch 0/5] [Batch 869/938] [D loss: 0.6994434595108032] [G loss: 2.6453847885131836]\n",
      "[Epoch 0/5] [Batch 870/938] [D loss: 0.8273695707321167] [G loss: 0.386575311422348]\n",
      "[Epoch 0/5] [Batch 871/938] [D loss: 0.5568686127662659] [G loss: 1.9391319751739502]\n",
      "[Epoch 0/5] [Batch 872/938] [D loss: 0.5318289995193481] [G loss: 1.1059818267822266]\n",
      "[Epoch 0/5] [Batch 873/938] [D loss: 0.5306708216667175] [G loss: 1.1899336576461792]\n",
      "[Epoch 0/5] [Batch 874/938] [D loss: 0.5506013631820679] [G loss: 1.2645028829574585]\n",
      "[Epoch 0/5] [Batch 875/938] [D loss: 0.474857896566391] [G loss: 1.068153977394104]\n",
      "[Epoch 0/5] [Batch 876/938] [D loss: 0.4616711139678955] [G loss: 1.4490687847137451]\n",
      "[Epoch 0/5] [Batch 877/938] [D loss: 0.4724775552749634] [G loss: 0.9579975605010986]\n",
      "[Epoch 0/5] [Batch 878/938] [D loss: 0.4618653953075409] [G loss: 1.2648608684539795]\n",
      "[Epoch 0/5] [Batch 879/938] [D loss: 0.5550248622894287] [G loss: 1.2365810871124268]\n",
      "[Epoch 0/5] [Batch 880/938] [D loss: 0.44979003071784973] [G loss: 1.1451166868209839]\n",
      "[Epoch 0/5] [Batch 881/938] [D loss: 0.4553830027580261] [G loss: 1.406461238861084]\n",
      "[Epoch 0/5] [Batch 882/938] [D loss: 0.5028639435768127] [G loss: 1.1241976022720337]\n",
      "[Epoch 0/5] [Batch 883/938] [D loss: 0.5445458889007568] [G loss: 1.4759430885314941]\n",
      "[Epoch 0/5] [Batch 884/938] [D loss: 0.5768047571182251] [G loss: 0.8455027937889099]\n",
      "[Epoch 0/5] [Batch 885/938] [D loss: 0.43497416377067566] [G loss: 1.8345292806625366]\n",
      "[Epoch 0/5] [Batch 886/938] [D loss: 0.5454369187355042] [G loss: 0.8457726836204529]\n",
      "[Epoch 0/5] [Batch 887/938] [D loss: 0.5154378414154053] [G loss: 1.8695957660675049]\n",
      "[Epoch 0/5] [Batch 888/938] [D loss: 0.47518888115882874] [G loss: 0.9510146975517273]\n",
      "[Epoch 0/5] [Batch 889/938] [D loss: 0.5093775987625122] [G loss: 1.2546906471252441]\n",
      "[Epoch 0/5] [Batch 890/938] [D loss: 0.5366988182067871] [G loss: 1.6091821193695068]\n",
      "[Epoch 0/5] [Batch 891/938] [D loss: 0.5655457973480225] [G loss: 0.7904981374740601]\n",
      "[Epoch 0/5] [Batch 892/938] [D loss: 0.5503974556922913] [G loss: 1.960864543914795]\n",
      "[Epoch 0/5] [Batch 893/938] [D loss: 0.5216593742370605] [G loss: 0.78000408411026]\n",
      "[Epoch 0/5] [Batch 894/938] [D loss: 0.5681508779525757] [G loss: 1.4887102842330933]\n",
      "[Epoch 0/5] [Batch 895/938] [D loss: 0.4751428961753845] [G loss: 1.240736484527588]\n",
      "[Epoch 0/5] [Batch 896/938] [D loss: 0.5400854349136353] [G loss: 1.2264125347137451]\n",
      "[Epoch 0/5] [Batch 897/938] [D loss: 0.5059816837310791] [G loss: 1.2540761232376099]\n",
      "[Epoch 0/5] [Batch 898/938] [D loss: 0.4662182033061981] [G loss: 1.3759753704071045]\n",
      "[Epoch 0/5] [Batch 899/938] [D loss: 0.5084670782089233] [G loss: 1.048883080482483]\n",
      "[Epoch 0/5] [Batch 900/938] [D loss: 0.4621127247810364] [G loss: 1.119441270828247]\n",
      "[Epoch 0/5] [Batch 901/938] [D loss: 0.47974270582199097] [G loss: 1.4490605592727661]\n",
      "[Epoch 0/5] [Batch 902/938] [D loss: 0.5261649489402771] [G loss: 0.8181266188621521]\n",
      "[Epoch 0/5] [Batch 903/938] [D loss: 0.6158123016357422] [G loss: 2.2950525283813477]\n",
      "[Epoch 0/5] [Batch 904/938] [D loss: 0.6241891980171204] [G loss: 0.5164616107940674]\n",
      "[Epoch 0/5] [Batch 905/938] [D loss: 0.5053827166557312] [G loss: 1.712057113647461]\n",
      "[Epoch 0/5] [Batch 906/938] [D loss: 0.4348238706588745] [G loss: 1.6627728939056396]\n",
      "[Epoch 0/5] [Batch 907/938] [D loss: 0.5282743573188782] [G loss: 1.035301923751831]\n",
      "[Epoch 0/5] [Batch 908/938] [D loss: 0.4470181167125702] [G loss: 1.6055446863174438]\n",
      "[Epoch 0/5] [Batch 909/938] [D loss: 0.497145414352417] [G loss: 1.1367076635360718]\n",
      "[Epoch 0/5] [Batch 910/938] [D loss: 0.49124574661254883] [G loss: 1.3372728824615479]\n",
      "[Epoch 0/5] [Batch 911/938] [D loss: 0.5132626295089722] [G loss: 1.1583938598632812]\n",
      "[Epoch 0/5] [Batch 912/938] [D loss: 0.47611868381500244] [G loss: 1.0460662841796875]\n",
      "[Epoch 0/5] [Batch 913/938] [D loss: 0.5608799457550049] [G loss: 2.1350395679473877]\n",
      "[Epoch 0/5] [Batch 914/938] [D loss: 0.6355535984039307] [G loss: 0.5389696955680847]\n",
      "[Epoch 0/5] [Batch 915/938] [D loss: 0.5809662342071533] [G loss: 1.8650288581848145]\n",
      "[Epoch 0/5] [Batch 916/938] [D loss: 0.525238573551178] [G loss: 0.7513346672058105]\n",
      "[Epoch 0/5] [Batch 917/938] [D loss: 0.4641037881374359] [G loss: 1.3226438760757446]\n",
      "[Epoch 0/5] [Batch 918/938] [D loss: 0.5867601037025452] [G loss: 1.6964415311813354]\n",
      "[Epoch 0/5] [Batch 919/938] [D loss: 0.6203725934028625] [G loss: 0.5570524334907532]\n",
      "[Epoch 0/5] [Batch 920/938] [D loss: 0.5638910531997681] [G loss: 1.4298657178878784]\n",
      "[Epoch 0/5] [Batch 921/938] [D loss: 0.4379831552505493] [G loss: 1.3208165168762207]\n",
      "[Epoch 0/5] [Batch 922/938] [D loss: 0.5547043681144714] [G loss: 1.1971065998077393]\n",
      "[Epoch 0/5] [Batch 923/938] [D loss: 0.5222426056861877] [G loss: 1.419377088546753]\n",
      "[Epoch 0/5] [Batch 924/938] [D loss: 0.5212322473526001] [G loss: 1.0692757368087769]\n",
      "[Epoch 0/5] [Batch 925/938] [D loss: 0.46532195806503296] [G loss: 1.3129914999008179]\n",
      "[Epoch 0/5] [Batch 926/938] [D loss: 0.5179509520530701] [G loss: 1.4316056966781616]\n",
      "[Epoch 0/5] [Batch 927/938] [D loss: 0.540661096572876] [G loss: 0.8929660320281982]\n",
      "[Epoch 0/5] [Batch 928/938] [D loss: 0.48816102743148804] [G loss: 1.3529601097106934]\n",
      "[Epoch 0/5] [Batch 929/938] [D loss: 0.5682049989700317] [G loss: 1.339972972869873]\n",
      "[Epoch 0/5] [Batch 930/938] [D loss: 0.6238704919815063] [G loss: 0.6272343397140503]\n",
      "[Epoch 0/5] [Batch 931/938] [D loss: 0.6504595875740051] [G loss: 2.3856101036071777]\n",
      "[Epoch 0/5] [Batch 932/938] [D loss: 0.5828424692153931] [G loss: 0.6978244781494141]\n",
      "[Epoch 0/5] [Batch 933/938] [D loss: 0.5396069288253784] [G loss: 1.2368899583816528]\n",
      "[Epoch 0/5] [Batch 934/938] [D loss: 0.476249635219574] [G loss: 1.4327752590179443]\n",
      "[Epoch 0/5] [Batch 935/938] [D loss: 0.558580219745636] [G loss: 0.9060407876968384]\n",
      "[Epoch 0/5] [Batch 936/938] [D loss: 0.45517295598983765] [G loss: 1.4948766231536865]\n",
      "[Epoch 0/5] [Batch 937/938] [D loss: 0.565346360206604] [G loss: 1.2391449213027954]\n",
      "[Epoch 1/5] [Batch 0/938] [D loss: 0.4638708233833313] [G loss: 1.1509654521942139]\n",
      "[Epoch 1/5] [Batch 1/938] [D loss: 0.43700459599494934] [G loss: 1.0871952772140503]\n",
      "[Epoch 1/5] [Batch 2/938] [D loss: 0.5084353685379028] [G loss: 1.1727968454360962]\n",
      "[Epoch 1/5] [Batch 3/938] [D loss: 0.46500372886657715] [G loss: 1.3418354988098145]\n",
      "[Epoch 1/5] [Batch 4/938] [D loss: 0.4763393998146057] [G loss: 1.1762844324111938]\n",
      "[Epoch 1/5] [Batch 5/938] [D loss: 0.5793255567550659] [G loss: 1.645155429840088]\n",
      "[Epoch 1/5] [Batch 6/938] [D loss: 0.6725964546203613] [G loss: 0.7155184745788574]\n",
      "[Epoch 1/5] [Batch 7/938] [D loss: 0.6248962879180908] [G loss: 2.195080280303955]\n",
      "[Epoch 1/5] [Batch 8/938] [D loss: 0.5663338899612427] [G loss: 0.6567243337631226]\n",
      "[Epoch 1/5] [Batch 9/938] [D loss: 0.4872630536556244] [G loss: 1.238684892654419]\n",
      "[Epoch 1/5] [Batch 10/938] [D loss: 0.5287237167358398] [G loss: 1.4926362037658691]\n",
      "[Epoch 1/5] [Batch 11/938] [D loss: 0.47670942544937134] [G loss: 0.9260106086730957]\n",
      "[Epoch 1/5] [Batch 12/938] [D loss: 0.5562214255332947] [G loss: 1.7441351413726807]\n",
      "[Epoch 1/5] [Batch 13/938] [D loss: 0.48583605885505676] [G loss: 0.8135756850242615]\n",
      "[Epoch 1/5] [Batch 14/938] [D loss: 0.4443272352218628] [G loss: 1.4996461868286133]\n",
      "[Epoch 1/5] [Batch 15/938] [D loss: 0.4684574007987976] [G loss: 1.337355136871338]\n",
      "[Epoch 1/5] [Batch 16/938] [D loss: 0.5330836176872253] [G loss: 1.2403955459594727]\n",
      "[Epoch 1/5] [Batch 17/938] [D loss: 0.5547268390655518] [G loss: 0.8571625351905823]\n",
      "[Epoch 1/5] [Batch 18/938] [D loss: 0.5433298349380493] [G loss: 1.9375958442687988]\n",
      "[Epoch 1/5] [Batch 19/938] [D loss: 0.5575640201568604] [G loss: 0.6419082880020142]\n",
      "[Epoch 1/5] [Batch 20/938] [D loss: 0.4300881028175354] [G loss: 1.5337998867034912]\n",
      "[Epoch 1/5] [Batch 21/938] [D loss: 0.44330689311027527] [G loss: 1.5406321287155151]\n",
      "[Epoch 1/5] [Batch 22/938] [D loss: 0.5474652051925659] [G loss: 1.0898710489273071]\n",
      "[Epoch 1/5] [Batch 23/938] [D loss: 0.5333763360977173] [G loss: 1.7405357360839844]\n",
      "[Epoch 1/5] [Batch 24/938] [D loss: 0.5381865501403809] [G loss: 0.9149143695831299]\n",
      "[Epoch 1/5] [Batch 25/938] [D loss: 0.4799901247024536] [G loss: 1.6115390062332153]\n",
      "[Epoch 1/5] [Batch 26/938] [D loss: 0.47777146100997925] [G loss: 1.3811386823654175]\n",
      "[Epoch 1/5] [Batch 27/938] [D loss: 0.5672433972358704] [G loss: 1.026735782623291]\n",
      "[Epoch 1/5] [Batch 28/938] [D loss: 0.46203047037124634] [G loss: 1.5777968168258667]\n",
      "[Epoch 1/5] [Batch 29/938] [D loss: 0.4776217043399811] [G loss: 1.3841887712478638]\n",
      "[Epoch 1/5] [Batch 30/938] [D loss: 0.4982932209968567] [G loss: 1.1362113952636719]\n",
      "[Epoch 1/5] [Batch 31/938] [D loss: 0.5629104375839233] [G loss: 1.6067860126495361]\n",
      "[Epoch 1/5] [Batch 32/938] [D loss: 0.6974621415138245] [G loss: 0.5516789555549622]\n",
      "[Epoch 1/5] [Batch 33/938] [D loss: 0.6585827469825745] [G loss: 2.334242582321167]\n",
      "[Epoch 1/5] [Batch 34/938] [D loss: 0.5541494488716125] [G loss: 0.7777434587478638]\n",
      "[Epoch 1/5] [Batch 35/938] [D loss: 0.5344088077545166] [G loss: 1.08925461769104]\n",
      "[Epoch 1/5] [Batch 36/938] [D loss: 0.5569840669631958] [G loss: 1.5633389949798584]\n",
      "[Epoch 1/5] [Batch 37/938] [D loss: 0.5556752681732178] [G loss: 1.0182621479034424]\n",
      "[Epoch 1/5] [Batch 38/938] [D loss: 0.5140790939331055] [G loss: 1.126399040222168]\n",
      "[Epoch 1/5] [Batch 39/938] [D loss: 0.553545355796814] [G loss: 1.6658332347869873]\n",
      "[Epoch 1/5] [Batch 40/938] [D loss: 0.48452335596084595] [G loss: 0.9465489387512207]\n",
      "[Epoch 1/5] [Batch 41/938] [D loss: 0.452808141708374] [G loss: 1.2173985242843628]\n",
      "[Epoch 1/5] [Batch 42/938] [D loss: 0.5791690945625305] [G loss: 1.5330373048782349]\n",
      "[Epoch 1/5] [Batch 43/938] [D loss: 0.5416669845581055] [G loss: 0.9973018169403076]\n",
      "[Epoch 1/5] [Batch 44/938] [D loss: 0.6016222238540649] [G loss: 1.8409583568572998]\n",
      "[Epoch 1/5] [Batch 45/938] [D loss: 0.5852030515670776] [G loss: 0.6979637145996094]\n",
      "[Epoch 1/5] [Batch 46/938] [D loss: 0.537859320640564] [G loss: 1.8293565511703491]\n",
      "[Epoch 1/5] [Batch 47/938] [D loss: 0.5904866456985474] [G loss: 0.875236451625824]\n",
      "[Epoch 1/5] [Batch 48/938] [D loss: 0.49673840403556824] [G loss: 1.6726455688476562]\n",
      "[Epoch 1/5] [Batch 49/938] [D loss: 0.5277937054634094] [G loss: 1.00914466381073]\n",
      "[Epoch 1/5] [Batch 50/938] [D loss: 0.508569598197937] [G loss: 1.2691383361816406]\n",
      "[Epoch 1/5] [Batch 51/938] [D loss: 0.4748944044113159] [G loss: 1.1517112255096436]\n",
      "[Epoch 1/5] [Batch 52/938] [D loss: 0.4713687300682068] [G loss: 1.2030115127563477]\n",
      "[Epoch 1/5] [Batch 53/938] [D loss: 0.4954763352870941] [G loss: 1.3338940143585205]\n",
      "[Epoch 1/5] [Batch 54/938] [D loss: 0.4896056056022644] [G loss: 1.047589898109436]\n",
      "[Epoch 1/5] [Batch 55/938] [D loss: 0.4195316433906555] [G loss: 1.6277744770050049]\n",
      "[Epoch 1/5] [Batch 56/938] [D loss: 0.42692282795906067] [G loss: 1.230622410774231]\n",
      "[Epoch 1/5] [Batch 57/938] [D loss: 0.5095640420913696] [G loss: 1.1934893131256104]\n",
      "[Epoch 1/5] [Batch 58/938] [D loss: 0.5055825114250183] [G loss: 1.4247690439224243]\n",
      "[Epoch 1/5] [Batch 59/938] [D loss: 0.5500974059104919] [G loss: 1.1351704597473145]\n",
      "[Epoch 1/5] [Batch 60/938] [D loss: 0.4787518382072449] [G loss: 1.386246919631958]\n",
      "[Epoch 1/5] [Batch 61/938] [D loss: 0.5050551891326904] [G loss: 1.351855754852295]\n",
      "[Epoch 1/5] [Batch 62/938] [D loss: 0.5462312698364258] [G loss: 0.9975804090499878]\n",
      "[Epoch 1/5] [Batch 63/938] [D loss: 0.505577027797699] [G loss: 1.8580669164657593]\n",
      "[Epoch 1/5] [Batch 64/938] [D loss: 0.49717092514038086] [G loss: 0.9169024229049683]\n",
      "[Epoch 1/5] [Batch 65/938] [D loss: 0.4549698233604431] [G loss: 1.4497923851013184]\n",
      "[Epoch 1/5] [Batch 66/938] [D loss: 0.4684254825115204] [G loss: 1.2000293731689453]\n",
      "[Epoch 1/5] [Batch 67/938] [D loss: 0.46504807472229004] [G loss: 1.375210165977478]\n",
      "[Epoch 1/5] [Batch 68/938] [D loss: 0.49093133211135864] [G loss: 1.6100879907608032]\n",
      "[Epoch 1/5] [Batch 69/938] [D loss: 0.5131059288978577] [G loss: 0.8346589803695679]\n",
      "[Epoch 1/5] [Batch 70/938] [D loss: 0.5444847345352173] [G loss: 1.981207251548767]\n",
      "[Epoch 1/5] [Batch 71/938] [D loss: 0.5990773439407349] [G loss: 0.7931743860244751]\n",
      "[Epoch 1/5] [Batch 72/938] [D loss: 0.4537371098995209] [G loss: 1.4829860925674438]\n",
      "[Epoch 1/5] [Batch 73/938] [D loss: 0.5318015813827515] [G loss: 1.0847409963607788]\n",
      "[Epoch 1/5] [Batch 74/938] [D loss: 0.6271491646766663] [G loss: 1.4185476303100586]\n",
      "[Epoch 1/5] [Batch 75/938] [D loss: 0.5878640413284302] [G loss: 0.8039039373397827]\n",
      "[Epoch 1/5] [Batch 76/938] [D loss: 0.5895516276359558] [G loss: 2.0984373092651367]\n",
      "[Epoch 1/5] [Batch 77/938] [D loss: 0.6086604595184326] [G loss: 0.745015025138855]\n",
      "[Epoch 1/5] [Batch 78/938] [D loss: 0.46160799264907837] [G loss: 1.4785823822021484]\n",
      "[Epoch 1/5] [Batch 79/938] [D loss: 0.5003450512886047] [G loss: 1.3632749319076538]\n",
      "[Epoch 1/5] [Batch 80/938] [D loss: 0.4975320100784302] [G loss: 0.6624626517295837]\n",
      "[Epoch 1/5] [Batch 81/938] [D loss: 0.5417829751968384] [G loss: 1.8458834886550903]\n",
      "[Epoch 1/5] [Batch 82/938] [D loss: 0.5233825445175171] [G loss: 0.8625344634056091]\n",
      "[Epoch 1/5] [Batch 83/938] [D loss: 0.5874854326248169] [G loss: 1.7689083814620972]\n",
      "[Epoch 1/5] [Batch 84/938] [D loss: 0.45200133323669434] [G loss: 0.8410970568656921]\n",
      "[Epoch 1/5] [Batch 85/938] [D loss: 0.5386638045310974] [G loss: 1.7727694511413574]\n",
      "[Epoch 1/5] [Batch 86/938] [D loss: 0.47168397903442383] [G loss: 0.9986824989318848]\n",
      "[Epoch 1/5] [Batch 87/938] [D loss: 0.4740322530269623] [G loss: 1.5873422622680664]\n",
      "[Epoch 1/5] [Batch 88/938] [D loss: 0.5125313401222229] [G loss: 1.1915788650512695]\n",
      "[Epoch 1/5] [Batch 89/938] [D loss: 0.4838489294052124] [G loss: 1.1448843479156494]\n",
      "[Epoch 1/5] [Batch 90/938] [D loss: 0.5099932551383972] [G loss: 1.3748223781585693]\n",
      "[Epoch 1/5] [Batch 91/938] [D loss: 0.5210855007171631] [G loss: 1.1812859773635864]\n",
      "[Epoch 1/5] [Batch 92/938] [D loss: 0.4319271743297577] [G loss: 1.5154860019683838]\n",
      "[Epoch 1/5] [Batch 93/938] [D loss: 0.500478982925415] [G loss: 1.3431386947631836]\n",
      "[Epoch 1/5] [Batch 94/938] [D loss: 0.5127729177474976] [G loss: 1.2773255109786987]\n",
      "[Epoch 1/5] [Batch 95/938] [D loss: 0.49823153018951416] [G loss: 1.4784821271896362]\n",
      "[Epoch 1/5] [Batch 96/938] [D loss: 0.5309491157531738] [G loss: 0.7350998520851135]\n",
      "[Epoch 1/5] [Batch 97/938] [D loss: 0.4920123219490051] [G loss: 2.085293769836426]\n",
      "[Epoch 1/5] [Batch 98/938] [D loss: 0.6213868856430054] [G loss: 0.8252086639404297]\n",
      "[Epoch 1/5] [Batch 99/938] [D loss: 0.5828804969787598] [G loss: 1.750069499015808]\n",
      "[Epoch 1/5] [Batch 100/938] [D loss: 0.5872282385826111] [G loss: 0.6872256398200989]\n",
      "[Epoch 1/5] [Batch 101/938] [D loss: 0.5561041831970215] [G loss: 1.8124152421951294]\n",
      "[Epoch 1/5] [Batch 102/938] [D loss: 0.5155705809593201] [G loss: 1.16486656665802]\n",
      "[Epoch 1/5] [Batch 103/938] [D loss: 0.5170266628265381] [G loss: 1.086547613143921]\n",
      "[Epoch 1/5] [Batch 104/938] [D loss: 0.4733475148677826] [G loss: 1.2813588380813599]\n",
      "[Epoch 1/5] [Batch 105/938] [D loss: 0.5034898519515991] [G loss: 1.316841721534729]\n",
      "[Epoch 1/5] [Batch 106/938] [D loss: 0.4784514307975769] [G loss: 1.0696483850479126]\n",
      "[Epoch 1/5] [Batch 107/938] [D loss: 0.4626239836215973] [G loss: 1.4679880142211914]\n",
      "[Epoch 1/5] [Batch 108/938] [D loss: 0.5591367483139038] [G loss: 0.9525129199028015]\n",
      "[Epoch 1/5] [Batch 109/938] [D loss: 0.5129028558731079] [G loss: 1.1130543947219849]\n",
      "[Epoch 1/5] [Batch 110/938] [D loss: 0.496891587972641] [G loss: 1.212947130203247]\n",
      "[Epoch 1/5] [Batch 111/938] [D loss: 0.5245147943496704] [G loss: 1.0268875360488892]\n",
      "[Epoch 1/5] [Batch 112/938] [D loss: 0.3927161693572998] [G loss: 1.790855050086975]\n",
      "[Epoch 1/5] [Batch 113/938] [D loss: 0.5021860003471375] [G loss: 1.2557117938995361]\n",
      "[Epoch 1/5] [Batch 114/938] [D loss: 0.5073471069335938] [G loss: 1.1646325588226318]\n",
      "[Epoch 1/5] [Batch 115/938] [D loss: 0.4918423593044281] [G loss: 0.9935835003852844]\n",
      "[Epoch 1/5] [Batch 116/938] [D loss: 0.5870553255081177] [G loss: 1.8088425397872925]\n",
      "[Epoch 1/5] [Batch 117/938] [D loss: 0.6339461207389832] [G loss: 0.6070579290390015]\n",
      "[Epoch 1/5] [Batch 118/938] [D loss: 0.6429698467254639] [G loss: 2.1543593406677246]\n",
      "[Epoch 1/5] [Batch 119/938] [D loss: 0.5668165683746338] [G loss: 0.6938082575798035]\n",
      "[Epoch 1/5] [Batch 120/938] [D loss: 0.4674183130264282] [G loss: 1.6370662450790405]\n",
      "[Epoch 1/5] [Batch 121/938] [D loss: 0.5515179634094238] [G loss: 1.5409334897994995]\n",
      "[Epoch 1/5] [Batch 122/938] [D loss: 0.6210539937019348] [G loss: 0.819485068321228]\n",
      "[Epoch 1/5] [Batch 123/938] [D loss: 0.4351171553134918] [G loss: 1.6256073713302612]\n",
      "[Epoch 1/5] [Batch 124/938] [D loss: 0.4804671108722687] [G loss: 1.2649235725402832]\n",
      "[Epoch 1/5] [Batch 125/938] [D loss: 0.4868466854095459] [G loss: 1.2406129837036133]\n",
      "[Epoch 1/5] [Batch 126/938] [D loss: 0.43028146028518677] [G loss: 1.4633320569992065]\n",
      "[Epoch 1/5] [Batch 127/938] [D loss: 0.5178327560424805] [G loss: 1.4422014951705933]\n",
      "[Epoch 1/5] [Batch 128/938] [D loss: 0.5253167152404785] [G loss: 1.3105934858322144]\n",
      "[Epoch 1/5] [Batch 129/938] [D loss: 0.5530432462692261] [G loss: 1.347959280014038]\n",
      "[Epoch 1/5] [Batch 130/938] [D loss: 0.477119505405426] [G loss: 0.9682450294494629]\n",
      "[Epoch 1/5] [Batch 131/938] [D loss: 0.4875492453575134] [G loss: 1.6696821451187134]\n",
      "[Epoch 1/5] [Batch 132/938] [D loss: 0.5364840030670166] [G loss: 1.1134576797485352]\n",
      "[Epoch 1/5] [Batch 133/938] [D loss: 0.5119144916534424] [G loss: 1.109234094619751]\n",
      "[Epoch 1/5] [Batch 134/938] [D loss: 0.48194921016693115] [G loss: 1.4069416522979736]\n",
      "[Epoch 1/5] [Batch 135/938] [D loss: 0.49708521366119385] [G loss: 1.2208985090255737]\n",
      "[Epoch 1/5] [Batch 136/938] [D loss: 0.4481264352798462] [G loss: 1.224123477935791]\n",
      "[Epoch 1/5] [Batch 137/938] [D loss: 0.49973374605178833] [G loss: 1.2493679523468018]\n",
      "[Epoch 1/5] [Batch 138/938] [D loss: 0.48245733976364136] [G loss: 1.0598169565200806]\n",
      "[Epoch 1/5] [Batch 139/938] [D loss: 0.48611509799957275] [G loss: 1.4175926446914673]\n",
      "[Epoch 1/5] [Batch 140/938] [D loss: 0.5526679754257202] [G loss: 1.4140217304229736]\n",
      "[Epoch 1/5] [Batch 141/938] [D loss: 0.5527949929237366] [G loss: 1.0257947444915771]\n",
      "[Epoch 1/5] [Batch 142/938] [D loss: 0.5190309882164001] [G loss: 1.8027747869491577]\n",
      "[Epoch 1/5] [Batch 143/938] [D loss: 0.8395944833755493] [G loss: 0.4837430715560913]\n",
      "[Epoch 1/5] [Batch 144/938] [D loss: 0.9582234621047974] [G loss: 3.222167491912842]\n",
      "[Epoch 1/5] [Batch 145/938] [D loss: 0.5958387851715088] [G loss: 0.6581032276153564]\n",
      "[Epoch 1/5] [Batch 146/938] [D loss: 0.6172621250152588] [G loss: 0.6845248937606812]\n",
      "[Epoch 1/5] [Batch 147/938] [D loss: 0.4814192056655884] [G loss: 1.4211729764938354]\n",
      "[Epoch 1/5] [Batch 148/938] [D loss: 0.5319871306419373] [G loss: 1.4506393671035767]\n",
      "[Epoch 1/5] [Batch 149/938] [D loss: 0.594917893409729] [G loss: 0.8881990313529968]\n",
      "[Epoch 1/5] [Batch 150/938] [D loss: 0.519140899181366] [G loss: 0.9713829159736633]\n",
      "[Epoch 1/5] [Batch 151/938] [D loss: 0.47695040702819824] [G loss: 1.5827938318252563]\n",
      "[Epoch 1/5] [Batch 152/938] [D loss: 0.5015106201171875] [G loss: 1.3495702743530273]\n",
      "[Epoch 1/5] [Batch 153/938] [D loss: 0.565421462059021] [G loss: 1.1086950302124023]\n",
      "[Epoch 1/5] [Batch 154/938] [D loss: 0.515407919883728] [G loss: 1.2224500179290771]\n",
      "[Epoch 1/5] [Batch 155/938] [D loss: 0.5014572739601135] [G loss: 1.193097710609436]\n",
      "[Epoch 1/5] [Batch 156/938] [D loss: 0.5273187160491943] [G loss: 1.2558050155639648]\n",
      "[Epoch 1/5] [Batch 157/938] [D loss: 0.525810956954956] [G loss: 1.1386606693267822]\n",
      "[Epoch 1/5] [Batch 158/938] [D loss: 0.4859340786933899] [G loss: 1.0009573698043823]\n",
      "[Epoch 1/5] [Batch 159/938] [D loss: 0.5646981000900269] [G loss: 1.5028189420700073]\n",
      "[Epoch 1/5] [Batch 160/938] [D loss: 0.46921736001968384] [G loss: 1.106296181678772]\n",
      "[Epoch 1/5] [Batch 161/938] [D loss: 0.4496021866798401] [G loss: 1.3519670963287354]\n",
      "[Epoch 1/5] [Batch 162/938] [D loss: 0.44883236289024353] [G loss: 1.1347718238830566]\n",
      "[Epoch 1/5] [Batch 163/938] [D loss: 0.4584386944770813] [G loss: 1.4261499643325806]\n",
      "[Epoch 1/5] [Batch 164/938] [D loss: 0.5377355813980103] [G loss: 1.3733444213867188]\n",
      "[Epoch 1/5] [Batch 165/938] [D loss: 0.5648462176322937] [G loss: 0.9821482300758362]\n",
      "[Epoch 1/5] [Batch 166/938] [D loss: 0.532920241355896] [G loss: 1.4730736017227173]\n",
      "[Epoch 1/5] [Batch 167/938] [D loss: 0.4756584167480469] [G loss: 1.0975043773651123]\n",
      "[Epoch 1/5] [Batch 168/938] [D loss: 0.5083557367324829] [G loss: 1.2593711614608765]\n",
      "[Epoch 1/5] [Batch 169/938] [D loss: 0.5610381364822388] [G loss: 1.2714028358459473]\n",
      "[Epoch 1/5] [Batch 170/938] [D loss: 0.5576112270355225] [G loss: 0.8514001369476318]\n",
      "[Epoch 1/5] [Batch 171/938] [D loss: 0.5815955400466919] [G loss: 1.9089610576629639]\n",
      "[Epoch 1/5] [Batch 172/938] [D loss: 0.6747406125068665] [G loss: 0.6829237937927246]\n",
      "[Epoch 1/5] [Batch 173/938] [D loss: 0.5169610977172852] [G loss: 1.3975199460983276]\n",
      "[Epoch 1/5] [Batch 174/938] [D loss: 0.47101467847824097] [G loss: 1.0505921840667725]\n",
      "[Epoch 1/5] [Batch 175/938] [D loss: 0.593835711479187] [G loss: 1.1267149448394775]\n",
      "[Epoch 1/5] [Batch 176/938] [D loss: 0.5684491395950317] [G loss: 0.8054082989692688]\n",
      "[Epoch 1/5] [Batch 177/938] [D loss: 0.5829662084579468] [G loss: 1.5795679092407227]\n",
      "[Epoch 1/5] [Batch 178/938] [D loss: 0.5163431167602539] [G loss: 1.1259933710098267]\n",
      "[Epoch 1/5] [Batch 179/938] [D loss: 0.5068665742874146] [G loss: 1.3796309232711792]\n",
      "[Epoch 1/5] [Batch 180/938] [D loss: 0.5437530279159546] [G loss: 1.335252046585083]\n",
      "[Epoch 1/5] [Batch 181/938] [D loss: 0.5398499965667725] [G loss: 0.9013918042182922]\n",
      "[Epoch 1/5] [Batch 182/938] [D loss: 0.5512744188308716] [G loss: 1.6013147830963135]\n",
      "[Epoch 1/5] [Batch 183/938] [D loss: 0.5636916160583496] [G loss: 0.9519110321998596]\n",
      "[Epoch 1/5] [Batch 184/938] [D loss: 0.5147799253463745] [G loss: 1.3597629070281982]\n",
      "[Epoch 1/5] [Batch 185/938] [D loss: 0.5055713057518005] [G loss: 0.9743024110794067]\n",
      "[Epoch 1/5] [Batch 186/938] [D loss: 0.5422340631484985] [G loss: 1.024086594581604]\n",
      "[Epoch 1/5] [Batch 187/938] [D loss: 0.48980414867401123] [G loss: 1.107075572013855]\n",
      "[Epoch 1/5] [Batch 188/938] [D loss: 0.5309176445007324] [G loss: 1.3242278099060059]\n",
      "[Epoch 1/5] [Batch 189/938] [D loss: 0.501213788986206] [G loss: 1.0050395727157593]\n",
      "[Epoch 1/5] [Batch 190/938] [D loss: 0.5178332328796387] [G loss: 1.7082746028900146]\n",
      "[Epoch 1/5] [Batch 191/938] [D loss: 0.46136677265167236] [G loss: 0.959702730178833]\n",
      "[Epoch 1/5] [Batch 192/938] [D loss: 0.5467132925987244] [G loss: 1.7512931823730469]\n",
      "[Epoch 1/5] [Batch 193/938] [D loss: 0.4775056838989258] [G loss: 0.8527107834815979]\n",
      "[Epoch 1/5] [Batch 194/938] [D loss: 0.5557524561882019] [G loss: 1.5497792959213257]\n",
      "[Epoch 1/5] [Batch 195/938] [D loss: 0.5853720307350159] [G loss: 0.8937352895736694]\n",
      "[Epoch 1/5] [Batch 196/938] [D loss: 0.5035014152526855] [G loss: 1.401947021484375]\n",
      "[Epoch 1/5] [Batch 197/938] [D loss: 0.5731450319290161] [G loss: 1.0005556344985962]\n",
      "[Epoch 1/5] [Batch 198/938] [D loss: 0.5172898173332214] [G loss: 1.1836963891983032]\n",
      "[Epoch 1/5] [Batch 199/938] [D loss: 0.5211767554283142] [G loss: 1.2874412536621094]\n",
      "[Epoch 1/5] [Batch 200/938] [D loss: 0.4834071695804596] [G loss: 0.9879591464996338]\n",
      "[Epoch 1/5] [Batch 201/938] [D loss: 0.49764886498451233] [G loss: 1.7820677757263184]\n",
      "[Epoch 1/5] [Batch 202/938] [D loss: 0.5585213303565979] [G loss: 0.6395555138587952]\n",
      "[Epoch 1/5] [Batch 203/938] [D loss: 0.5164563655853271] [G loss: 1.9047186374664307]\n",
      "[Epoch 1/5] [Batch 204/938] [D loss: 0.5113204121589661] [G loss: 1.0997445583343506]\n",
      "[Epoch 1/5] [Batch 205/938] [D loss: 0.5013282895088196] [G loss: 1.1563689708709717]\n",
      "[Epoch 1/5] [Batch 206/938] [D loss: 0.5485547184944153] [G loss: 1.674721598625183]\n",
      "[Epoch 1/5] [Batch 207/938] [D loss: 0.49285775423049927] [G loss: 0.8240493535995483]\n",
      "[Epoch 1/5] [Batch 208/938] [D loss: 0.5777631998062134] [G loss: 1.980880856513977]\n",
      "[Epoch 1/5] [Batch 209/938] [D loss: 0.6052646636962891] [G loss: 0.6891769170761108]\n",
      "[Epoch 1/5] [Batch 210/938] [D loss: 0.5226683020591736] [G loss: 1.7311785221099854]\n",
      "[Epoch 1/5] [Batch 211/938] [D loss: 0.48788201808929443] [G loss: 1.425502896308899]\n",
      "[Epoch 1/5] [Batch 212/938] [D loss: 0.5716651678085327] [G loss: 0.8875953555107117]\n",
      "[Epoch 1/5] [Batch 213/938] [D loss: 0.48804306983947754] [G loss: 1.717488408088684]\n",
      "[Epoch 1/5] [Batch 214/938] [D loss: 0.47916197776794434] [G loss: 0.9880074858665466]\n",
      "[Epoch 1/5] [Batch 215/938] [D loss: 0.460288405418396] [G loss: 1.5506911277770996]\n",
      "[Epoch 1/5] [Batch 216/938] [D loss: 0.49764329195022583] [G loss: 1.3563584089279175]\n",
      "[Epoch 1/5] [Batch 217/938] [D loss: 0.45066937804222107] [G loss: 1.0399501323699951]\n",
      "[Epoch 1/5] [Batch 218/938] [D loss: 0.5322756767272949] [G loss: 1.8575410842895508]\n",
      "[Epoch 1/5] [Batch 219/938] [D loss: 0.4910731911659241] [G loss: 1.2853798866271973]\n",
      "[Epoch 1/5] [Batch 220/938] [D loss: 0.5118229389190674] [G loss: 0.8572501540184021]\n",
      "[Epoch 1/5] [Batch 221/938] [D loss: 0.484464555978775] [G loss: 1.6127018928527832]\n",
      "[Epoch 1/5] [Batch 222/938] [D loss: 0.6104781627655029] [G loss: 0.7616376280784607]\n",
      "[Epoch 1/5] [Batch 223/938] [D loss: 0.5534616708755493] [G loss: 2.216336488723755]\n",
      "[Epoch 1/5] [Batch 224/938] [D loss: 0.5851922631263733] [G loss: 0.6214369535446167]\n",
      "[Epoch 1/5] [Batch 225/938] [D loss: 0.47979193925857544] [G loss: 1.6524288654327393]\n",
      "[Epoch 1/5] [Batch 226/938] [D loss: 0.4727691411972046] [G loss: 1.5043340921401978]\n",
      "[Epoch 1/5] [Batch 227/938] [D loss: 0.46303361654281616] [G loss: 1.0232656002044678]\n",
      "[Epoch 1/5] [Batch 228/938] [D loss: 0.5100657939910889] [G loss: 1.1769675016403198]\n",
      "[Epoch 1/5] [Batch 229/938] [D loss: 0.4235955476760864] [G loss: 1.3892335891723633]\n",
      "[Epoch 1/5] [Batch 230/938] [D loss: 0.5253666639328003] [G loss: 0.9899320602416992]\n",
      "[Epoch 1/5] [Batch 231/938] [D loss: 0.4841289520263672] [G loss: 1.7846297025680542]\n",
      "[Epoch 1/5] [Batch 232/938] [D loss: 0.5515440702438354] [G loss: 0.8130759000778198]\n",
      "[Epoch 1/5] [Batch 233/938] [D loss: 0.4358023405075073] [G loss: 1.9339100122451782]\n",
      "[Epoch 1/5] [Batch 234/938] [D loss: 0.5367755889892578] [G loss: 1.044416904449463]\n",
      "[Epoch 1/5] [Batch 235/938] [D loss: 0.46735161542892456] [G loss: 1.3803986310958862]\n",
      "[Epoch 1/5] [Batch 236/938] [D loss: 0.44069904088974] [G loss: 1.2445043325424194]\n",
      "[Epoch 1/5] [Batch 237/938] [D loss: 0.5369359850883484] [G loss: 1.5190755128860474]\n",
      "[Epoch 1/5] [Batch 238/938] [D loss: 0.5641164779663086] [G loss: 1.0067739486694336]\n",
      "[Epoch 1/5] [Batch 239/938] [D loss: 0.44305914640426636] [G loss: 1.5967888832092285]\n",
      "[Epoch 1/5] [Batch 240/938] [D loss: 0.5012894868850708] [G loss: 1.1848478317260742]\n",
      "[Epoch 1/5] [Batch 241/938] [D loss: 0.44541651010513306] [G loss: 1.619988203048706]\n",
      "[Epoch 1/5] [Batch 242/938] [D loss: 0.529913604259491] [G loss: 0.8775355219841003]\n",
      "[Epoch 1/5] [Batch 243/938] [D loss: 0.6097562313079834] [G loss: 2.2565503120422363]\n",
      "[Epoch 1/5] [Batch 244/938] [D loss: 0.7204963564872742] [G loss: 0.49711304903030396]\n",
      "[Epoch 1/5] [Batch 245/938] [D loss: 0.558476448059082] [G loss: 2.2420735359191895]\n",
      "[Epoch 1/5] [Batch 246/938] [D loss: 0.5147302150726318] [G loss: 1.0708692073822021]\n",
      "[Epoch 1/5] [Batch 247/938] [D loss: 0.49647894501686096] [G loss: 1.00371253490448]\n",
      "[Epoch 1/5] [Batch 248/938] [D loss: 0.5910465717315674] [G loss: 1.9051320552825928]\n",
      "[Epoch 1/5] [Batch 249/938] [D loss: 0.5104910731315613] [G loss: 0.8982701301574707]\n",
      "[Epoch 1/5] [Batch 250/938] [D loss: 0.48933669924736023] [G loss: 1.1637940406799316]\n",
      "[Epoch 1/5] [Batch 251/938] [D loss: 0.5577700138092041] [G loss: 1.5229992866516113]\n",
      "[Epoch 1/5] [Batch 252/938] [D loss: 0.4964205026626587] [G loss: 1.0187861919403076]\n",
      "[Epoch 1/5] [Batch 253/938] [D loss: 0.5468125343322754] [G loss: 1.285601258277893]\n",
      "[Epoch 1/5] [Batch 254/938] [D loss: 0.5639888048171997] [G loss: 1.2334024906158447]\n",
      "[Epoch 1/5] [Batch 255/938] [D loss: 0.4709792733192444] [G loss: 1.27565336227417]\n",
      "[Epoch 1/5] [Batch 256/938] [D loss: 0.5464499592781067] [G loss: 1.3058708906173706]\n",
      "[Epoch 1/5] [Batch 257/938] [D loss: 0.5073792934417725] [G loss: 1.318255066871643]\n",
      "[Epoch 1/5] [Batch 258/938] [D loss: 0.4570313096046448] [G loss: 1.0704277753829956]\n",
      "[Epoch 1/5] [Batch 259/938] [D loss: 0.47015732526779175] [G loss: 1.4935635328292847]\n",
      "[Epoch 1/5] [Batch 260/938] [D loss: 0.5474541187286377] [G loss: 1.1023057699203491]\n",
      "[Epoch 1/5] [Batch 261/938] [D loss: 0.5314559936523438] [G loss: 1.5385055541992188]\n",
      "[Epoch 1/5] [Batch 262/938] [D loss: 0.5879504084587097] [G loss: 0.9362533688545227]\n",
      "[Epoch 1/5] [Batch 263/938] [D loss: 0.6628530025482178] [G loss: 2.4846901893615723]\n",
      "[Epoch 1/5] [Batch 264/938] [D loss: 0.8040059804916382] [G loss: 0.3211982846260071]\n",
      "[Epoch 1/5] [Batch 265/938] [D loss: 0.5153269171714783] [G loss: 1.655558466911316]\n",
      "[Epoch 1/5] [Batch 266/938] [D loss: 0.4926590323448181] [G loss: 1.6468596458435059]\n",
      "[Epoch 1/5] [Batch 267/938] [D loss: 0.4861699640750885] [G loss: 0.9977917671203613]\n",
      "[Epoch 1/5] [Batch 268/938] [D loss: 0.46952158212661743] [G loss: 1.1071758270263672]\n",
      "[Epoch 1/5] [Batch 269/938] [D loss: 0.4818173050880432] [G loss: 1.4959585666656494]\n",
      "[Epoch 1/5] [Batch 270/938] [D loss: 0.477658212184906] [G loss: 1.0563080310821533]\n",
      "[Epoch 1/5] [Batch 271/938] [D loss: 0.480482280254364] [G loss: 1.1062034368515015]\n",
      "[Epoch 1/5] [Batch 272/938] [D loss: 0.46736466884613037] [G loss: 1.483329176902771]\n",
      "[Epoch 1/5] [Batch 273/938] [D loss: 0.5435545444488525] [G loss: 1.1000328063964844]\n",
      "[Epoch 1/5] [Batch 274/938] [D loss: 0.5038604736328125] [G loss: 1.1578067541122437]\n",
      "[Epoch 1/5] [Batch 275/938] [D loss: 0.47371506690979004] [G loss: 1.1643080711364746]\n",
      "[Epoch 1/5] [Batch 276/938] [D loss: 0.5733599066734314] [G loss: 0.8696067333221436]\n",
      "[Epoch 1/5] [Batch 277/938] [D loss: 0.4640495181083679] [G loss: 1.6854785680770874]\n",
      "[Epoch 1/5] [Batch 278/938] [D loss: 0.4844259023666382] [G loss: 1.2434172630310059]\n",
      "[Epoch 1/5] [Batch 279/938] [D loss: 0.4803990125656128] [G loss: 1.4230620861053467]\n",
      "[Epoch 1/5] [Batch 280/938] [D loss: 0.5208680033683777] [G loss: 0.9327593445777893]\n",
      "[Epoch 1/5] [Batch 281/938] [D loss: 0.6025995016098022] [G loss: 1.9828770160675049]\n",
      "[Epoch 1/5] [Batch 282/938] [D loss: 0.6076134443283081] [G loss: 0.6579816341400146]\n",
      "[Epoch 1/5] [Batch 283/938] [D loss: 0.5781479477882385] [G loss: 1.866248607635498]\n",
      "[Epoch 1/5] [Batch 284/938] [D loss: 0.5740919709205627] [G loss: 0.8606330752372742]\n",
      "[Epoch 1/5] [Batch 285/938] [D loss: 0.5155975222587585] [G loss: 1.266152024269104]\n",
      "[Epoch 1/5] [Batch 286/938] [D loss: 0.5371313095092773] [G loss: 1.2695096731185913]\n",
      "[Epoch 1/5] [Batch 287/938] [D loss: 0.5124290585517883] [G loss: 1.0537878274917603]\n",
      "[Epoch 1/5] [Batch 288/938] [D loss: 0.5194448828697205] [G loss: 1.4473613500595093]\n",
      "[Epoch 1/5] [Batch 289/938] [D loss: 0.4881065785884857] [G loss: 0.9705687761306763]\n",
      "[Epoch 1/5] [Batch 290/938] [D loss: 0.582629382610321] [G loss: 1.1809825897216797]\n",
      "[Epoch 1/5] [Batch 291/938] [D loss: 0.5162560939788818] [G loss: 1.41609525680542]\n",
      "[Epoch 1/5] [Batch 292/938] [D loss: 0.5310405492782593] [G loss: 0.9429901242256165]\n",
      "[Epoch 1/5] [Batch 293/938] [D loss: 0.5535190105438232] [G loss: 1.5147391557693481]\n",
      "[Epoch 1/5] [Batch 294/938] [D loss: 0.49515092372894287] [G loss: 1.2582999467849731]\n",
      "[Epoch 1/5] [Batch 295/938] [D loss: 0.5240740180015564] [G loss: 1.103344202041626]\n",
      "[Epoch 1/5] [Batch 296/938] [D loss: 0.515343427658081] [G loss: 1.2344087362289429]\n",
      "[Epoch 1/5] [Batch 297/938] [D loss: 0.43968379497528076] [G loss: 0.9238914847373962]\n",
      "[Epoch 1/5] [Batch 298/938] [D loss: 0.5470609068870544] [G loss: 1.8291651010513306]\n",
      "[Epoch 1/5] [Batch 299/938] [D loss: 0.6178680658340454] [G loss: 0.730038046836853]\n",
      "[Epoch 1/5] [Batch 300/938] [D loss: 0.6802473068237305] [G loss: 2.1710171699523926]\n",
      "[Epoch 1/5] [Batch 301/938] [D loss: 0.5768439173698425] [G loss: 0.5839515328407288]\n",
      "[Epoch 1/5] [Batch 302/938] [D loss: 0.5512481927871704] [G loss: 1.4699437618255615]\n",
      "[Epoch 1/5] [Batch 303/938] [D loss: 0.5643197894096375] [G loss: 1.830836296081543]\n",
      "[Epoch 1/5] [Batch 304/938] [D loss: 0.5409005880355835] [G loss: 0.8925888538360596]\n",
      "[Epoch 1/5] [Batch 305/938] [D loss: 0.5562865138053894] [G loss: 1.1552245616912842]\n",
      "[Epoch 1/5] [Batch 306/938] [D loss: 0.5407395362854004] [G loss: 1.1355341672897339]\n",
      "[Epoch 1/5] [Batch 307/938] [D loss: 0.4932629466056824] [G loss: 1.2751760482788086]\n",
      "[Epoch 1/5] [Batch 308/938] [D loss: 0.43661418557167053] [G loss: 1.206505298614502]\n",
      "[Epoch 1/5] [Batch 309/938] [D loss: 0.4958154559135437] [G loss: 1.3707913160324097]\n",
      "[Epoch 1/5] [Batch 310/938] [D loss: 0.5222850441932678] [G loss: 1.0639803409576416]\n",
      "[Epoch 1/5] [Batch 311/938] [D loss: 0.6108695268630981] [G loss: 1.064512014389038]\n",
      "[Epoch 1/5] [Batch 312/938] [D loss: 0.6104085445404053] [G loss: 1.755289912223816]\n",
      "[Epoch 1/5] [Batch 313/938] [D loss: 0.5856627225875854] [G loss: 0.7815688848495483]\n",
      "[Epoch 1/5] [Batch 314/938] [D loss: 0.49975958466529846] [G loss: 1.6757136583328247]\n",
      "[Epoch 1/5] [Batch 315/938] [D loss: 0.5566688776016235] [G loss: 1.0621391534805298]\n",
      "[Epoch 1/5] [Batch 316/938] [D loss: 0.4472546875476837] [G loss: 1.3191128969192505]\n",
      "[Epoch 1/5] [Batch 317/938] [D loss: 0.4536181688308716] [G loss: 1.154550552368164]\n",
      "[Epoch 1/5] [Batch 318/938] [D loss: 0.5111619830131531] [G loss: 1.336503267288208]\n",
      "[Epoch 1/5] [Batch 319/938] [D loss: 0.48963475227355957] [G loss: 1.2767632007598877]\n",
      "[Epoch 1/5] [Batch 320/938] [D loss: 0.47173207998275757] [G loss: 1.4487504959106445]\n",
      "[Epoch 1/5] [Batch 321/938] [D loss: 0.47822314500808716] [G loss: 1.1188437938690186]\n",
      "[Epoch 1/5] [Batch 322/938] [D loss: 0.566276490688324] [G loss: 1.3790853023529053]\n",
      "[Epoch 1/5] [Batch 323/938] [D loss: 0.5839504599571228] [G loss: 0.804697573184967]\n",
      "[Epoch 1/5] [Batch 324/938] [D loss: 0.5920763611793518] [G loss: 2.206017255783081]\n",
      "[Epoch 1/5] [Batch 325/938] [D loss: 0.6349406838417053] [G loss: 0.5222439765930176]\n",
      "[Epoch 1/5] [Batch 326/938] [D loss: 0.48341310024261475] [G loss: 1.6811171770095825]\n",
      "[Epoch 1/5] [Batch 327/938] [D loss: 0.5054563879966736] [G loss: 1.3310363292694092]\n",
      "[Epoch 1/5] [Batch 328/938] [D loss: 0.5169745683670044] [G loss: 0.9146242737770081]\n",
      "[Epoch 1/5] [Batch 329/938] [D loss: 0.5222313404083252] [G loss: 1.372988224029541]\n",
      "[Epoch 1/5] [Batch 330/938] [D loss: 0.4982588589191437] [G loss: 1.4569281339645386]\n",
      "[Epoch 1/5] [Batch 331/938] [D loss: 0.5286357402801514] [G loss: 1.0446869134902954]\n",
      "[Epoch 1/5] [Batch 332/938] [D loss: 0.5277482271194458] [G loss: 1.738378643989563]\n",
      "[Epoch 1/5] [Batch 333/938] [D loss: 0.5109292268753052] [G loss: 1.044130563735962]\n",
      "[Epoch 1/5] [Batch 334/938] [D loss: 0.5028207302093506] [G loss: 1.4590909481048584]\n",
      "[Epoch 1/5] [Batch 335/938] [D loss: 0.5576330423355103] [G loss: 0.8916996121406555]\n",
      "[Epoch 1/5] [Batch 336/938] [D loss: 0.42403918504714966] [G loss: 1.4482144117355347]\n",
      "[Epoch 1/5] [Batch 337/938] [D loss: 0.48674246668815613] [G loss: 1.4852697849273682]\n",
      "[Epoch 1/5] [Batch 338/938] [D loss: 0.555573046207428] [G loss: 0.8329483270645142]\n",
      "[Epoch 1/5] [Batch 339/938] [D loss: 0.5411988496780396] [G loss: 1.6361790895462036]\n",
      "[Epoch 1/5] [Batch 340/938] [D loss: 0.5563905835151672] [G loss: 0.8567972779273987]\n",
      "[Epoch 1/5] [Batch 341/938] [D loss: 0.46795719861984253] [G loss: 1.5304434299468994]\n",
      "[Epoch 1/5] [Batch 342/938] [D loss: 0.49758380651474] [G loss: 1.2187871932983398]\n",
      "[Epoch 1/5] [Batch 343/938] [D loss: 0.5045861005783081] [G loss: 0.9247702360153198]\n",
      "[Epoch 1/5] [Batch 344/938] [D loss: 0.5108575820922852] [G loss: 1.6967324018478394]\n",
      "[Epoch 1/5] [Batch 345/938] [D loss: 0.5237228274345398] [G loss: 1.0511500835418701]\n",
      "[Epoch 1/5] [Batch 346/938] [D loss: 0.5351738929748535] [G loss: 1.6193734407424927]\n",
      "[Epoch 1/5] [Batch 347/938] [D loss: 0.5447390079498291] [G loss: 0.8324081897735596]\n",
      "[Epoch 1/5] [Batch 348/938] [D loss: 0.565106987953186] [G loss: 1.7186284065246582]\n",
      "[Epoch 1/5] [Batch 349/938] [D loss: 0.5567929744720459] [G loss: 0.7219963073730469]\n",
      "[Epoch 1/5] [Batch 350/938] [D loss: 0.5203003883361816] [G loss: 1.2518260478973389]\n",
      "[Epoch 1/5] [Batch 351/938] [D loss: 0.5055419206619263] [G loss: 1.3721815347671509]\n",
      "[Epoch 1/5] [Batch 352/938] [D loss: 0.4802049398422241] [G loss: 1.2334403991699219]\n",
      "[Epoch 1/5] [Batch 353/938] [D loss: 0.38681694865226746] [G loss: 1.5512727499008179]\n",
      "[Epoch 1/5] [Batch 354/938] [D loss: 0.4984562397003174] [G loss: 1.2914801836013794]\n",
      "[Epoch 1/5] [Batch 355/938] [D loss: 0.4885845184326172] [G loss: 1.3311653137207031]\n",
      "[Epoch 1/5] [Batch 356/938] [D loss: 0.468239963054657] [G loss: 1.1687722206115723]\n",
      "[Epoch 1/5] [Batch 357/938] [D loss: 0.558379054069519] [G loss: 1.5959651470184326]\n",
      "[Epoch 1/5] [Batch 358/938] [D loss: 0.5823056697845459] [G loss: 0.7275886535644531]\n",
      "[Epoch 1/5] [Batch 359/938] [D loss: 0.5823439359664917] [G loss: 2.2258543968200684]\n",
      "[Epoch 1/5] [Batch 360/938] [D loss: 0.6405676603317261] [G loss: 0.6265157461166382]\n",
      "[Epoch 1/5] [Batch 361/938] [D loss: 0.6117498874664307] [G loss: 1.293430209159851]\n",
      "[Epoch 1/5] [Batch 362/938] [D loss: 0.5297924280166626] [G loss: 1.3656375408172607]\n",
      "[Epoch 1/5] [Batch 363/938] [D loss: 0.5399833917617798] [G loss: 1.1281516551971436]\n",
      "[Epoch 1/5] [Batch 364/938] [D loss: 0.47720324993133545] [G loss: 1.0423345565795898]\n",
      "[Epoch 1/5] [Batch 365/938] [D loss: 0.562252402305603] [G loss: 1.6328790187835693]\n",
      "[Epoch 1/5] [Batch 366/938] [D loss: 0.546291172504425] [G loss: 0.7021452188491821]\n",
      "[Epoch 1/5] [Batch 367/938] [D loss: 0.49642321467399597] [G loss: 1.4535181522369385]\n",
      "[Epoch 1/5] [Batch 368/938] [D loss: 0.4649050235748291] [G loss: 1.330353021621704]\n",
      "[Epoch 1/5] [Batch 369/938] [D loss: 0.581462025642395] [G loss: 1.179265022277832]\n",
      "[Epoch 1/5] [Batch 370/938] [D loss: 0.481362521648407] [G loss: 1.0260816812515259]\n",
      "[Epoch 1/5] [Batch 371/938] [D loss: 0.5199810862541199] [G loss: 1.3882781267166138]\n",
      "[Epoch 1/5] [Batch 372/938] [D loss: 0.5755322575569153] [G loss: 1.1035678386688232]\n",
      "[Epoch 1/5] [Batch 373/938] [D loss: 0.4938664138317108] [G loss: 1.418533444404602]\n",
      "[Epoch 1/5] [Batch 374/938] [D loss: 0.5988677740097046] [G loss: 1.2261303663253784]\n",
      "[Epoch 1/5] [Batch 375/938] [D loss: 0.5654309988021851] [G loss: 0.963218629360199]\n",
      "[Epoch 1/5] [Batch 376/938] [D loss: 0.5895771980285645] [G loss: 1.600346326828003]\n",
      "[Epoch 1/5] [Batch 377/938] [D loss: 0.6975637078285217] [G loss: 0.5395008325576782]\n",
      "[Epoch 1/5] [Batch 378/938] [D loss: 0.6771082282066345] [G loss: 1.9996390342712402]\n",
      "[Epoch 1/5] [Batch 379/938] [D loss: 0.5846514701843262] [G loss: 0.8182305097579956]\n",
      "[Epoch 1/5] [Batch 380/938] [D loss: 0.588295578956604] [G loss: 0.7763252854347229]\n",
      "[Epoch 1/5] [Batch 381/938] [D loss: 0.5862668752670288] [G loss: 1.7020351886749268]\n",
      "[Epoch 1/5] [Batch 382/938] [D loss: 0.5359050035476685] [G loss: 1.0064642429351807]\n",
      "[Epoch 1/5] [Batch 383/938] [D loss: 0.5544660091400146] [G loss: 1.0083006620407104]\n",
      "[Epoch 1/5] [Batch 384/938] [D loss: 0.6074488162994385] [G loss: 1.3009657859802246]\n",
      "[Epoch 1/5] [Batch 385/938] [D loss: 0.5696874856948853] [G loss: 0.9967725276947021]\n",
      "[Epoch 1/5] [Batch 386/938] [D loss: 0.53448486328125] [G loss: 0.9478215575218201]\n",
      "[Epoch 1/5] [Batch 387/938] [D loss: 0.5913328528404236] [G loss: 1.3990485668182373]\n",
      "[Epoch 1/5] [Batch 388/938] [D loss: 0.5114980936050415] [G loss: 0.783578097820282]\n",
      "[Epoch 1/5] [Batch 389/938] [D loss: 0.5215141773223877] [G loss: 1.2329081296920776]\n",
      "[Epoch 1/5] [Batch 390/938] [D loss: 0.4696999788284302] [G loss: 1.4932976961135864]\n",
      "[Epoch 1/5] [Batch 391/938] [D loss: 0.5368801951408386] [G loss: 1.1059112548828125]\n",
      "[Epoch 1/5] [Batch 392/938] [D loss: 0.5503841042518616] [G loss: 1.0726312398910522]\n",
      "[Epoch 1/5] [Batch 393/938] [D loss: 0.49515867233276367] [G loss: 1.1253631114959717]\n",
      "[Epoch 1/5] [Batch 394/938] [D loss: 0.5368657112121582] [G loss: 1.267716407775879]\n",
      "[Epoch 1/5] [Batch 395/938] [D loss: 0.5334175229072571] [G loss: 0.9374269843101501]\n",
      "[Epoch 1/5] [Batch 396/938] [D loss: 0.5948489904403687] [G loss: 1.1396446228027344]\n",
      "[Epoch 1/5] [Batch 397/938] [D loss: 0.4912973940372467] [G loss: 1.0796430110931396]\n",
      "[Epoch 1/5] [Batch 398/938] [D loss: 0.601725161075592] [G loss: 0.9844688177108765]\n",
      "[Epoch 1/5] [Batch 399/938] [D loss: 0.5011639595031738] [G loss: 1.4005919694900513]\n",
      "[Epoch 1/5] [Batch 400/938] [D loss: 0.4721235930919647] [G loss: 1.3024718761444092]\n",
      "[Epoch 1/5] [Batch 401/938] [D loss: 0.5472731590270996] [G loss: 1.385239601135254]\n",
      "[Epoch 1/5] [Batch 402/938] [D loss: 0.5688841342926025] [G loss: 0.7689977884292603]\n",
      "[Epoch 1/5] [Batch 403/938] [D loss: 0.526477575302124] [G loss: 1.7155474424362183]\n",
      "[Epoch 1/5] [Batch 404/938] [D loss: 0.547350287437439] [G loss: 1.0828819274902344]\n",
      "[Epoch 1/5] [Batch 405/938] [D loss: 0.5962151885032654] [G loss: 1.3456066846847534]\n",
      "[Epoch 1/5] [Batch 406/938] [D loss: 0.5076678991317749] [G loss: 1.0017949342727661]\n",
      "[Epoch 1/5] [Batch 407/938] [D loss: 0.5659838914871216] [G loss: 1.7101939916610718]\n",
      "[Epoch 1/5] [Batch 408/938] [D loss: 0.6211086511611938] [G loss: 0.7404324412345886]\n",
      "[Epoch 1/5] [Batch 409/938] [D loss: 0.4937559962272644] [G loss: 1.5423908233642578]\n",
      "[Epoch 1/5] [Batch 410/938] [D loss: 0.5104531049728394] [G loss: 1.1784392595291138]\n",
      "[Epoch 1/5] [Batch 411/938] [D loss: 0.5660442113876343] [G loss: 1.1088107824325562]\n",
      "[Epoch 1/5] [Batch 412/938] [D loss: 0.49418574571609497] [G loss: 1.1995420455932617]\n",
      "[Epoch 1/5] [Batch 413/938] [D loss: 0.523054838180542] [G loss: 1.5459402799606323]\n",
      "[Epoch 1/5] [Batch 414/938] [D loss: 0.5676823854446411] [G loss: 1.1984684467315674]\n",
      "[Epoch 1/5] [Batch 415/938] [D loss: 0.5237662196159363] [G loss: 1.1194872856140137]\n",
      "[Epoch 1/5] [Batch 416/938] [D loss: 0.5294516086578369] [G loss: 1.2888110876083374]\n",
      "[Epoch 1/5] [Batch 417/938] [D loss: 0.5072886943817139] [G loss: 1.5345730781555176]\n",
      "[Epoch 1/5] [Batch 418/938] [D loss: 0.49043160676956177] [G loss: 1.0838044881820679]\n",
      "[Epoch 1/5] [Batch 419/938] [D loss: 0.5466116070747375] [G loss: 1.729626178741455]\n",
      "[Epoch 1/5] [Batch 420/938] [D loss: 0.5057176351547241] [G loss: 0.9822686910629272]\n",
      "[Epoch 1/5] [Batch 421/938] [D loss: 0.5439194440841675] [G loss: 1.5823229551315308]\n",
      "[Epoch 1/5] [Batch 422/938] [D loss: 0.5565369129180908] [G loss: 0.7247210741043091]\n",
      "[Epoch 1/5] [Batch 423/938] [D loss: 0.6965420246124268] [G loss: 2.2192797660827637]\n",
      "[Epoch 1/5] [Batch 424/938] [D loss: 0.8964363932609558] [G loss: 0.3324185311794281]\n",
      "[Epoch 1/5] [Batch 425/938] [D loss: 0.5427077412605286] [G loss: 1.6287732124328613]\n",
      "[Epoch 1/5] [Batch 426/938] [D loss: 0.6119554042816162] [G loss: 1.5585639476776123]\n",
      "[Epoch 1/5] [Batch 427/938] [D loss: 0.5215801000595093] [G loss: 0.875514030456543]\n",
      "[Epoch 1/5] [Batch 428/938] [D loss: 0.525093138217926] [G loss: 0.920905590057373]\n",
      "[Epoch 1/5] [Batch 429/938] [D loss: 0.505744457244873] [G loss: 1.2002925872802734]\n",
      "[Epoch 1/5] [Batch 430/938] [D loss: 0.49372029304504395] [G loss: 1.1895744800567627]\n",
      "[Epoch 1/5] [Batch 431/938] [D loss: 0.5021379590034485] [G loss: 1.0777857303619385]\n",
      "[Epoch 1/5] [Batch 432/938] [D loss: 0.539103627204895] [G loss: 1.165264368057251]\n",
      "[Epoch 1/5] [Batch 433/938] [D loss: 0.45116138458251953] [G loss: 1.2025941610336304]\n",
      "[Epoch 1/5] [Batch 434/938] [D loss: 0.4871540069580078] [G loss: 1.4383739233016968]\n",
      "[Epoch 1/5] [Batch 435/938] [D loss: 0.5350908041000366] [G loss: 1.0194262266159058]\n",
      "[Epoch 1/5] [Batch 436/938] [D loss: 0.5361822247505188] [G loss: 1.0740667581558228]\n",
      "[Epoch 1/5] [Batch 437/938] [D loss: 0.5218921899795532] [G loss: 1.3993407487869263]\n",
      "[Epoch 1/5] [Batch 438/938] [D loss: 0.4825281500816345] [G loss: 1.0960209369659424]\n",
      "[Epoch 1/5] [Batch 439/938] [D loss: 0.5170587301254272] [G loss: 1.3898338079452515]\n",
      "[Epoch 1/5] [Batch 440/938] [D loss: 0.5792341828346252] [G loss: 1.3798952102661133]\n",
      "[Epoch 1/5] [Batch 441/938] [D loss: 0.627140998840332] [G loss: 0.796134352684021]\n",
      "[Epoch 1/5] [Batch 442/938] [D loss: 0.5382182598114014] [G loss: 1.6726081371307373]\n",
      "[Epoch 1/5] [Batch 443/938] [D loss: 0.5256214141845703] [G loss: 0.9105058908462524]\n",
      "[Epoch 1/5] [Batch 444/938] [D loss: 0.5628464221954346] [G loss: 1.240661859512329]\n",
      "[Epoch 1/5] [Batch 445/938] [D loss: 0.4805757403373718] [G loss: 1.1111705303192139]\n",
      "[Epoch 1/5] [Batch 446/938] [D loss: 0.5304125547409058] [G loss: 1.4266146421432495]\n",
      "[Epoch 1/5] [Batch 447/938] [D loss: 0.47776955366134644] [G loss: 1.2976328134536743]\n",
      "[Epoch 1/5] [Batch 448/938] [D loss: 0.5232072472572327] [G loss: 1.2999531030654907]\n",
      "[Epoch 1/5] [Batch 449/938] [D loss: 0.5334485173225403] [G loss: 0.934868574142456]\n",
      "[Epoch 1/5] [Batch 450/938] [D loss: 0.5285940766334534] [G loss: 1.2763911485671997]\n",
      "[Epoch 1/5] [Batch 451/938] [D loss: 0.5409021377563477] [G loss: 1.145630121231079]\n",
      "[Epoch 1/5] [Batch 452/938] [D loss: 0.4646632671356201] [G loss: 1.2601242065429688]\n",
      "[Epoch 1/5] [Batch 453/938] [D loss: 0.5022221803665161] [G loss: 1.1344127655029297]\n",
      "[Epoch 1/5] [Batch 454/938] [D loss: 0.4657275080680847] [G loss: 1.616888165473938]\n",
      "[Epoch 1/5] [Batch 455/938] [D loss: 0.5114319324493408] [G loss: 1.1019154787063599]\n",
      "[Epoch 1/5] [Batch 456/938] [D loss: 0.4674350321292877] [G loss: 1.6477222442626953]\n",
      "[Epoch 1/5] [Batch 457/938] [D loss: 0.5196800231933594] [G loss: 1.0520069599151611]\n",
      "[Epoch 1/5] [Batch 458/938] [D loss: 0.555296778678894] [G loss: 1.206868052482605]\n",
      "[Epoch 1/5] [Batch 459/938] [D loss: 0.48995280265808105] [G loss: 1.0150448083877563]\n",
      "[Epoch 1/5] [Batch 460/938] [D loss: 0.6347540020942688] [G loss: 1.7728502750396729]\n",
      "[Epoch 1/5] [Batch 461/938] [D loss: 0.7339732050895691] [G loss: 0.47272104024887085]\n",
      "[Epoch 1/5] [Batch 462/938] [D loss: 0.6017951965332031] [G loss: 2.050808906555176]\n",
      "[Epoch 1/5] [Batch 463/938] [D loss: 0.5137144327163696] [G loss: 1.1364978551864624]\n",
      "[Epoch 1/5] [Batch 464/938] [D loss: 0.4902791380882263] [G loss: 0.9400880336761475]\n",
      "[Epoch 1/5] [Batch 465/938] [D loss: 0.5211277604103088] [G loss: 1.4168299436569214]\n",
      "[Epoch 1/5] [Batch 466/938] [D loss: 0.5403928160667419] [G loss: 1.0844237804412842]\n",
      "[Epoch 1/5] [Batch 467/938] [D loss: 0.523160994052887] [G loss: 1.4716994762420654]\n",
      "[Epoch 1/5] [Batch 468/938] [D loss: 0.5646710991859436] [G loss: 0.8350600004196167]\n",
      "[Epoch 1/5] [Batch 469/938] [D loss: 0.5013542771339417] [G loss: 1.6476460695266724]\n",
      "[Epoch 1/5] [Batch 470/938] [D loss: 0.5415635108947754] [G loss: 1.203057050704956]\n",
      "[Epoch 1/5] [Batch 471/938] [D loss: 0.4969545900821686] [G loss: 0.9880128502845764]\n",
      "[Epoch 1/5] [Batch 472/938] [D loss: 0.6186778545379639] [G loss: 1.424483060836792]\n",
      "[Epoch 1/5] [Batch 473/938] [D loss: 0.47847414016723633] [G loss: 1.0069804191589355]\n",
      "[Epoch 1/5] [Batch 474/938] [D loss: 0.5888133645057678] [G loss: 1.5622731447219849]\n",
      "[Epoch 1/5] [Batch 475/938] [D loss: 0.5575689077377319] [G loss: 0.8136393427848816]\n",
      "[Epoch 1/5] [Batch 476/938] [D loss: 0.5358898639678955] [G loss: 1.4126358032226562]\n",
      "[Epoch 1/5] [Batch 477/938] [D loss: 0.49192750453948975] [G loss: 1.6109751462936401]\n",
      "[Epoch 1/5] [Batch 478/938] [D loss: 0.5822130441665649] [G loss: 0.8359529972076416]\n",
      "[Epoch 1/5] [Batch 479/938] [D loss: 0.5374979972839355] [G loss: 1.2452714443206787]\n",
      "[Epoch 1/5] [Batch 480/938] [D loss: 0.5506025552749634] [G loss: 1.1830568313598633]\n",
      "[Epoch 1/5] [Batch 481/938] [D loss: 0.5307060480117798] [G loss: 1.0102462768554688]\n",
      "[Epoch 1/5] [Batch 482/938] [D loss: 0.5617098808288574] [G loss: 1.7259896993637085]\n",
      "[Epoch 1/5] [Batch 483/938] [D loss: 0.595374345779419] [G loss: 0.6217403411865234]\n",
      "[Epoch 1/5] [Batch 484/938] [D loss: 0.6576088666915894] [G loss: 1.9941415786743164]\n",
      "[Epoch 1/5] [Batch 485/938] [D loss: 0.6423425078392029] [G loss: 0.7133201360702515]\n",
      "[Epoch 1/5] [Batch 486/938] [D loss: 0.4973350167274475] [G loss: 1.1878001689910889]\n",
      "[Epoch 1/5] [Batch 487/938] [D loss: 0.5191521048545837] [G loss: 1.3827733993530273]\n",
      "[Epoch 1/5] [Batch 488/938] [D loss: 0.5186375975608826] [G loss: 1.1498957872390747]\n",
      "[Epoch 1/5] [Batch 489/938] [D loss: 0.46644127368927] [G loss: 1.047688364982605]\n",
      "[Epoch 1/5] [Batch 490/938] [D loss: 0.5599759817123413] [G loss: 1.228289246559143]\n",
      "[Epoch 1/5] [Batch 491/938] [D loss: 0.5425494313240051] [G loss: 1.0124632120132446]\n",
      "[Epoch 1/5] [Batch 492/938] [D loss: 0.48755383491516113] [G loss: 1.2645914554595947]\n",
      "[Epoch 1/5] [Batch 493/938] [D loss: 0.5161119699478149] [G loss: 1.1633856296539307]\n",
      "[Epoch 1/5] [Batch 494/938] [D loss: 0.5413541197776794] [G loss: 1.4356989860534668]\n",
      "[Epoch 1/5] [Batch 495/938] [D loss: 0.4893346428871155] [G loss: 1.3214582204818726]\n",
      "[Epoch 1/5] [Batch 496/938] [D loss: 0.5242553949356079] [G loss: 0.9667898416519165]\n",
      "[Epoch 1/5] [Batch 497/938] [D loss: 0.5516747236251831] [G loss: 1.6332674026489258]\n",
      "[Epoch 1/5] [Batch 498/938] [D loss: 0.6056394577026367] [G loss: 0.7498165965080261]\n",
      "[Epoch 1/5] [Batch 499/938] [D loss: 0.6533636450767517] [G loss: 1.7719368934631348]\n",
      "[Epoch 1/5] [Batch 500/938] [D loss: 0.5951752066612244] [G loss: 0.8834009766578674]\n",
      "[Epoch 1/5] [Batch 501/938] [D loss: 0.48793265223503113] [G loss: 1.2050623893737793]\n",
      "[Epoch 1/5] [Batch 502/938] [D loss: 0.5173391103744507] [G loss: 1.3091833591461182]\n",
      "[Epoch 1/5] [Batch 503/938] [D loss: 0.5736094117164612] [G loss: 0.8111473321914673]\n",
      "[Epoch 1/5] [Batch 504/938] [D loss: 0.51458340883255] [G loss: 1.3842928409576416]\n",
      "[Epoch 1/5] [Batch 505/938] [D loss: 0.47551602125167847] [G loss: 1.0910037755966187]\n",
      "[Epoch 1/5] [Batch 506/938] [D loss: 0.5106608271598816] [G loss: 1.0290488004684448]\n",
      "[Epoch 1/5] [Batch 507/938] [D loss: 0.5795061588287354] [G loss: 1.4480159282684326]\n",
      "[Epoch 1/5] [Batch 508/938] [D loss: 0.49166998267173767] [G loss: 0.9962032437324524]\n",
      "[Epoch 1/5] [Batch 509/938] [D loss: 0.4723623991012573] [G loss: 1.1779240369796753]\n",
      "[Epoch 1/5] [Batch 510/938] [D loss: 0.4658151865005493] [G loss: 1.572473406791687]\n",
      "[Epoch 1/5] [Batch 511/938] [D loss: 0.4306926131248474] [G loss: 1.3787559270858765]\n",
      "[Epoch 1/5] [Batch 512/938] [D loss: 0.5249484777450562] [G loss: 1.0656987428665161]\n",
      "[Epoch 1/5] [Batch 513/938] [D loss: 0.4844449758529663] [G loss: 1.6339689493179321]\n",
      "[Epoch 1/5] [Batch 514/938] [D loss: 0.5574134588241577] [G loss: 0.6128722429275513]\n",
      "[Epoch 1/5] [Batch 515/938] [D loss: 0.5413263440132141] [G loss: 2.2532339096069336]\n",
      "[Epoch 1/5] [Batch 516/938] [D loss: 0.5456748604774475] [G loss: 0.8934868574142456]\n",
      "[Epoch 1/5] [Batch 517/938] [D loss: 0.5126744508743286] [G loss: 1.3716779947280884]\n",
      "[Epoch 1/5] [Batch 518/938] [D loss: 0.44418567419052124] [G loss: 1.1278225183486938]\n",
      "[Epoch 1/5] [Batch 519/938] [D loss: 0.5151576995849609] [G loss: 1.3796370029449463]\n",
      "[Epoch 1/5] [Batch 520/938] [D loss: 0.4557250738143921] [G loss: 0.9908900260925293]\n",
      "[Epoch 1/5] [Batch 521/938] [D loss: 0.5006166100502014] [G loss: 1.078234314918518]\n",
      "[Epoch 1/5] [Batch 522/938] [D loss: 0.46057724952697754] [G loss: 1.4478979110717773]\n",
      "[Epoch 1/5] [Batch 523/938] [D loss: 0.47686222195625305] [G loss: 1.155277967453003]\n",
      "[Epoch 1/5] [Batch 524/938] [D loss: 0.49738284945487976] [G loss: 1.179503083229065]\n",
      "[Epoch 1/5] [Batch 525/938] [D loss: 0.5699477195739746] [G loss: 1.8997046947479248]\n",
      "[Epoch 1/5] [Batch 526/938] [D loss: 0.7755657434463501] [G loss: 0.39925143122673035]\n",
      "[Epoch 1/5] [Batch 527/938] [D loss: 0.7401024103164673] [G loss: 2.5475504398345947]\n",
      "[Epoch 1/5] [Batch 528/938] [D loss: 0.49643808603286743] [G loss: 0.9904971122741699]\n",
      "[Epoch 1/5] [Batch 529/938] [D loss: 0.5617930293083191] [G loss: 0.8563895225524902]\n",
      "[Epoch 1/5] [Batch 530/938] [D loss: 0.49144935607910156] [G loss: 1.2877024412155151]\n",
      "[Epoch 1/5] [Batch 531/938] [D loss: 0.47974827885627747] [G loss: 1.3087937831878662]\n",
      "[Epoch 1/5] [Batch 532/938] [D loss: 0.4898240268230438] [G loss: 1.026842474937439]\n",
      "[Epoch 1/5] [Batch 533/938] [D loss: 0.45232176780700684] [G loss: 1.4019849300384521]\n",
      "[Epoch 1/5] [Batch 534/938] [D loss: 0.5238152742385864] [G loss: 1.3399738073349]\n",
      "[Epoch 1/5] [Batch 535/938] [D loss: 0.5292503237724304] [G loss: 1.1949578523635864]\n",
      "[Epoch 1/5] [Batch 536/938] [D loss: 0.5395205616950989] [G loss: 1.0940998792648315]\n",
      "[Epoch 1/5] [Batch 537/938] [D loss: 0.4722546339035034] [G loss: 1.1276689767837524]\n",
      "[Epoch 1/5] [Batch 538/938] [D loss: 0.5010929107666016] [G loss: 1.401220440864563]\n",
      "[Epoch 1/5] [Batch 539/938] [D loss: 0.545637845993042] [G loss: 0.8888794183731079]\n",
      "[Epoch 1/5] [Batch 540/938] [D loss: 0.5613847970962524] [G loss: 1.6611747741699219]\n",
      "[Epoch 1/5] [Batch 541/938] [D loss: 0.6294833421707153] [G loss: 0.560480535030365]\n",
      "[Epoch 1/5] [Batch 542/938] [D loss: 0.554335355758667] [G loss: 2.0393621921539307]\n",
      "[Epoch 1/5] [Batch 543/938] [D loss: 0.59395432472229] [G loss: 0.9612473249435425]\n",
      "[Epoch 1/5] [Batch 544/938] [D loss: 0.5886912941932678] [G loss: 0.9463744163513184]\n",
      "[Epoch 1/5] [Batch 545/938] [D loss: 0.5760945081710815] [G loss: 1.7208399772644043]\n",
      "[Epoch 1/5] [Batch 546/938] [D loss: 0.5306351184844971] [G loss: 0.839881181716919]\n",
      "[Epoch 1/5] [Batch 547/938] [D loss: 0.49358320236206055] [G loss: 1.2314980030059814]\n",
      "[Epoch 1/5] [Batch 548/938] [D loss: 0.5832833051681519] [G loss: 1.3623753786087036]\n",
      "[Epoch 1/5] [Batch 549/938] [D loss: 0.5623869895935059] [G loss: 0.8737293481826782]\n",
      "[Epoch 1/5] [Batch 550/938] [D loss: 0.49281230568885803] [G loss: 1.0930547714233398]\n",
      "[Epoch 1/5] [Batch 551/938] [D loss: 0.5850580930709839] [G loss: 1.4588490724563599]\n",
      "[Epoch 1/5] [Batch 552/938] [D loss: 0.5501096248626709] [G loss: 0.7088282108306885]\n",
      "[Epoch 1/5] [Batch 553/938] [D loss: 0.5729405879974365] [G loss: 1.3044110536575317]\n",
      "[Epoch 1/5] [Batch 554/938] [D loss: 0.5180413722991943] [G loss: 1.2543002367019653]\n",
      "[Epoch 1/5] [Batch 555/938] [D loss: 0.578144371509552] [G loss: 1.1636745929718018]\n",
      "[Epoch 1/5] [Batch 556/938] [D loss: 0.6330975294113159] [G loss: 1.0749766826629639]\n",
      "[Epoch 1/5] [Batch 557/938] [D loss: 0.5051321387290955] [G loss: 1.0444235801696777]\n",
      "[Epoch 1/5] [Batch 558/938] [D loss: 0.5749537348747253] [G loss: 1.5543265342712402]\n",
      "[Epoch 1/5] [Batch 559/938] [D loss: 0.5268616080284119] [G loss: 0.7172983288764954]\n",
      "[Epoch 1/5] [Batch 560/938] [D loss: 0.512499988079071] [G loss: 1.3847928047180176]\n",
      "[Epoch 1/5] [Batch 561/938] [D loss: 0.5889502167701721] [G loss: 1.1711288690567017]\n",
      "[Epoch 1/5] [Batch 562/938] [D loss: 0.4957517683506012] [G loss: 0.9721764922142029]\n",
      "[Epoch 1/5] [Batch 563/938] [D loss: 0.5648221969604492] [G loss: 1.6930861473083496]\n",
      "[Epoch 1/5] [Batch 564/938] [D loss: 0.6352629065513611] [G loss: 0.795939028263092]\n",
      "[Epoch 1/5] [Batch 565/938] [D loss: 0.5188056230545044] [G loss: 1.3523414134979248]\n",
      "[Epoch 1/5] [Batch 566/938] [D loss: 0.5520573854446411] [G loss: 1.0981913805007935]\n",
      "[Epoch 1/5] [Batch 567/938] [D loss: 0.5920788049697876] [G loss: 0.9684733748435974]\n",
      "[Epoch 1/5] [Batch 568/938] [D loss: 0.5408893823623657] [G loss: 1.2354358434677124]\n",
      "[Epoch 1/5] [Batch 569/938] [D loss: 0.4968148171901703] [G loss: 1.012953519821167]\n",
      "[Epoch 1/5] [Batch 570/938] [D loss: 0.524568498134613] [G loss: 1.403136968612671]\n",
      "[Epoch 1/5] [Batch 571/938] [D loss: 0.5291794538497925] [G loss: 0.8423781991004944]\n",
      "[Epoch 1/5] [Batch 572/938] [D loss: 0.484102725982666] [G loss: 1.2307355403900146]\n",
      "[Epoch 1/5] [Batch 573/938] [D loss: 0.5550462007522583] [G loss: 1.291925311088562]\n",
      "[Epoch 1/5] [Batch 574/938] [D loss: 0.5712089538574219] [G loss: 1.049601435661316]\n",
      "[Epoch 1/5] [Batch 575/938] [D loss: 0.5419086217880249] [G loss: 1.316085934638977]\n",
      "[Epoch 1/5] [Batch 576/938] [D loss: 0.6018595695495605] [G loss: 0.8373202085494995]\n",
      "[Epoch 1/5] [Batch 577/938] [D loss: 0.508650541305542] [G loss: 1.4830933809280396]\n",
      "[Epoch 1/5] [Batch 578/938] [D loss: 0.48820728063583374] [G loss: 1.3529024124145508]\n",
      "[Epoch 1/5] [Batch 579/938] [D loss: 0.5832387804985046] [G loss: 0.7793182730674744]\n",
      "[Epoch 1/5] [Batch 580/938] [D loss: 0.6229743957519531] [G loss: 1.68472158908844]\n",
      "[Epoch 1/5] [Batch 581/938] [D loss: 0.6525722742080688] [G loss: 0.5080394744873047]\n",
      "[Epoch 1/5] [Batch 582/938] [D loss: 0.618728518486023] [G loss: 1.3883455991744995]\n",
      "[Epoch 1/5] [Batch 583/938] [D loss: 0.5463037490844727] [G loss: 0.9335960149765015]\n",
      "[Epoch 1/5] [Batch 584/938] [D loss: 0.5362571477890015] [G loss: 0.9109894037246704]\n",
      "[Epoch 1/5] [Batch 585/938] [D loss: 0.6529514193534851] [G loss: 1.222055435180664]\n",
      "[Epoch 1/5] [Batch 586/938] [D loss: 0.5371153950691223] [G loss: 0.9472200870513916]\n",
      "[Epoch 1/5] [Batch 587/938] [D loss: 0.5951794385910034] [G loss: 1.0008974075317383]\n",
      "[Epoch 1/5] [Batch 588/938] [D loss: 0.5014284253120422] [G loss: 1.3251545429229736]\n",
      "[Epoch 1/5] [Batch 589/938] [D loss: 0.5026589632034302] [G loss: 1.2368555068969727]\n",
      "[Epoch 1/5] [Batch 590/938] [D loss: 0.521924614906311] [G loss: 1.030120611190796]\n",
      "[Epoch 1/5] [Batch 591/938] [D loss: 0.5429642200469971] [G loss: 1.437516212463379]\n",
      "[Epoch 1/5] [Batch 592/938] [D loss: 0.5852664709091187] [G loss: 0.9118910431861877]\n",
      "[Epoch 1/5] [Batch 593/938] [D loss: 0.5194982290267944] [G loss: 1.4236996173858643]\n",
      "[Epoch 1/5] [Batch 594/938] [D loss: 0.5985654592514038] [G loss: 0.8380867838859558]\n",
      "[Epoch 1/5] [Batch 595/938] [D loss: 0.5520592331886292] [G loss: 1.5581412315368652]\n",
      "[Epoch 1/5] [Batch 596/938] [D loss: 0.5529091358184814] [G loss: 0.9142388105392456]\n",
      "[Epoch 1/5] [Batch 597/938] [D loss: 0.5219111442565918] [G loss: 1.3821277618408203]\n",
      "[Epoch 1/5] [Batch 598/938] [D loss: 0.5538747310638428] [G loss: 0.9577250480651855]\n",
      "[Epoch 1/5] [Batch 599/938] [D loss: 0.5603888630867004] [G loss: 1.7502756118774414]\n",
      "[Epoch 1/5] [Batch 600/938] [D loss: 0.5744092464447021] [G loss: 0.9151350259780884]\n",
      "[Epoch 1/5] [Batch 601/938] [D loss: 0.5457378625869751] [G loss: 1.1506675481796265]\n",
      "[Epoch 1/5] [Batch 602/938] [D loss: 0.5711424350738525] [G loss: 1.294708251953125]\n",
      "[Epoch 1/5] [Batch 603/938] [D loss: 0.5109206438064575] [G loss: 0.7960224151611328]\n",
      "[Epoch 1/5] [Batch 604/938] [D loss: 0.5410665273666382] [G loss: 1.584383249282837]\n",
      "[Epoch 1/5] [Batch 605/938] [D loss: 0.5374025106430054] [G loss: 1.0573344230651855]\n",
      "[Epoch 1/5] [Batch 606/938] [D loss: 0.5108128786087036] [G loss: 1.3562902212142944]\n",
      "[Epoch 1/5] [Batch 607/938] [D loss: 0.49917054176330566] [G loss: 1.174191951751709]\n",
      "[Epoch 1/5] [Batch 608/938] [D loss: 0.5347486138343811] [G loss: 1.3028887510299683]\n",
      "[Epoch 1/5] [Batch 609/938] [D loss: 0.49935922026634216] [G loss: 0.7514789700508118]\n",
      "[Epoch 1/5] [Batch 610/938] [D loss: 0.5496989488601685] [G loss: 2.0848886966705322]\n",
      "[Epoch 1/5] [Batch 611/938] [D loss: 0.6255004405975342] [G loss: 0.6384052038192749]\n",
      "[Epoch 1/5] [Batch 612/938] [D loss: 0.4798732399940491] [G loss: 1.781646728515625]\n",
      "[Epoch 1/5] [Batch 613/938] [D loss: 0.4955369532108307] [G loss: 1.2239513397216797]\n",
      "[Epoch 1/5] [Batch 614/938] [D loss: 0.5398620963096619] [G loss: 0.7663139700889587]\n",
      "[Epoch 1/5] [Batch 615/938] [D loss: 0.5541927814483643] [G loss: 1.5903105735778809]\n",
      "[Epoch 1/5] [Batch 616/938] [D loss: 0.6105104684829712] [G loss: 0.7556749582290649]\n",
      "[Epoch 1/5] [Batch 617/938] [D loss: 0.5126557946205139] [G loss: 1.4357391595840454]\n",
      "[Epoch 1/5] [Batch 618/938] [D loss: 0.5070189237594604] [G loss: 1.235142707824707]\n",
      "[Epoch 1/5] [Batch 619/938] [D loss: 0.58221435546875] [G loss: 0.7978197932243347]\n",
      "[Epoch 1/5] [Batch 620/938] [D loss: 0.6338224411010742] [G loss: 1.5934193134307861]\n",
      "[Epoch 1/5] [Batch 621/938] [D loss: 0.5733689069747925] [G loss: 0.6856771111488342]\n",
      "[Epoch 1/5] [Batch 622/938] [D loss: 0.5525354146957397] [G loss: 1.3940916061401367]\n",
      "[Epoch 1/5] [Batch 623/938] [D loss: 0.5702695846557617] [G loss: 1.527175784111023]\n",
      "[Epoch 1/5] [Batch 624/938] [D loss: 0.490960955619812] [G loss: 1.0870898962020874]\n",
      "[Epoch 1/5] [Batch 625/938] [D loss: 0.5457497835159302] [G loss: 1.2791253328323364]\n",
      "[Epoch 1/5] [Batch 626/938] [D loss: 0.5271602272987366] [G loss: 1.0625988245010376]\n",
      "[Epoch 1/5] [Batch 627/938] [D loss: 0.5129226446151733] [G loss: 1.1199548244476318]\n",
      "[Epoch 1/5] [Batch 628/938] [D loss: 0.519601583480835] [G loss: 1.2358454465866089]\n",
      "[Epoch 1/5] [Batch 629/938] [D loss: 0.5335035920143127] [G loss: 0.8756415247917175]\n",
      "[Epoch 1/5] [Batch 630/938] [D loss: 0.4982180595397949] [G loss: 1.339897632598877]\n",
      "[Epoch 1/5] [Batch 631/938] [D loss: 0.5457793474197388] [G loss: 0.9823968410491943]\n",
      "[Epoch 1/5] [Batch 632/938] [D loss: 0.5368428230285645] [G loss: 1.3132376670837402]\n",
      "[Epoch 1/5] [Batch 633/938] [D loss: 0.5567241907119751] [G loss: 1.0516448020935059]\n",
      "[Epoch 1/5] [Batch 634/938] [D loss: 0.551757276058197] [G loss: 1.1414101123809814]\n",
      "[Epoch 1/5] [Batch 635/938] [D loss: 0.5644703507423401] [G loss: 1.5777853727340698]\n",
      "[Epoch 1/5] [Batch 636/938] [D loss: 0.5954988598823547] [G loss: 0.7010762095451355]\n",
      "[Epoch 1/5] [Batch 637/938] [D loss: 0.5943631529808044] [G loss: 1.4430673122406006]\n",
      "[Epoch 1/5] [Batch 638/938] [D loss: 0.4549896717071533] [G loss: 1.036400318145752]\n",
      "[Epoch 1/5] [Batch 639/938] [D loss: 0.5712180733680725] [G loss: 0.9569334983825684]\n",
      "[Epoch 1/5] [Batch 640/938] [D loss: 0.5332135558128357] [G loss: 1.2827144861221313]\n",
      "[Epoch 1/5] [Batch 641/938] [D loss: 0.5801553130149841] [G loss: 0.9551780819892883]\n",
      "[Epoch 1/5] [Batch 642/938] [D loss: 0.5664316415786743] [G loss: 1.3905370235443115]\n",
      "[Epoch 1/5] [Batch 643/938] [D loss: 0.545241117477417] [G loss: 0.9906997084617615]\n",
      "[Epoch 1/5] [Batch 644/938] [D loss: 0.5416982173919678] [G loss: 1.0391758680343628]\n",
      "[Epoch 1/5] [Batch 645/938] [D loss: 0.5231318473815918] [G loss: 1.2010624408721924]\n",
      "[Epoch 1/5] [Batch 646/938] [D loss: 0.5269059538841248] [G loss: 0.9927740693092346]\n",
      "[Epoch 1/5] [Batch 647/938] [D loss: 0.5427972078323364] [G loss: 1.2643319368362427]\n",
      "[Epoch 1/5] [Batch 648/938] [D loss: 0.5490169525146484] [G loss: 1.282122015953064]\n",
      "[Epoch 1/5] [Batch 649/938] [D loss: 0.5338436961174011] [G loss: 0.9315367937088013]\n",
      "[Epoch 1/5] [Batch 650/938] [D loss: 0.5821639895439148] [G loss: 1.5429646968841553]\n",
      "[Epoch 1/5] [Batch 651/938] [D loss: 0.6221592426300049] [G loss: 0.8700107336044312]\n",
      "[Epoch 1/5] [Batch 652/938] [D loss: 0.5487602353096008] [G loss: 1.439622163772583]\n",
      "[Epoch 1/5] [Batch 653/938] [D loss: 0.5414590835571289] [G loss: 0.9434981942176819]\n",
      "[Epoch 1/5] [Batch 654/938] [D loss: 0.5467698574066162] [G loss: 1.4600557088851929]\n",
      "[Epoch 1/5] [Batch 655/938] [D loss: 0.6307041645050049] [G loss: 0.8574044704437256]\n",
      "[Epoch 1/5] [Batch 656/938] [D loss: 0.49096691608428955] [G loss: 1.2349214553833008]\n",
      "[Epoch 1/5] [Batch 657/938] [D loss: 0.5774438381195068] [G loss: 1.0496745109558105]\n",
      "[Epoch 1/5] [Batch 658/938] [D loss: 0.6103559136390686] [G loss: 1.0575907230377197]\n",
      "[Epoch 1/5] [Batch 659/938] [D loss: 0.5469980239868164] [G loss: 1.0376051664352417]\n",
      "[Epoch 1/5] [Batch 660/938] [D loss: 0.5635888576507568] [G loss: 1.3531967401504517]\n",
      "[Epoch 1/5] [Batch 661/938] [D loss: 0.5699069499969482] [G loss: 0.7804737091064453]\n",
      "[Epoch 1/5] [Batch 662/938] [D loss: 0.5895828008651733] [G loss: 1.9010874032974243]\n",
      "[Epoch 1/5] [Batch 663/938] [D loss: 0.611992359161377] [G loss: 0.5634042024612427]\n",
      "[Epoch 1/5] [Batch 664/938] [D loss: 0.5250298380851746] [G loss: 1.530106544494629]\n",
      "[Epoch 1/5] [Batch 665/938] [D loss: 0.5402041077613831] [G loss: 1.3772599697113037]\n",
      "[Epoch 1/5] [Batch 666/938] [D loss: 0.5533236861228943] [G loss: 0.9975233674049377]\n",
      "[Epoch 1/5] [Batch 667/938] [D loss: 0.5801217555999756] [G loss: 1.0365031957626343]\n",
      "[Epoch 1/5] [Batch 668/938] [D loss: 0.5032951831817627] [G loss: 1.1434988975524902]\n",
      "[Epoch 1/5] [Batch 669/938] [D loss: 0.574206531047821] [G loss: 1.0685648918151855]\n",
      "[Epoch 1/5] [Batch 670/938] [D loss: 0.5377157330513] [G loss: 1.1533406972885132]\n",
      "[Epoch 1/5] [Batch 671/938] [D loss: 0.5543572902679443] [G loss: 1.0716707706451416]\n",
      "[Epoch 1/5] [Batch 672/938] [D loss: 0.5295767188072205] [G loss: 1.0197644233703613]\n",
      "[Epoch 1/5] [Batch 673/938] [D loss: 0.5497604608535767] [G loss: 1.559147596359253]\n",
      "[Epoch 1/5] [Batch 674/938] [D loss: 0.6271764039993286] [G loss: 0.5512357950210571]\n",
      "[Epoch 1/5] [Batch 675/938] [D loss: 0.5952146649360657] [G loss: 1.8956711292266846]\n",
      "[Epoch 1/5] [Batch 676/938] [D loss: 0.5642744302749634] [G loss: 1.165531873703003]\n",
      "[Epoch 1/5] [Batch 677/938] [D loss: 0.5908092260360718] [G loss: 0.7669132947921753]\n",
      "[Epoch 1/5] [Batch 678/938] [D loss: 0.5613834261894226] [G loss: 1.6093425750732422]\n",
      "[Epoch 1/5] [Batch 679/938] [D loss: 0.5646311640739441] [G loss: 0.9259666204452515]\n",
      "[Epoch 1/5] [Batch 680/938] [D loss: 0.5464222431182861] [G loss: 0.9727269411087036]\n",
      "[Epoch 1/5] [Batch 681/938] [D loss: 0.5662409067153931] [G loss: 1.268103837966919]\n",
      "[Epoch 1/5] [Batch 682/938] [D loss: 0.5518196225166321] [G loss: 1.0855571031570435]\n",
      "[Epoch 1/5] [Batch 683/938] [D loss: 0.5676699280738831] [G loss: 0.8285189270973206]\n",
      "[Epoch 1/5] [Batch 684/938] [D loss: 0.5510638952255249] [G loss: 1.4449461698532104]\n",
      "[Epoch 1/5] [Batch 685/938] [D loss: 0.5535923838615417] [G loss: 0.9514689445495605]\n",
      "[Epoch 1/5] [Batch 686/938] [D loss: 0.49264106154441833] [G loss: 1.2284166812896729]\n",
      "[Epoch 1/5] [Batch 687/938] [D loss: 0.567102313041687] [G loss: 1.2417880296707153]\n",
      "[Epoch 1/5] [Batch 688/938] [D loss: 0.5547733306884766] [G loss: 1.052480697631836]\n",
      "[Epoch 1/5] [Batch 689/938] [D loss: 0.5669354796409607] [G loss: 1.6367708444595337]\n",
      "[Epoch 1/5] [Batch 690/938] [D loss: 0.7361435890197754] [G loss: 0.5276539921760559]\n",
      "[Epoch 1/5] [Batch 691/938] [D loss: 0.5133079886436462] [G loss: 1.5380773544311523]\n",
      "[Epoch 1/5] [Batch 692/938] [D loss: 0.5597738027572632] [G loss: 1.2003709077835083]\n",
      "[Epoch 1/5] [Batch 693/938] [D loss: 0.5344810485839844] [G loss: 1.0072027444839478]\n",
      "[Epoch 1/5] [Batch 694/938] [D loss: 0.585991621017456] [G loss: 0.7853776216506958]\n",
      "[Epoch 1/5] [Batch 695/938] [D loss: 0.5199097394943237] [G loss: 1.1010850667953491]\n",
      "[Epoch 1/5] [Batch 696/938] [D loss: 0.5880931615829468] [G loss: 1.2379597425460815]\n",
      "[Epoch 1/5] [Batch 697/938] [D loss: 0.5582805871963501] [G loss: 0.8897193670272827]\n",
      "[Epoch 1/5] [Batch 698/938] [D loss: 0.5841575264930725] [G loss: 0.9954223036766052]\n",
      "[Epoch 1/5] [Batch 699/938] [D loss: 0.5233854651451111] [G loss: 1.2312674522399902]\n",
      "[Epoch 1/5] [Batch 700/938] [D loss: 0.5496571063995361] [G loss: 1.0613048076629639]\n",
      "[Epoch 1/5] [Batch 701/938] [D loss: 0.5207968354225159] [G loss: 1.193324089050293]\n",
      "[Epoch 1/5] [Batch 702/938] [D loss: 0.6053585410118103] [G loss: 1.0964703559875488]\n",
      "[Epoch 1/5] [Batch 703/938] [D loss: 0.570121169090271] [G loss: 0.9281081557273865]\n",
      "[Epoch 1/5] [Batch 704/938] [D loss: 0.5944732427597046] [G loss: 1.2242710590362549]\n",
      "[Epoch 1/5] [Batch 705/938] [D loss: 0.633689284324646] [G loss: 0.7187879681587219]\n",
      "[Epoch 1/5] [Batch 706/938] [D loss: 0.6413780450820923] [G loss: 1.5106576681137085]\n",
      "[Epoch 1/5] [Batch 707/938] [D loss: 0.5909106731414795] [G loss: 0.6691688895225525]\n",
      "[Epoch 1/5] [Batch 708/938] [D loss: 0.5022018551826477] [G loss: 1.1880874633789062]\n",
      "[Epoch 1/5] [Batch 709/938] [D loss: 0.5220347046852112] [G loss: 1.3312482833862305]\n",
      "[Epoch 1/5] [Batch 710/938] [D loss: 0.5959649085998535] [G loss: 0.8887996673583984]\n",
      "[Epoch 1/5] [Batch 711/938] [D loss: 0.5529612898826599] [G loss: 1.1010488271713257]\n",
      "[Epoch 1/5] [Batch 712/938] [D loss: 0.5967986583709717] [G loss: 0.9651120901107788]\n",
      "[Epoch 1/5] [Batch 713/938] [D loss: 0.5587273240089417] [G loss: 1.0262527465820312]\n",
      "[Epoch 1/5] [Batch 714/938] [D loss: 0.6020890474319458] [G loss: 1.0210050344467163]\n",
      "[Epoch 1/5] [Batch 715/938] [D loss: 0.5763732194900513] [G loss: 0.9018878936767578]\n",
      "[Epoch 1/5] [Batch 716/938] [D loss: 0.5972927212715149] [G loss: 1.1900250911712646]\n",
      "[Epoch 1/5] [Batch 717/938] [D loss: 0.5649317502975464] [G loss: 0.9391560554504395]\n",
      "[Epoch 1/5] [Batch 718/938] [D loss: 0.5394846200942993] [G loss: 1.0256649255752563]\n",
      "[Epoch 1/5] [Batch 719/938] [D loss: 0.5137338638305664] [G loss: 1.144624948501587]\n",
      "[Epoch 1/5] [Batch 720/938] [D loss: 0.6207711696624756] [G loss: 1.1923885345458984]\n",
      "[Epoch 1/5] [Batch 721/938] [D loss: 0.5306225419044495] [G loss: 1.0534502267837524]\n",
      "[Epoch 1/5] [Batch 722/938] [D loss: 0.5116779804229736] [G loss: 1.569573163986206]\n",
      "[Epoch 1/5] [Batch 723/938] [D loss: 0.5618109703063965] [G loss: 0.9578710198402405]\n",
      "[Epoch 1/5] [Batch 724/938] [D loss: 0.5383655428886414] [G loss: 1.1602709293365479]\n",
      "[Epoch 1/5] [Batch 725/938] [D loss: 0.5361686944961548] [G loss: 1.0162413120269775]\n",
      "[Epoch 1/5] [Batch 726/938] [D loss: 0.5298157930374146] [G loss: 1.1625497341156006]\n",
      "[Epoch 1/5] [Batch 727/938] [D loss: 0.46624985337257385] [G loss: 1.082017421722412]\n",
      "[Epoch 1/5] [Batch 728/938] [D loss: 0.5605217218399048] [G loss: 1.6500182151794434]\n",
      "[Epoch 1/5] [Batch 729/938] [D loss: 0.70192551612854] [G loss: 0.4736265242099762]\n",
      "[Epoch 1/5] [Batch 730/938] [D loss: 0.6150951981544495] [G loss: 1.7032828330993652]\n",
      "[Epoch 1/5] [Batch 731/938] [D loss: 0.5870144367218018] [G loss: 0.9029147624969482]\n",
      "[Epoch 1/5] [Batch 732/938] [D loss: 0.564297080039978] [G loss: 0.9319428205490112]\n",
      "[Epoch 1/5] [Batch 733/938] [D loss: 0.5404189825057983] [G loss: 1.3443658351898193]\n",
      "[Epoch 1/5] [Batch 734/938] [D loss: 0.5589026212692261] [G loss: 1.2893202304840088]\n",
      "[Epoch 1/5] [Batch 735/938] [D loss: 0.5399185419082642] [G loss: 1.0186498165130615]\n",
      "[Epoch 1/5] [Batch 736/938] [D loss: 0.552696168422699] [G loss: 0.9531562328338623]\n",
      "[Epoch 1/5] [Batch 737/938] [D loss: 0.5090235471725464] [G loss: 1.4047350883483887]\n",
      "[Epoch 1/5] [Batch 738/938] [D loss: 0.5443271398544312] [G loss: 1.1075477600097656]\n",
      "[Epoch 1/5] [Batch 739/938] [D loss: 0.5381898880004883] [G loss: 0.9363042712211609]\n",
      "[Epoch 1/5] [Batch 740/938] [D loss: 0.6169441938400269] [G loss: 1.5204923152923584]\n",
      "[Epoch 1/5] [Batch 741/938] [D loss: 0.6016303300857544] [G loss: 0.7792915105819702]\n",
      "[Epoch 1/5] [Batch 742/938] [D loss: 0.6221240758895874] [G loss: 1.5858924388885498]\n",
      "[Epoch 1/5] [Batch 743/938] [D loss: 0.5772423148155212] [G loss: 0.785967230796814]\n",
      "[Epoch 1/5] [Batch 744/938] [D loss: 0.5028756856918335] [G loss: 1.1345199346542358]\n",
      "[Epoch 1/5] [Batch 745/938] [D loss: 0.5975321531295776] [G loss: 1.1327605247497559]\n",
      "[Epoch 1/5] [Batch 746/938] [D loss: 0.5521471500396729] [G loss: 0.9241174459457397]\n",
      "[Epoch 1/5] [Batch 747/938] [D loss: 0.5015590786933899] [G loss: 1.3263983726501465]\n",
      "[Epoch 1/5] [Batch 748/938] [D loss: 0.562002420425415] [G loss: 1.078827142715454]\n",
      "[Epoch 1/5] [Batch 749/938] [D loss: 0.5917786359786987] [G loss: 1.0527658462524414]\n",
      "[Epoch 1/5] [Batch 750/938] [D loss: 0.5705912113189697] [G loss: 1.2604964971542358]\n",
      "[Epoch 1/5] [Batch 751/938] [D loss: 0.5022735595703125] [G loss: 1.041374683380127]\n",
      "[Epoch 1/5] [Batch 752/938] [D loss: 0.49407562613487244] [G loss: 1.007476806640625]\n",
      "[Epoch 1/5] [Batch 753/938] [D loss: 0.5293765664100647] [G loss: 1.1859321594238281]\n",
      "[Epoch 1/5] [Batch 754/938] [D loss: 0.5225610733032227] [G loss: 0.9679176211357117]\n",
      "[Epoch 1/5] [Batch 755/938] [D loss: 0.4948362410068512] [G loss: 1.5384124517440796]\n",
      "[Epoch 1/5] [Batch 756/938] [D loss: 0.5645414590835571] [G loss: 1.0593076944351196]\n",
      "[Epoch 1/5] [Batch 757/938] [D loss: 0.5017232298851013] [G loss: 1.3040144443511963]\n",
      "[Epoch 1/5] [Batch 758/938] [D loss: 0.5599294304847717] [G loss: 0.8470200300216675]\n",
      "[Epoch 1/5] [Batch 759/938] [D loss: 0.5434418320655823] [G loss: 1.6077611446380615]\n",
      "[Epoch 1/5] [Batch 760/938] [D loss: 0.5066359639167786] [G loss: 1.1506788730621338]\n",
      "[Epoch 1/5] [Batch 761/938] [D loss: 0.49729621410369873] [G loss: 1.2564637660980225]\n",
      "[Epoch 1/5] [Batch 762/938] [D loss: 0.5120748281478882] [G loss: 1.1595284938812256]\n",
      "[Epoch 1/5] [Batch 763/938] [D loss: 0.4872855246067047] [G loss: 1.2808942794799805]\n",
      "[Epoch 1/5] [Batch 764/938] [D loss: 0.4535420536994934] [G loss: 1.265726089477539]\n",
      "[Epoch 1/5] [Batch 765/938] [D loss: 0.5723433494567871] [G loss: 1.4322428703308105]\n",
      "[Epoch 1/5] [Batch 766/938] [D loss: 0.5719648599624634] [G loss: 0.9950149059295654]\n",
      "[Epoch 1/5] [Batch 767/938] [D loss: 0.5989620089530945] [G loss: 1.2796673774719238]\n",
      "[Epoch 1/5] [Batch 768/938] [D loss: 0.5348724722862244] [G loss: 1.0246407985687256]\n",
      "[Epoch 1/5] [Batch 769/938] [D loss: 0.6428672075271606] [G loss: 1.7469714879989624]\n",
      "[Epoch 1/5] [Batch 770/938] [D loss: 0.8240718841552734] [G loss: 0.39369726181030273]\n",
      "[Epoch 1/5] [Batch 771/938] [D loss: 0.6801517605781555] [G loss: 2.0924999713897705]\n",
      "[Epoch 1/5] [Batch 772/938] [D loss: 0.5024406909942627] [G loss: 1.1235113143920898]\n",
      "[Epoch 1/5] [Batch 773/938] [D loss: 0.5566559433937073] [G loss: 0.9092273712158203]\n",
      "[Epoch 1/5] [Batch 774/938] [D loss: 0.5681167840957642] [G loss: 1.011181354522705]\n",
      "[Epoch 1/5] [Batch 775/938] [D loss: 0.5381156206130981] [G loss: 1.3215596675872803]\n",
      "[Epoch 1/5] [Batch 776/938] [D loss: 0.461048424243927] [G loss: 1.1319135427474976]\n",
      "[Epoch 1/5] [Batch 777/938] [D loss: 0.5145429372787476] [G loss: 1.0779261589050293]\n",
      "[Epoch 1/5] [Batch 778/938] [D loss: 0.5886063575744629] [G loss: 1.0942177772521973]\n",
      "[Epoch 1/5] [Batch 779/938] [D loss: 0.5591212511062622] [G loss: 1.045219898223877]\n",
      "[Epoch 1/5] [Batch 780/938] [D loss: 0.5739306807518005] [G loss: 1.0126385688781738]\n",
      "[Epoch 1/5] [Batch 781/938] [D loss: 0.5616117119789124] [G loss: 1.6192516088485718]\n",
      "[Epoch 1/5] [Batch 782/938] [D loss: 0.5399366617202759] [G loss: 0.7469794154167175]\n",
      "[Epoch 1/5] [Batch 783/938] [D loss: 0.5619317889213562] [G loss: 1.3513363599777222]\n",
      "[Epoch 1/5] [Batch 784/938] [D loss: 0.5292119979858398] [G loss: 1.5864530801773071]\n",
      "[Epoch 1/5] [Batch 785/938] [D loss: 0.5398464202880859] [G loss: 0.8235283493995667]\n",
      "[Epoch 1/5] [Batch 786/938] [D loss: 0.5367407202720642] [G loss: 1.3348453044891357]\n",
      "[Epoch 1/5] [Batch 787/938] [D loss: 0.5307709574699402] [G loss: 0.809043288230896]\n",
      "[Epoch 1/5] [Batch 788/938] [D loss: 0.5631565451622009] [G loss: 1.2039713859558105]\n",
      "[Epoch 1/5] [Batch 789/938] [D loss: 0.5697135925292969] [G loss: 1.2967472076416016]\n",
      "[Epoch 1/5] [Batch 790/938] [D loss: 0.5507376790046692] [G loss: 1.011179804801941]\n",
      "[Epoch 1/5] [Batch 791/938] [D loss: 0.5780444145202637] [G loss: 1.2526077032089233]\n",
      "[Epoch 1/5] [Batch 792/938] [D loss: 0.5698190331459045] [G loss: 0.9920353889465332]\n",
      "[Epoch 1/5] [Batch 793/938] [D loss: 0.58525550365448] [G loss: 1.5394724607467651]\n",
      "[Epoch 1/5] [Batch 794/938] [D loss: 0.6211978197097778] [G loss: 0.8238173723220825]\n",
      "[Epoch 1/5] [Batch 795/938] [D loss: 0.5372125506401062] [G loss: 1.6445536613464355]\n",
      "[Epoch 1/5] [Batch 796/938] [D loss: 0.5368918776512146] [G loss: 0.8753759264945984]\n",
      "[Epoch 1/5] [Batch 797/938] [D loss: 0.5602095723152161] [G loss: 1.2274757623672485]\n",
      "[Epoch 1/5] [Batch 798/938] [D loss: 0.5629305839538574] [G loss: 0.9901681542396545]\n",
      "[Epoch 1/5] [Batch 799/938] [D loss: 0.6370302438735962] [G loss: 1.0187000036239624]\n",
      "[Epoch 1/5] [Batch 800/938] [D loss: 0.589942216873169] [G loss: 1.3615752458572388]\n",
      "[Epoch 1/5] [Batch 801/938] [D loss: 0.5373636484146118] [G loss: 0.892770528793335]\n",
      "[Epoch 1/5] [Batch 802/938] [D loss: 0.5412217378616333] [G loss: 1.346081018447876]\n",
      "[Epoch 1/5] [Batch 803/938] [D loss: 0.5304887890815735] [G loss: 1.2086164951324463]\n",
      "[Epoch 1/5] [Batch 804/938] [D loss: 0.5024921894073486] [G loss: 1.0977449417114258]\n",
      "[Epoch 1/5] [Batch 805/938] [D loss: 0.5930278301239014] [G loss: 1.0575592517852783]\n",
      "[Epoch 1/5] [Batch 806/938] [D loss: 0.5708821415901184] [G loss: 1.1540093421936035]\n",
      "[Epoch 1/5] [Batch 807/938] [D loss: 0.5590118765830994] [G loss: 1.2447577714920044]\n",
      "[Epoch 1/5] [Batch 808/938] [D loss: 0.5761768817901611] [G loss: 0.8699621558189392]\n",
      "[Epoch 1/5] [Batch 809/938] [D loss: 0.6030246615409851] [G loss: 1.564502477645874]\n",
      "[Epoch 1/5] [Batch 810/938] [D loss: 0.5912154912948608] [G loss: 0.8416557312011719]\n",
      "[Epoch 1/5] [Batch 811/938] [D loss: 0.5569356679916382] [G loss: 1.067591905593872]\n",
      "[Epoch 1/5] [Batch 812/938] [D loss: 0.5322851538658142] [G loss: 1.3407319784164429]\n",
      "[Epoch 1/5] [Batch 813/938] [D loss: 0.5293756127357483] [G loss: 0.9889969825744629]\n",
      "[Epoch 1/5] [Batch 814/938] [D loss: 0.5843477249145508] [G loss: 0.9492037892341614]\n",
      "[Epoch 1/5] [Batch 815/938] [D loss: 0.5591251850128174] [G loss: 1.319730281829834]\n",
      "[Epoch 1/5] [Batch 816/938] [D loss: 0.5266120433807373] [G loss: 1.1275198459625244]\n",
      "[Epoch 1/5] [Batch 817/938] [D loss: 0.5332270264625549] [G loss: 1.218485713005066]\n",
      "[Epoch 1/5] [Batch 818/938] [D loss: 0.5845239162445068] [G loss: 0.9865665435791016]\n",
      "[Epoch 1/5] [Batch 819/938] [D loss: 0.6294595003128052] [G loss: 1.327460765838623]\n",
      "[Epoch 1/5] [Batch 820/938] [D loss: 0.5957015156745911] [G loss: 0.7449542880058289]\n",
      "[Epoch 1/5] [Batch 821/938] [D loss: 0.5921930074691772] [G loss: 1.2579493522644043]\n",
      "[Epoch 1/5] [Batch 822/938] [D loss: 0.6211100816726685] [G loss: 0.9132181406021118]\n",
      "[Epoch 1/5] [Batch 823/938] [D loss: 0.6490557193756104] [G loss: 1.0542049407958984]\n",
      "[Epoch 1/5] [Batch 824/938] [D loss: 0.5248454213142395] [G loss: 1.1934926509857178]\n",
      "[Epoch 1/5] [Batch 825/938] [D loss: 0.5603169202804565] [G loss: 1.0086212158203125]\n",
      "[Epoch 1/5] [Batch 826/938] [D loss: 0.6267723441123962] [G loss: 1.2611674070358276]\n",
      "[Epoch 1/5] [Batch 827/938] [D loss: 0.5766910314559937] [G loss: 0.8111496567726135]\n",
      "[Epoch 1/5] [Batch 828/938] [D loss: 0.604601263999939] [G loss: 1.2793904542922974]\n",
      "[Epoch 1/5] [Batch 829/938] [D loss: 0.49805936217308044] [G loss: 1.1570422649383545]\n",
      "[Epoch 1/5] [Batch 830/938] [D loss: 0.5937393307685852] [G loss: 0.8249043226242065]\n",
      "[Epoch 1/5] [Batch 831/938] [D loss: 0.5378050208091736] [G loss: 1.3862299919128418]\n",
      "[Epoch 1/5] [Batch 832/938] [D loss: 0.5389076471328735] [G loss: 0.9582672715187073]\n",
      "[Epoch 1/5] [Batch 833/938] [D loss: 0.4981144368648529] [G loss: 1.1877820491790771]\n",
      "[Epoch 1/5] [Batch 834/938] [D loss: 0.565829873085022] [G loss: 1.049102544784546]\n",
      "[Epoch 1/5] [Batch 835/938] [D loss: 0.5499274730682373] [G loss: 0.9633097648620605]\n",
      "[Epoch 1/5] [Batch 836/938] [D loss: 0.6448566317558289] [G loss: 1.2824636697769165]\n",
      "[Epoch 1/5] [Batch 837/938] [D loss: 0.6259309649467468] [G loss: 0.941627025604248]\n",
      "[Epoch 1/5] [Batch 838/938] [D loss: 0.5120797753334045] [G loss: 1.1498491764068604]\n",
      "[Epoch 1/5] [Batch 839/938] [D loss: 0.5118387937545776] [G loss: 1.2694722414016724]\n",
      "[Epoch 1/5] [Batch 840/938] [D loss: 0.5820744037628174] [G loss: 1.0105592012405396]\n",
      "[Epoch 1/5] [Batch 841/938] [D loss: 0.5135020017623901] [G loss: 1.1566712856292725]\n",
      "[Epoch 1/5] [Batch 842/938] [D loss: 0.5422766208648682] [G loss: 1.241132378578186]\n",
      "[Epoch 1/5] [Batch 843/938] [D loss: 0.5075462460517883] [G loss: 1.1922565698623657]\n",
      "[Epoch 1/5] [Batch 844/938] [D loss: 0.5747213363647461] [G loss: 0.9677421450614929]\n",
      "[Epoch 1/5] [Batch 845/938] [D loss: 0.5833702087402344] [G loss: 1.1773899793624878]\n",
      "[Epoch 1/5] [Batch 846/938] [D loss: 0.578660786151886] [G loss: 0.8372371792793274]\n",
      "[Epoch 1/5] [Batch 847/938] [D loss: 0.6194169521331787] [G loss: 1.4626072645187378]\n",
      "[Epoch 1/5] [Batch 848/938] [D loss: 0.5810163021087646] [G loss: 0.6770203113555908]\n",
      "[Epoch 1/5] [Batch 849/938] [D loss: 0.5044013261795044] [G loss: 0.9909182786941528]\n",
      "[Epoch 1/5] [Batch 850/938] [D loss: 0.6120076775550842] [G loss: 1.468658447265625]\n",
      "[Epoch 1/5] [Batch 851/938] [D loss: 0.6112474799156189] [G loss: 0.6780195236206055]\n",
      "[Epoch 1/5] [Batch 852/938] [D loss: 0.5286843776702881] [G loss: 1.3639416694641113]\n",
      "[Epoch 1/5] [Batch 853/938] [D loss: 0.5822453498840332] [G loss: 1.2072083950042725]\n",
      "[Epoch 1/5] [Batch 854/938] [D loss: 0.6478687524795532] [G loss: 0.8011744022369385]\n",
      "[Epoch 1/5] [Batch 855/938] [D loss: 0.5260297060012817] [G loss: 1.2675999402999878]\n",
      "[Epoch 1/5] [Batch 856/938] [D loss: 0.5599806308746338] [G loss: 1.1209204196929932]\n",
      "[Epoch 1/5] [Batch 857/938] [D loss: 0.5741047263145447] [G loss: 0.9372418522834778]\n",
      "[Epoch 1/5] [Batch 858/938] [D loss: 0.5190781950950623] [G loss: 1.2741738557815552]\n",
      "[Epoch 1/5] [Batch 859/938] [D loss: 0.5562124848365784] [G loss: 1.0018129348754883]\n",
      "[Epoch 1/5] [Batch 860/938] [D loss: 0.5559574961662292] [G loss: 0.9447612166404724]\n",
      "[Epoch 1/5] [Batch 861/938] [D loss: 0.5640630125999451] [G loss: 1.3467495441436768]\n",
      "[Epoch 1/5] [Batch 862/938] [D loss: 0.5216653347015381] [G loss: 0.7487212419509888]\n",
      "[Epoch 1/5] [Batch 863/938] [D loss: 0.608918309211731] [G loss: 1.288527250289917]\n",
      "[Epoch 1/5] [Batch 864/938] [D loss: 0.48146921396255493] [G loss: 0.9994457960128784]\n",
      "[Epoch 1/5] [Batch 865/938] [D loss: 0.5554817914962769] [G loss: 0.9020206928253174]\n",
      "[Epoch 1/5] [Batch 866/938] [D loss: 0.5635374188423157] [G loss: 1.3755104541778564]\n",
      "[Epoch 1/5] [Batch 867/938] [D loss: 0.6551562547683716] [G loss: 0.8628954887390137]\n",
      "[Epoch 1/5] [Batch 868/938] [D loss: 0.5422319173812866] [G loss: 0.9689016938209534]\n",
      "[Epoch 1/5] [Batch 869/938] [D loss: 0.5407233238220215] [G loss: 1.599637508392334]\n",
      "[Epoch 1/5] [Batch 870/938] [D loss: 0.5367926359176636] [G loss: 0.9039046168327332]\n",
      "[Epoch 1/5] [Batch 871/938] [D loss: 0.5533929467201233] [G loss: 1.4611018896102905]\n",
      "[Epoch 1/5] [Batch 872/938] [D loss: 0.5658506155014038] [G loss: 0.7670124769210815]\n",
      "[Epoch 1/5] [Batch 873/938] [D loss: 0.6503535509109497] [G loss: 1.5951552391052246]\n",
      "[Epoch 1/5] [Batch 874/938] [D loss: 0.6060138940811157] [G loss: 0.6644617915153503]\n",
      "[Epoch 1/5] [Batch 875/938] [D loss: 0.5661698579788208] [G loss: 1.2427661418914795]\n",
      "[Epoch 1/5] [Batch 876/938] [D loss: 0.5209217071533203] [G loss: 1.2098207473754883]\n",
      "[Epoch 1/5] [Batch 877/938] [D loss: 0.5568910837173462] [G loss: 1.213633418083191]\n",
      "[Epoch 1/5] [Batch 878/938] [D loss: 0.6231526136398315] [G loss: 0.8832132816314697]\n",
      "[Epoch 1/5] [Batch 879/938] [D loss: 0.49466556310653687] [G loss: 1.105934739112854]\n",
      "[Epoch 1/5] [Batch 880/938] [D loss: 0.5728803277015686] [G loss: 1.4464130401611328]\n",
      "[Epoch 1/5] [Batch 881/938] [D loss: 0.5776888728141785] [G loss: 0.7947550415992737]\n",
      "[Epoch 1/5] [Batch 882/938] [D loss: 0.5222183465957642] [G loss: 1.041571855545044]\n",
      "[Epoch 1/5] [Batch 883/938] [D loss: 0.6188079714775085] [G loss: 1.7893723249435425]\n",
      "[Epoch 1/5] [Batch 884/938] [D loss: 0.5999025702476501] [G loss: 0.7518690824508667]\n",
      "[Epoch 1/5] [Batch 885/938] [D loss: 0.5583502054214478] [G loss: 1.0183475017547607]\n",
      "[Epoch 1/5] [Batch 886/938] [D loss: 0.5031156539916992] [G loss: 1.361377239227295]\n",
      "[Epoch 1/5] [Batch 887/938] [D loss: 0.5649540424346924] [G loss: 1.0282857418060303]\n",
      "[Epoch 1/5] [Batch 888/938] [D loss: 0.5952651500701904] [G loss: 0.9334279298782349]\n",
      "[Epoch 1/5] [Batch 889/938] [D loss: 0.5122897624969482] [G loss: 1.216374158859253]\n",
      "[Epoch 1/5] [Batch 890/938] [D loss: 0.5619515776634216] [G loss: 1.107694149017334]\n",
      "[Epoch 1/5] [Batch 891/938] [D loss: 0.5670950412750244] [G loss: 1.1979069709777832]\n",
      "[Epoch 1/5] [Batch 892/938] [D loss: 0.5283634066581726] [G loss: 0.9267029166221619]\n",
      "[Epoch 1/5] [Batch 893/938] [D loss: 0.5697209239006042] [G loss: 1.2277733087539673]\n",
      "[Epoch 1/5] [Batch 894/938] [D loss: 0.6752166748046875] [G loss: 1.0343879461288452]\n",
      "[Epoch 1/5] [Batch 895/938] [D loss: 0.6001587510108948] [G loss: 0.9253050088882446]\n",
      "[Epoch 1/5] [Batch 896/938] [D loss: 0.5937828421592712] [G loss: 1.2320926189422607]\n",
      "[Epoch 1/5] [Batch 897/938] [D loss: 0.5661950707435608] [G loss: 0.9327644109725952]\n",
      "[Epoch 1/5] [Batch 898/938] [D loss: 0.49486491084098816] [G loss: 1.4419649839401245]\n",
      "[Epoch 1/5] [Batch 899/938] [D loss: 0.5591810941696167] [G loss: 1.0623178482055664]\n",
      "[Epoch 1/5] [Batch 900/938] [D loss: 0.5290732979774475] [G loss: 1.236720085144043]\n",
      "[Epoch 1/5] [Batch 901/938] [D loss: 0.5644779205322266] [G loss: 0.992942214012146]\n",
      "[Epoch 1/5] [Batch 902/938] [D loss: 0.5368146896362305] [G loss: 1.1589808464050293]\n",
      "[Epoch 1/5] [Batch 903/938] [D loss: 0.5536219477653503] [G loss: 1.3839209079742432]\n",
      "[Epoch 1/5] [Batch 904/938] [D loss: 0.5086382627487183] [G loss: 0.8106549382209778]\n",
      "[Epoch 1/5] [Batch 905/938] [D loss: 0.5408554077148438] [G loss: 1.6149039268493652]\n",
      "[Epoch 1/5] [Batch 906/938] [D loss: 0.6006446480751038] [G loss: 0.9142467379570007]\n",
      "[Epoch 1/5] [Batch 907/938] [D loss: 0.6106106042861938] [G loss: 1.2376052141189575]\n",
      "[Epoch 1/5] [Batch 908/938] [D loss: 0.5634973049163818] [G loss: 1.0296133756637573]\n",
      "[Epoch 1/5] [Batch 909/938] [D loss: 0.6742194890975952] [G loss: 1.55089271068573]\n",
      "[Epoch 1/5] [Batch 910/938] [D loss: 0.700968861579895] [G loss: 0.49181056022644043]\n",
      "[Epoch 1/5] [Batch 911/938] [D loss: 0.5206753015518188] [G loss: 1.1159236431121826]\n",
      "[Epoch 1/5] [Batch 912/938] [D loss: 0.5082738399505615] [G loss: 1.3490245342254639]\n",
      "[Epoch 1/5] [Batch 913/938] [D loss: 0.5272465944290161] [G loss: 0.8925298452377319]\n",
      "[Epoch 1/5] [Batch 914/938] [D loss: 0.4971601366996765] [G loss: 1.154505729675293]\n",
      "[Epoch 1/5] [Batch 915/938] [D loss: 0.4860665500164032] [G loss: 1.0793421268463135]\n",
      "[Epoch 1/5] [Batch 916/938] [D loss: 0.5245195031166077] [G loss: 1.3380440473556519]\n",
      "[Epoch 1/5] [Batch 917/938] [D loss: 0.5596045255661011] [G loss: 0.7184312343597412]\n",
      "[Epoch 1/5] [Batch 918/938] [D loss: 0.5422940850257874] [G loss: 1.5105907917022705]\n",
      "[Epoch 1/5] [Batch 919/938] [D loss: 0.5706614851951599] [G loss: 1.14888334274292]\n",
      "[Epoch 1/5] [Batch 920/938] [D loss: 0.5460149049758911] [G loss: 0.9518230557441711]\n",
      "[Epoch 1/5] [Batch 921/938] [D loss: 0.5298423767089844] [G loss: 1.0078884363174438]\n",
      "[Epoch 1/5] [Batch 922/938] [D loss: 0.5579990148544312] [G loss: 1.3174782991409302]\n",
      "[Epoch 1/5] [Batch 923/938] [D loss: 0.554302453994751] [G loss: 0.7296880483627319]\n",
      "[Epoch 1/5] [Batch 924/938] [D loss: 0.5208514928817749] [G loss: 1.1670277118682861]\n",
      "[Epoch 1/5] [Batch 925/938] [D loss: 0.5577026605606079] [G loss: 1.7927114963531494]\n",
      "[Epoch 1/5] [Batch 926/938] [D loss: 0.7692145109176636] [G loss: 0.39919543266296387]\n",
      "[Epoch 1/5] [Batch 927/938] [D loss: 0.696635365486145] [G loss: 1.722710132598877]\n",
      "[Epoch 1/5] [Batch 928/938] [D loss: 0.5476167798042297] [G loss: 0.956535816192627]\n",
      "[Epoch 1/5] [Batch 929/938] [D loss: 0.5153518915176392] [G loss: 0.8774844408035278]\n",
      "[Epoch 1/5] [Batch 930/938] [D loss: 0.5769515037536621] [G loss: 1.1655091047286987]\n",
      "[Epoch 1/5] [Batch 931/938] [D loss: 0.5524617433547974] [G loss: 0.9524223804473877]\n",
      "[Epoch 1/5] [Batch 932/938] [D loss: 0.5504271984100342] [G loss: 1.164940595626831]\n",
      "[Epoch 1/5] [Batch 933/938] [D loss: 0.6105824708938599] [G loss: 0.8365172147750854]\n",
      "[Epoch 1/5] [Batch 934/938] [D loss: 0.5170831084251404] [G loss: 1.5275505781173706]\n",
      "[Epoch 1/5] [Batch 935/938] [D loss: 0.4939790964126587] [G loss: 1.109645128250122]\n",
      "[Epoch 1/5] [Batch 936/938] [D loss: 0.6142215132713318] [G loss: 0.8446749448776245]\n",
      "[Epoch 1/5] [Batch 937/938] [D loss: 0.52333003282547] [G loss: 1.3901093006134033]\n",
      "[Epoch 2/5] [Batch 0/938] [D loss: 0.5433744192123413] [G loss: 1.0581222772598267]\n",
      "[Epoch 2/5] [Batch 1/938] [D loss: 0.5734750628471375] [G loss: 0.8283332586288452]\n",
      "[Epoch 2/5] [Batch 2/938] [D loss: 0.5092591047286987] [G loss: 1.561096429824829]\n",
      "[Epoch 2/5] [Batch 3/938] [D loss: 0.5498656034469604] [G loss: 1.0107579231262207]\n",
      "[Epoch 2/5] [Batch 4/938] [D loss: 0.6028492450714111] [G loss: 0.8597047328948975]\n",
      "[Epoch 2/5] [Batch 5/938] [D loss: 0.5851229429244995] [G loss: 1.3692697286605835]\n",
      "[Epoch 2/5] [Batch 6/938] [D loss: 0.5563172101974487] [G loss: 0.9641295671463013]\n",
      "[Epoch 2/5] [Batch 7/938] [D loss: 0.5272003412246704] [G loss: 0.9468703866004944]\n",
      "[Epoch 2/5] [Batch 8/938] [D loss: 0.547350287437439] [G loss: 1.2722340822219849]\n",
      "[Epoch 2/5] [Batch 9/938] [D loss: 0.5963424444198608] [G loss: 1.1636004447937012]\n",
      "[Epoch 2/5] [Batch 10/938] [D loss: 0.5665781497955322] [G loss: 0.8429204821586609]\n",
      "[Epoch 2/5] [Batch 11/938] [D loss: 0.5874345898628235] [G loss: 0.9809783101081848]\n",
      "[Epoch 2/5] [Batch 12/938] [D loss: 0.5137079358100891] [G loss: 1.2134928703308105]\n",
      "[Epoch 2/5] [Batch 13/938] [D loss: 0.5910032987594604] [G loss: 0.8242809772491455]\n",
      "[Epoch 2/5] [Batch 14/938] [D loss: 0.5546447038650513] [G loss: 1.319006323814392]\n",
      "[Epoch 2/5] [Batch 15/938] [D loss: 0.5533322095870972] [G loss: 1.5756092071533203]\n",
      "[Epoch 2/5] [Batch 16/938] [D loss: 0.6436474919319153] [G loss: 0.7736294269561768]\n",
      "[Epoch 2/5] [Batch 17/938] [D loss: 0.5632619857788086] [G loss: 1.691500186920166]\n",
      "[Epoch 2/5] [Batch 18/938] [D loss: 0.581473708152771] [G loss: 0.9803583025932312]\n",
      "[Epoch 2/5] [Batch 19/938] [D loss: 0.6604434847831726] [G loss: 0.6638197302818298]\n",
      "[Epoch 2/5] [Batch 20/938] [D loss: 0.5857268571853638] [G loss: 1.2818635702133179]\n",
      "[Epoch 2/5] [Batch 21/938] [D loss: 0.5455141067504883] [G loss: 1.2544513940811157]\n",
      "[Epoch 2/5] [Batch 22/938] [D loss: 0.6061583161354065] [G loss: 0.7648308873176575]\n",
      "[Epoch 2/5] [Batch 23/938] [D loss: 0.580376923084259] [G loss: 0.8765972852706909]\n",
      "[Epoch 2/5] [Batch 24/938] [D loss: 0.5226773023605347] [G loss: 1.266002893447876]\n",
      "[Epoch 2/5] [Batch 25/938] [D loss: 0.5654915571212769] [G loss: 1.309290885925293]\n",
      "[Epoch 2/5] [Batch 26/938] [D loss: 0.5899003744125366] [G loss: 0.8044546246528625]\n",
      "[Epoch 2/5] [Batch 27/938] [D loss: 0.5559762716293335] [G loss: 1.0529966354370117]\n",
      "[Epoch 2/5] [Batch 28/938] [D loss: 0.5957919955253601] [G loss: 1.415369987487793]\n",
      "[Epoch 2/5] [Batch 29/938] [D loss: 0.6012279987335205] [G loss: 0.9848979115486145]\n",
      "[Epoch 2/5] [Batch 30/938] [D loss: 0.542239248752594] [G loss: 0.8911207318305969]\n",
      "[Epoch 2/5] [Batch 31/938] [D loss: 0.6131134033203125] [G loss: 1.087415099143982]\n",
      "[Epoch 2/5] [Batch 32/938] [D loss: 0.6304625272750854] [G loss: 0.8407801389694214]\n",
      "[Epoch 2/5] [Batch 33/938] [D loss: 0.5990604162216187] [G loss: 1.0115388631820679]\n",
      "[Epoch 2/5] [Batch 34/938] [D loss: 0.5616741180419922] [G loss: 1.1456987857818604]\n",
      "[Epoch 2/5] [Batch 35/938] [D loss: 0.5508798956871033] [G loss: 0.8757283687591553]\n",
      "[Epoch 2/5] [Batch 36/938] [D loss: 0.5787162780761719] [G loss: 0.9567394256591797]\n",
      "[Epoch 2/5] [Batch 37/938] [D loss: 0.5113075375556946] [G loss: 1.1848106384277344]\n",
      "[Epoch 2/5] [Batch 38/938] [D loss: 0.6371883153915405] [G loss: 0.9806565642356873]\n",
      "[Epoch 2/5] [Batch 39/938] [D loss: 0.5848947167396545] [G loss: 1.112863302230835]\n",
      "[Epoch 2/5] [Batch 40/938] [D loss: 0.6394366025924683] [G loss: 1.107648253440857]\n",
      "[Epoch 2/5] [Batch 41/938] [D loss: 0.5700459480285645] [G loss: 0.869316816329956]\n",
      "[Epoch 2/5] [Batch 42/938] [D loss: 0.6660099029541016] [G loss: 0.9709248542785645]\n",
      "[Epoch 2/5] [Batch 43/938] [D loss: 0.5884476900100708] [G loss: 1.1556986570358276]\n",
      "[Epoch 2/5] [Batch 44/938] [D loss: 0.6209136843681335] [G loss: 0.6169863939285278]\n",
      "[Epoch 2/5] [Batch 45/938] [D loss: 0.5951202511787415] [G loss: 1.3287665843963623]\n",
      "[Epoch 2/5] [Batch 46/938] [D loss: 0.5199280977249146] [G loss: 1.0183368921279907]\n",
      "[Epoch 2/5] [Batch 47/938] [D loss: 0.5975385308265686] [G loss: 0.7894973158836365]\n",
      "[Epoch 2/5] [Batch 48/938] [D loss: 0.5243772864341736] [G loss: 1.196216344833374]\n",
      "[Epoch 2/5] [Batch 49/938] [D loss: 0.5974211692810059] [G loss: 1.0579214096069336]\n",
      "[Epoch 2/5] [Batch 50/938] [D loss: 0.5322229862213135] [G loss: 0.9893018007278442]\n",
      "[Epoch 2/5] [Batch 51/938] [D loss: 0.5352479219436646] [G loss: 1.2434219121932983]\n",
      "[Epoch 2/5] [Batch 52/938] [D loss: 0.48427891731262207] [G loss: 1.0771558284759521]\n",
      "[Epoch 2/5] [Batch 53/938] [D loss: 0.5623432397842407] [G loss: 1.1846510171890259]\n",
      "[Epoch 2/5] [Batch 54/938] [D loss: 0.5507798790931702] [G loss: 0.9063632488250732]\n",
      "[Epoch 2/5] [Batch 55/938] [D loss: 0.5180175304412842] [G loss: 1.0490952730178833]\n",
      "[Epoch 2/5] [Batch 56/938] [D loss: 0.5417057871818542] [G loss: 1.1751221418380737]\n",
      "[Epoch 2/5] [Batch 57/938] [D loss: 0.5612770318984985] [G loss: 0.9424033164978027]\n",
      "[Epoch 2/5] [Batch 58/938] [D loss: 0.5123882293701172] [G loss: 1.3801790475845337]\n",
      "[Epoch 2/5] [Batch 59/938] [D loss: 0.5121329426765442] [G loss: 0.7854261994361877]\n",
      "[Epoch 2/5] [Batch 60/938] [D loss: 0.5216784477233887] [G loss: 1.1273210048675537]\n",
      "[Epoch 2/5] [Batch 61/938] [D loss: 0.5095000267028809] [G loss: 1.2259491682052612]\n",
      "[Epoch 2/5] [Batch 62/938] [D loss: 0.5104233622550964] [G loss: 0.9463015198707581]\n",
      "[Epoch 2/5] [Batch 63/938] [D loss: 0.5401570796966553] [G loss: 1.2955166101455688]\n",
      "[Epoch 2/5] [Batch 64/938] [D loss: 0.5466504693031311] [G loss: 1.2607783079147339]\n",
      "[Epoch 2/5] [Batch 65/938] [D loss: 0.6274912357330322] [G loss: 0.6740844249725342]\n",
      "[Epoch 2/5] [Batch 66/938] [D loss: 0.6247064471244812] [G loss: 2.1207544803619385]\n",
      "[Epoch 2/5] [Batch 67/938] [D loss: 0.6864891648292542] [G loss: 0.5134546756744385]\n",
      "[Epoch 2/5] [Batch 68/938] [D loss: 0.5389865636825562] [G loss: 1.3085064888000488]\n",
      "[Epoch 2/5] [Batch 69/938] [D loss: 0.615311861038208] [G loss: 1.3433482646942139]\n",
      "[Epoch 2/5] [Batch 70/938] [D loss: 0.559785783290863] [G loss: 0.833548903465271]\n",
      "[Epoch 2/5] [Batch 71/938] [D loss: 0.5311110615730286] [G loss: 0.9894378781318665]\n",
      "[Epoch 2/5] [Batch 72/938] [D loss: 0.5891144275665283] [G loss: 1.1814908981323242]\n",
      "[Epoch 2/5] [Batch 73/938] [D loss: 0.5105355381965637] [G loss: 0.9584679007530212]\n",
      "[Epoch 2/5] [Batch 74/938] [D loss: 0.5462749600410461] [G loss: 1.0440776348114014]\n",
      "[Epoch 2/5] [Batch 75/938] [D loss: 0.4995374381542206] [G loss: 1.2613569498062134]\n",
      "[Epoch 2/5] [Batch 76/938] [D loss: 0.5491101145744324] [G loss: 1.1339584589004517]\n",
      "[Epoch 2/5] [Batch 77/938] [D loss: 0.6010215282440186] [G loss: 0.8200594186782837]\n",
      "[Epoch 2/5] [Batch 78/938] [D loss: 0.5343043208122253] [G loss: 1.2774097919464111]\n",
      "[Epoch 2/5] [Batch 79/938] [D loss: 0.5371966361999512] [G loss: 1.0933715105056763]\n",
      "[Epoch 2/5] [Batch 80/938] [D loss: 0.5344498157501221] [G loss: 1.3019368648529053]\n",
      "[Epoch 2/5] [Batch 81/938] [D loss: 0.5310884118080139] [G loss: 0.9358638525009155]\n",
      "[Epoch 2/5] [Batch 82/938] [D loss: 0.5596667528152466] [G loss: 1.375200867652893]\n",
      "[Epoch 2/5] [Batch 83/938] [D loss: 0.5774132013320923] [G loss: 1.1092088222503662]\n",
      "[Epoch 2/5] [Batch 84/938] [D loss: 0.5905985832214355] [G loss: 1.2499594688415527]\n",
      "[Epoch 2/5] [Batch 85/938] [D loss: 0.6327040195465088] [G loss: 0.7645335793495178]\n",
      "[Epoch 2/5] [Batch 86/938] [D loss: 0.5898803472518921] [G loss: 1.6127346754074097]\n",
      "[Epoch 2/5] [Batch 87/938] [D loss: 0.5127289891242981] [G loss: 1.1925634145736694]\n",
      "[Epoch 2/5] [Batch 88/938] [D loss: 0.5665026903152466] [G loss: 0.9274910688400269]\n",
      "[Epoch 2/5] [Batch 89/938] [D loss: 0.5413763523101807] [G loss: 1.203305959701538]\n",
      "[Epoch 2/5] [Batch 90/938] [D loss: 0.549681544303894] [G loss: 0.839636504650116]\n",
      "[Epoch 2/5] [Batch 91/938] [D loss: 0.5248736143112183] [G loss: 1.384706735610962]\n",
      "[Epoch 2/5] [Batch 92/938] [D loss: 0.6297018527984619] [G loss: 0.7272967100143433]\n",
      "[Epoch 2/5] [Batch 93/938] [D loss: 0.6453050374984741] [G loss: 1.8465280532836914]\n",
      "[Epoch 2/5] [Batch 94/938] [D loss: 0.5943255424499512] [G loss: 0.6501271724700928]\n",
      "[Epoch 2/5] [Batch 95/938] [D loss: 0.579221248626709] [G loss: 1.125685214996338]\n",
      "[Epoch 2/5] [Batch 96/938] [D loss: 0.5979365706443787] [G loss: 1.0992947816848755]\n",
      "[Epoch 2/5] [Batch 97/938] [D loss: 0.5567287802696228] [G loss: 0.9603868722915649]\n",
      "[Epoch 2/5] [Batch 98/938] [D loss: 0.5600619316101074] [G loss: 0.9007631540298462]\n",
      "[Epoch 2/5] [Batch 99/938] [D loss: 0.5413969159126282] [G loss: 1.1454797983169556]\n",
      "[Epoch 2/5] [Batch 100/938] [D loss: 0.49041593074798584] [G loss: 0.9853947758674622]\n",
      "[Epoch 2/5] [Batch 101/938] [D loss: 0.5612339973449707] [G loss: 1.2795624732971191]\n",
      "[Epoch 2/5] [Batch 102/938] [D loss: 0.5725469589233398] [G loss: 0.8484236001968384]\n",
      "[Epoch 2/5] [Batch 103/938] [D loss: 0.5606711506843567] [G loss: 1.2196424007415771]\n",
      "[Epoch 2/5] [Batch 104/938] [D loss: 0.5422483682632446] [G loss: 1.11776864528656]\n",
      "[Epoch 2/5] [Batch 105/938] [D loss: 0.5114713907241821] [G loss: 1.023172378540039]\n",
      "[Epoch 2/5] [Batch 106/938] [D loss: 0.5663670897483826] [G loss: 1.515681266784668]\n",
      "[Epoch 2/5] [Batch 107/938] [D loss: 0.6446899771690369] [G loss: 0.6057767868041992]\n",
      "[Epoch 2/5] [Batch 108/938] [D loss: 0.6051267385482788] [G loss: 1.6994367837905884]\n",
      "[Epoch 2/5] [Batch 109/938] [D loss: 0.5557386875152588] [G loss: 0.7717332243919373]\n",
      "[Epoch 2/5] [Batch 110/938] [D loss: 0.5747069120407104] [G loss: 1.3984456062316895]\n",
      "[Epoch 2/5] [Batch 111/938] [D loss: 0.5213440656661987] [G loss: 1.0349204540252686]\n",
      "[Epoch 2/5] [Batch 112/938] [D loss: 0.5492730140686035] [G loss: 0.9102482795715332]\n",
      "[Epoch 2/5] [Batch 113/938] [D loss: 0.506641149520874] [G loss: 1.129712700843811]\n",
      "[Epoch 2/5] [Batch 114/938] [D loss: 0.5130791664123535] [G loss: 1.350582480430603]\n",
      "[Epoch 2/5] [Batch 115/938] [D loss: 0.5713901519775391] [G loss: 0.9323148131370544]\n",
      "[Epoch 2/5] [Batch 116/938] [D loss: 0.5139143466949463] [G loss: 1.2319964170455933]\n",
      "[Epoch 2/5] [Batch 117/938] [D loss: 0.5833448767662048] [G loss: 1.370144009590149]\n",
      "[Epoch 2/5] [Batch 118/938] [D loss: 0.549597442150116] [G loss: 1.0505170822143555]\n",
      "[Epoch 2/5] [Batch 119/938] [D loss: 0.5992571115493774] [G loss: 1.6110574007034302]\n",
      "[Epoch 2/5] [Batch 120/938] [D loss: 0.6081122159957886] [G loss: 0.5534303188323975]\n",
      "[Epoch 2/5] [Batch 121/938] [D loss: 0.6407728791236877] [G loss: 1.5825530290603638]\n",
      "[Epoch 2/5] [Batch 122/938] [D loss: 0.5475485920906067] [G loss: 1.0719341039657593]\n",
      "[Epoch 2/5] [Batch 123/938] [D loss: 0.6356451511383057] [G loss: 0.9088325500488281]\n",
      "[Epoch 2/5] [Batch 124/938] [D loss: 0.6219237446784973] [G loss: 1.2155425548553467]\n",
      "[Epoch 2/5] [Batch 125/938] [D loss: 0.6267923712730408] [G loss: 0.9154674410820007]\n",
      "[Epoch 2/5] [Batch 126/938] [D loss: 0.6143569946289062] [G loss: 0.7730072140693665]\n",
      "[Epoch 2/5] [Batch 127/938] [D loss: 0.5315504670143127] [G loss: 1.0595391988754272]\n",
      "[Epoch 2/5] [Batch 128/938] [D loss: 0.5429543256759644] [G loss: 1.035208821296692]\n",
      "[Epoch 2/5] [Batch 129/938] [D loss: 0.510455310344696] [G loss: 0.9740484952926636]\n",
      "[Epoch 2/5] [Batch 130/938] [D loss: 0.5775662660598755] [G loss: 0.9591171145439148]\n",
      "[Epoch 2/5] [Batch 131/938] [D loss: 0.5352815389633179] [G loss: 1.0489357709884644]\n",
      "[Epoch 2/5] [Batch 132/938] [D loss: 0.5205374360084534] [G loss: 1.3598668575286865]\n",
      "[Epoch 2/5] [Batch 133/938] [D loss: 0.49399375915527344] [G loss: 0.9868584871292114]\n",
      "[Epoch 2/5] [Batch 134/938] [D loss: 0.5332205295562744] [G loss: 1.0748838186264038]\n",
      "[Epoch 2/5] [Batch 135/938] [D loss: 0.5504671335220337] [G loss: 1.2295584678649902]\n",
      "[Epoch 2/5] [Batch 136/938] [D loss: 0.5358266830444336] [G loss: 1.297619104385376]\n",
      "[Epoch 2/5] [Batch 137/938] [D loss: 0.5606945157051086] [G loss: 1.6418850421905518]\n",
      "[Epoch 2/5] [Batch 138/938] [D loss: 0.5799257159233093] [G loss: 0.7804552316665649]\n",
      "[Epoch 2/5] [Batch 139/938] [D loss: 0.5660665035247803] [G loss: 1.2732276916503906]\n",
      "[Epoch 2/5] [Batch 140/938] [D loss: 0.5512020587921143] [G loss: 0.9354533553123474]\n",
      "[Epoch 2/5] [Batch 141/938] [D loss: 0.5351534485816956] [G loss: 1.2755398750305176]\n",
      "[Epoch 2/5] [Batch 142/938] [D loss: 0.5276600122451782] [G loss: 0.9642421007156372]\n",
      "[Epoch 2/5] [Batch 143/938] [D loss: 0.5821565389633179] [G loss: 0.9214096069335938]\n",
      "[Epoch 2/5] [Batch 144/938] [D loss: 0.5324437022209167] [G loss: 1.3174086809158325]\n",
      "[Epoch 2/5] [Batch 145/938] [D loss: 0.5487053394317627] [G loss: 1.0411741733551025]\n",
      "[Epoch 2/5] [Batch 146/938] [D loss: 0.5374768972396851] [G loss: 1.022300124168396]\n",
      "[Epoch 2/5] [Batch 147/938] [D loss: 0.5734708905220032] [G loss: 1.3571230173110962]\n",
      "[Epoch 2/5] [Batch 148/938] [D loss: 0.5676727890968323] [G loss: 1.015472650527954]\n",
      "[Epoch 2/5] [Batch 149/938] [D loss: 0.5676482915878296] [G loss: 1.1294875144958496]\n",
      "[Epoch 2/5] [Batch 150/938] [D loss: 0.5782896280288696] [G loss: 1.1206855773925781]\n",
      "[Epoch 2/5] [Batch 151/938] [D loss: 0.5035157203674316] [G loss: 1.0425575971603394]\n",
      "[Epoch 2/5] [Batch 152/938] [D loss: 0.5925787687301636] [G loss: 1.6583830118179321]\n",
      "[Epoch 2/5] [Batch 153/938] [D loss: 0.6501078605651855] [G loss: 0.5882347822189331]\n",
      "[Epoch 2/5] [Batch 154/938] [D loss: 0.5412770509719849] [G loss: 1.8550326824188232]\n",
      "[Epoch 2/5] [Batch 155/938] [D loss: 0.5476080775260925] [G loss: 1.1866364479064941]\n",
      "[Epoch 2/5] [Batch 156/938] [D loss: 0.6057782769203186] [G loss: 0.6626595258712769]\n",
      "[Epoch 2/5] [Batch 157/938] [D loss: 0.6359421014785767] [G loss: 1.5582494735717773]\n",
      "[Epoch 2/5] [Batch 158/938] [D loss: 0.6110068559646606] [G loss: 0.9055600166320801]\n",
      "[Epoch 2/5] [Batch 159/938] [D loss: 0.5940988063812256] [G loss: 0.8637518882751465]\n",
      "[Epoch 2/5] [Batch 160/938] [D loss: 0.5797585248947144] [G loss: 1.2932441234588623]\n",
      "[Epoch 2/5] [Batch 161/938] [D loss: 0.5635569095611572] [G loss: 0.820644736289978]\n",
      "[Epoch 2/5] [Batch 162/938] [D loss: 0.616523265838623] [G loss: 1.030686616897583]\n",
      "[Epoch 2/5] [Batch 163/938] [D loss: 0.5583851337432861] [G loss: 1.2115601301193237]\n",
      "[Epoch 2/5] [Batch 164/938] [D loss: 0.6064749956130981] [G loss: 1.0233426094055176]\n",
      "[Epoch 2/5] [Batch 165/938] [D loss: 0.5980031490325928] [G loss: 0.8260824084281921]\n",
      "[Epoch 2/5] [Batch 166/938] [D loss: 0.5924862623214722] [G loss: 1.3166097402572632]\n",
      "[Epoch 2/5] [Batch 167/938] [D loss: 0.566087543964386] [G loss: 0.8380134105682373]\n",
      "[Epoch 2/5] [Batch 168/938] [D loss: 0.575393795967102] [G loss: 1.1334848403930664]\n",
      "[Epoch 2/5] [Batch 169/938] [D loss: 0.5159127712249756] [G loss: 1.1133356094360352]\n",
      "[Epoch 2/5] [Batch 170/938] [D loss: 0.5591687560081482] [G loss: 0.8669262528419495]\n",
      "[Epoch 2/5] [Batch 171/938] [D loss: 0.5586819648742676] [G loss: 1.0571365356445312]\n",
      "[Epoch 2/5] [Batch 172/938] [D loss: 0.5716012716293335] [G loss: 1.1282997131347656]\n",
      "[Epoch 2/5] [Batch 173/938] [D loss: 0.5263832807540894] [G loss: 0.9740740060806274]\n",
      "[Epoch 2/5] [Batch 174/938] [D loss: 0.5606300830841064] [G loss: 1.0484371185302734]\n",
      "[Epoch 2/5] [Batch 175/938] [D loss: 0.5538819432258606] [G loss: 1.329848289489746]\n",
      "[Epoch 2/5] [Batch 176/938] [D loss: 0.5842902660369873] [G loss: 0.9024319648742676]\n",
      "[Epoch 2/5] [Batch 177/938] [D loss: 0.63370281457901] [G loss: 1.163004994392395]\n",
      "[Epoch 2/5] [Batch 178/938] [D loss: 0.6303200721740723] [G loss: 0.8789913058280945]\n",
      "[Epoch 2/5] [Batch 179/938] [D loss: 0.6005445122718811] [G loss: 1.1777267456054688]\n",
      "[Epoch 2/5] [Batch 180/938] [D loss: 0.6007538437843323] [G loss: 0.8905954360961914]\n",
      "[Epoch 2/5] [Batch 181/938] [D loss: 0.5902883410453796] [G loss: 1.1294283866882324]\n",
      "[Epoch 2/5] [Batch 182/938] [D loss: 0.5808125734329224] [G loss: 1.0260679721832275]\n",
      "[Epoch 2/5] [Batch 183/938] [D loss: 0.534868061542511] [G loss: 0.857833206653595]\n",
      "[Epoch 2/5] [Batch 184/938] [D loss: 0.5871778726577759] [G loss: 0.98517245054245]\n",
      "[Epoch 2/5] [Batch 185/938] [D loss: 0.6369359493255615] [G loss: 1.2410780191421509]\n",
      "[Epoch 2/5] [Batch 186/938] [D loss: 0.6585786938667297] [G loss: 0.5234485864639282]\n",
      "[Epoch 2/5] [Batch 187/938] [D loss: 0.6254477500915527] [G loss: 1.687966227531433]\n",
      "[Epoch 2/5] [Batch 188/938] [D loss: 0.6011590957641602] [G loss: 0.9248241186141968]\n",
      "[Epoch 2/5] [Batch 189/938] [D loss: 0.5606146454811096] [G loss: 0.8775675892829895]\n",
      "[Epoch 2/5] [Batch 190/938] [D loss: 0.5579450130462646] [G loss: 1.0903809070587158]\n",
      "[Epoch 2/5] [Batch 191/938] [D loss: 0.5124746561050415] [G loss: 0.9201182723045349]\n",
      "[Epoch 2/5] [Batch 192/938] [D loss: 0.5323001146316528] [G loss: 1.2750067710876465]\n",
      "[Epoch 2/5] [Batch 193/938] [D loss: 0.5457242131233215] [G loss: 1.3244060277938843]\n",
      "[Epoch 2/5] [Batch 194/938] [D loss: 0.6196181774139404] [G loss: 0.7772006392478943]\n",
      "[Epoch 2/5] [Batch 195/938] [D loss: 0.4450886845588684] [G loss: 1.098031759262085]\n",
      "[Epoch 2/5] [Batch 196/938] [D loss: 0.5417100787162781] [G loss: 1.1613755226135254]\n",
      "[Epoch 2/5] [Batch 197/938] [D loss: 0.6173161864280701] [G loss: 1.0620160102844238]\n",
      "[Epoch 2/5] [Batch 198/938] [D loss: 0.5721215605735779] [G loss: 1.111013412475586]\n",
      "[Epoch 2/5] [Batch 199/938] [D loss: 0.5571865439414978] [G loss: 0.8949610590934753]\n",
      "[Epoch 2/5] [Batch 200/938] [D loss: 0.5360122323036194] [G loss: 1.3836259841918945]\n",
      "[Epoch 2/5] [Batch 201/938] [D loss: 0.5865812301635742] [G loss: 1.1805981397628784]\n",
      "[Epoch 2/5] [Batch 202/938] [D loss: 0.5240762829780579] [G loss: 1.0309480428695679]\n",
      "[Epoch 2/5] [Batch 203/938] [D loss: 0.561383068561554] [G loss: 1.1113536357879639]\n",
      "[Epoch 2/5] [Batch 204/938] [D loss: 0.5551470518112183] [G loss: 0.977057933807373]\n",
      "[Epoch 2/5] [Batch 205/938] [D loss: 0.561848521232605] [G loss: 1.1165153980255127]\n",
      "[Epoch 2/5] [Batch 206/938] [D loss: 0.510202169418335] [G loss: 1.5098919868469238]\n",
      "[Epoch 2/5] [Batch 207/938] [D loss: 0.6105028390884399] [G loss: 1.0572845935821533]\n",
      "[Epoch 2/5] [Batch 208/938] [D loss: 0.5184342861175537] [G loss: 1.4056060314178467]\n",
      "[Epoch 2/5] [Batch 209/938] [D loss: 0.6403963565826416] [G loss: 1.0248379707336426]\n",
      "[Epoch 2/5] [Batch 210/938] [D loss: 0.5071000456809998] [G loss: 1.1812341213226318]\n",
      "[Epoch 2/5] [Batch 211/938] [D loss: 0.5356260538101196] [G loss: 1.2852764129638672]\n",
      "[Epoch 2/5] [Batch 212/938] [D loss: 0.5947984457015991] [G loss: 0.8828696012496948]\n",
      "[Epoch 2/5] [Batch 213/938] [D loss: 0.5820026397705078] [G loss: 1.6331595182418823]\n",
      "[Epoch 2/5] [Batch 214/938] [D loss: 0.6538453102111816] [G loss: 0.5745330452919006]\n",
      "[Epoch 2/5] [Batch 215/938] [D loss: 0.5751107931137085] [G loss: 1.585204005241394]\n",
      "[Epoch 2/5] [Batch 216/938] [D loss: 0.5997393131256104] [G loss: 1.1626313924789429]\n",
      "[Epoch 2/5] [Batch 217/938] [D loss: 0.5585818290710449] [G loss: 0.8145796060562134]\n",
      "[Epoch 2/5] [Batch 218/938] [D loss: 0.601434588432312] [G loss: 1.1569068431854248]\n",
      "[Epoch 2/5] [Batch 219/938] [D loss: 0.5643255710601807] [G loss: 1.143446445465088]\n",
      "[Epoch 2/5] [Batch 220/938] [D loss: 0.535315990447998] [G loss: 0.9456654787063599]\n",
      "[Epoch 2/5] [Batch 221/938] [D loss: 0.604661762714386] [G loss: 1.4537166357040405]\n",
      "[Epoch 2/5] [Batch 222/938] [D loss: 0.5398483276367188] [G loss: 0.9428623914718628]\n",
      "[Epoch 2/5] [Batch 223/938] [D loss: 0.6061766147613525] [G loss: 0.8900619149208069]\n",
      "[Epoch 2/5] [Batch 224/938] [D loss: 0.5950379371643066] [G loss: 1.667148232460022]\n",
      "[Epoch 2/5] [Batch 225/938] [D loss: 0.535202145576477] [G loss: 0.84624183177948]\n",
      "[Epoch 2/5] [Batch 226/938] [D loss: 0.5297022461891174] [G loss: 0.9011804461479187]\n",
      "[Epoch 2/5] [Batch 227/938] [D loss: 0.547284722328186] [G loss: 1.0957680940628052]\n",
      "[Epoch 2/5] [Batch 228/938] [D loss: 0.5727165937423706] [G loss: 1.133278250694275]\n",
      "[Epoch 2/5] [Batch 229/938] [D loss: 0.6531607508659363] [G loss: 0.7602141499519348]\n",
      "[Epoch 2/5] [Batch 230/938] [D loss: 0.6003522872924805] [G loss: 1.4221713542938232]\n",
      "[Epoch 2/5] [Batch 231/938] [D loss: 0.5900338888168335] [G loss: 0.7647116184234619]\n",
      "[Epoch 2/5] [Batch 232/938] [D loss: 0.5823780298233032] [G loss: 0.9725154638290405]\n",
      "[Epoch 2/5] [Batch 233/938] [D loss: 0.5798294544219971] [G loss: 1.1459399461746216]\n",
      "[Epoch 2/5] [Batch 234/938] [D loss: 0.6016101837158203] [G loss: 0.9207031726837158]\n",
      "[Epoch 2/5] [Batch 235/938] [D loss: 0.5326347947120667] [G loss: 0.9453753232955933]\n",
      "[Epoch 2/5] [Batch 236/938] [D loss: 0.5532922744750977] [G loss: 0.9724587202072144]\n",
      "[Epoch 2/5] [Batch 237/938] [D loss: 0.6514542698860168] [G loss: 1.1861480474472046]\n",
      "[Epoch 2/5] [Batch 238/938] [D loss: 0.6874707937240601] [G loss: 0.6945104598999023]\n",
      "[Epoch 2/5] [Batch 239/938] [D loss: 0.5704864263534546] [G loss: 1.2085292339324951]\n",
      "[Epoch 2/5] [Batch 240/938] [D loss: 0.57929527759552] [G loss: 1.1934502124786377]\n",
      "[Epoch 2/5] [Batch 241/938] [D loss: 0.5586539506912231] [G loss: 0.9064308404922485]\n",
      "[Epoch 2/5] [Batch 242/938] [D loss: 0.5565650463104248] [G loss: 1.013024091720581]\n",
      "[Epoch 2/5] [Batch 243/938] [D loss: 0.5716032981872559] [G loss: 1.3058654069900513]\n",
      "[Epoch 2/5] [Batch 244/938] [D loss: 0.527300238609314] [G loss: 0.9890634417533875]\n",
      "[Epoch 2/5] [Batch 245/938] [D loss: 0.5692250728607178] [G loss: 1.0304234027862549]\n",
      "[Epoch 2/5] [Batch 246/938] [D loss: 0.5627877712249756] [G loss: 1.158156394958496]\n",
      "[Epoch 2/5] [Batch 247/938] [D loss: 0.5824898481369019] [G loss: 0.9176137447357178]\n",
      "[Epoch 2/5] [Batch 248/938] [D loss: 0.5852123498916626] [G loss: 0.952572226524353]\n",
      "[Epoch 2/5] [Batch 249/938] [D loss: 0.5871667265892029] [G loss: 1.257446527481079]\n",
      "[Epoch 2/5] [Batch 250/938] [D loss: 0.615300178527832] [G loss: 0.6026322841644287]\n",
      "[Epoch 2/5] [Batch 251/938] [D loss: 0.6466052532196045] [G loss: 1.4647774696350098]\n",
      "[Epoch 2/5] [Batch 252/938] [D loss: 0.576361358165741] [G loss: 0.7894437313079834]\n",
      "[Epoch 2/5] [Batch 253/938] [D loss: 0.5374590754508972] [G loss: 1.2771191596984863]\n",
      "[Epoch 2/5] [Batch 254/938] [D loss: 0.5168516635894775] [G loss: 1.0307589769363403]\n",
      "[Epoch 2/5] [Batch 255/938] [D loss: 0.5775210857391357] [G loss: 0.8699176907539368]\n",
      "[Epoch 2/5] [Batch 256/938] [D loss: 0.5715997815132141] [G loss: 1.214678168296814]\n",
      "[Epoch 2/5] [Batch 257/938] [D loss: 0.7558112144470215] [G loss: 0.8891315460205078]\n",
      "[Epoch 2/5] [Batch 258/938] [D loss: 0.5387023091316223] [G loss: 1.201724648475647]\n",
      "[Epoch 2/5] [Batch 259/938] [D loss: 0.540739119052887] [G loss: 0.8330839276313782]\n",
      "[Epoch 2/5] [Batch 260/938] [D loss: 0.5939575433731079] [G loss: 1.2093029022216797]\n",
      "[Epoch 2/5] [Batch 261/938] [D loss: 0.6297211647033691] [G loss: 0.7385843396186829]\n",
      "[Epoch 2/5] [Batch 262/938] [D loss: 0.6034536361694336] [G loss: 1.1371179819107056]\n",
      "[Epoch 2/5] [Batch 263/938] [D loss: 0.6038768887519836] [G loss: 0.8873885273933411]\n",
      "[Epoch 2/5] [Batch 264/938] [D loss: 0.552259087562561] [G loss: 1.0805007219314575]\n",
      "[Epoch 2/5] [Batch 265/938] [D loss: 0.5112582445144653] [G loss: 1.1069481372833252]\n",
      "[Epoch 2/5] [Batch 266/938] [D loss: 0.5045655965805054] [G loss: 0.8894290924072266]\n",
      "[Epoch 2/5] [Batch 267/938] [D loss: 0.5897382497787476] [G loss: 1.3692073822021484]\n",
      "[Epoch 2/5] [Batch 268/938] [D loss: 0.6106283664703369] [G loss: 0.8463290929794312]\n",
      "[Epoch 2/5] [Batch 269/938] [D loss: 0.5819225907325745] [G loss: 0.9002174139022827]\n",
      "[Epoch 2/5] [Batch 270/938] [D loss: 0.5391473770141602] [G loss: 1.3444169759750366]\n",
      "[Epoch 2/5] [Batch 271/938] [D loss: 0.6125430464744568] [G loss: 0.8182483315467834]\n",
      "[Epoch 2/5] [Batch 272/938] [D loss: 0.6361241340637207] [G loss: 1.618720531463623]\n",
      "[Epoch 2/5] [Batch 273/938] [D loss: 0.6375113129615784] [G loss: 0.5837299227714539]\n",
      "[Epoch 2/5] [Batch 274/938] [D loss: 0.5544933676719666] [G loss: 1.2750715017318726]\n",
      "[Epoch 2/5] [Batch 275/938] [D loss: 0.5867387652397156] [G loss: 1.1127609014511108]\n",
      "[Epoch 2/5] [Batch 276/938] [D loss: 0.5774590969085693] [G loss: 0.8243420124053955]\n",
      "[Epoch 2/5] [Batch 277/938] [D loss: 0.5300928354263306] [G loss: 1.0658525228500366]\n",
      "[Epoch 2/5] [Batch 278/938] [D loss: 0.5540152788162231] [G loss: 0.9686774611473083]\n",
      "[Epoch 2/5] [Batch 279/938] [D loss: 0.6169754266738892] [G loss: 1.1190263032913208]\n",
      "[Epoch 2/5] [Batch 280/938] [D loss: 0.544766902923584] [G loss: 0.9666567444801331]\n",
      "[Epoch 2/5] [Batch 281/938] [D loss: 0.573789119720459] [G loss: 1.0407688617706299]\n",
      "[Epoch 2/5] [Batch 282/938] [D loss: 0.6337510943412781] [G loss: 1.135071039199829]\n",
      "[Epoch 2/5] [Batch 283/938] [D loss: 0.6450545191764832] [G loss: 0.877640962600708]\n",
      "[Epoch 2/5] [Batch 284/938] [D loss: 0.6591299772262573] [G loss: 1.1991249322891235]\n",
      "[Epoch 2/5] [Batch 285/938] [D loss: 0.5394915342330933] [G loss: 0.7998226881027222]\n",
      "[Epoch 2/5] [Batch 286/938] [D loss: 0.6128212213516235] [G loss: 0.9633923768997192]\n",
      "[Epoch 2/5] [Batch 287/938] [D loss: 0.5864382982254028] [G loss: 1.1464464664459229]\n",
      "[Epoch 2/5] [Batch 288/938] [D loss: 0.5778723955154419] [G loss: 0.8059280514717102]\n",
      "[Epoch 2/5] [Batch 289/938] [D loss: 0.5658709406852722] [G loss: 1.0439907312393188]\n",
      "[Epoch 2/5] [Batch 290/938] [D loss: 0.5874103903770447] [G loss: 1.23525071144104]\n",
      "[Epoch 2/5] [Batch 291/938] [D loss: 0.5612930059432983] [G loss: 0.9700217247009277]\n",
      "[Epoch 2/5] [Batch 292/938] [D loss: 0.46997979283332825] [G loss: 1.1251368522644043]\n",
      "[Epoch 2/5] [Batch 293/938] [D loss: 0.5720072984695435] [G loss: 1.1370586156845093]\n",
      "[Epoch 2/5] [Batch 294/938] [D loss: 0.6616255044937134] [G loss: 0.672718346118927]\n",
      "[Epoch 2/5] [Batch 295/938] [D loss: 0.7035569548606873] [G loss: 1.6319836378097534]\n",
      "[Epoch 2/5] [Batch 296/938] [D loss: 0.6460422873497009] [G loss: 0.6615137457847595]\n",
      "[Epoch 2/5] [Batch 297/938] [D loss: 0.5434074401855469] [G loss: 1.0137869119644165]\n",
      "[Epoch 2/5] [Batch 298/938] [D loss: 0.6022151708602905] [G loss: 1.2468256950378418]\n",
      "[Epoch 2/5] [Batch 299/938] [D loss: 0.582429051399231] [G loss: 0.8723724484443665]\n",
      "[Epoch 2/5] [Batch 300/938] [D loss: 0.6143893003463745] [G loss: 1.0442776679992676]\n",
      "[Epoch 2/5] [Batch 301/938] [D loss: 0.5386011600494385] [G loss: 1.069754719734192]\n",
      "[Epoch 2/5] [Batch 302/938] [D loss: 0.5385058522224426] [G loss: 0.9428421258926392]\n",
      "[Epoch 2/5] [Batch 303/938] [D loss: 0.5439199209213257] [G loss: 0.9419110417366028]\n",
      "[Epoch 2/5] [Batch 304/938] [D loss: 0.6221652626991272] [G loss: 0.9688976407051086]\n",
      "[Epoch 2/5] [Batch 305/938] [D loss: 0.6356937885284424] [G loss: 1.1636241674423218]\n",
      "[Epoch 2/5] [Batch 306/938] [D loss: 0.5489786863327026] [G loss: 0.8660346269607544]\n",
      "[Epoch 2/5] [Batch 307/938] [D loss: 0.5310962796211243] [G loss: 1.3635449409484863]\n",
      "[Epoch 2/5] [Batch 308/938] [D loss: 0.5681968927383423] [G loss: 0.8547348976135254]\n",
      "[Epoch 2/5] [Batch 309/938] [D loss: 0.5936071872711182] [G loss: 1.1096622943878174]\n",
      "[Epoch 2/5] [Batch 310/938] [D loss: 0.5950038433074951] [G loss: 0.9467539191246033]\n",
      "[Epoch 2/5] [Batch 311/938] [D loss: 0.6034790277481079] [G loss: 1.0833299160003662]\n",
      "[Epoch 2/5] [Batch 312/938] [D loss: 0.5718631148338318] [G loss: 0.9096249938011169]\n",
      "[Epoch 2/5] [Batch 313/938] [D loss: 0.5607472658157349] [G loss: 0.9063220620155334]\n",
      "[Epoch 2/5] [Batch 314/938] [D loss: 0.5771676301956177] [G loss: 1.1527771949768066]\n",
      "[Epoch 2/5] [Batch 315/938] [D loss: 0.5886728763580322] [G loss: 1.2542901039123535]\n",
      "[Epoch 2/5] [Batch 316/938] [D loss: 0.6329489946365356] [G loss: 0.9829259514808655]\n",
      "[Epoch 2/5] [Batch 317/938] [D loss: 0.6105929613113403] [G loss: 0.8472890853881836]\n",
      "[Epoch 2/5] [Batch 318/938] [D loss: 0.6433698534965515] [G loss: 1.077126383781433]\n",
      "[Epoch 2/5] [Batch 319/938] [D loss: 0.5612549781799316] [G loss: 0.9197514057159424]\n",
      "[Epoch 2/5] [Batch 320/938] [D loss: 0.5813921689987183] [G loss: 1.0622506141662598]\n",
      "[Epoch 2/5] [Batch 321/938] [D loss: 0.5626893639564514] [G loss: 1.0433173179626465]\n",
      "[Epoch 2/5] [Batch 322/938] [D loss: 0.5415207147598267] [G loss: 1.0214729309082031]\n",
      "[Epoch 2/5] [Batch 323/938] [D loss: 0.60771644115448] [G loss: 0.9040658473968506]\n",
      "[Epoch 2/5] [Batch 324/938] [D loss: 0.6035885810852051] [G loss: 0.8434000611305237]\n",
      "[Epoch 2/5] [Batch 325/938] [D loss: 0.5719037055969238] [G loss: 1.0328965187072754]\n",
      "[Epoch 2/5] [Batch 326/938] [D loss: 0.5552623271942139] [G loss: 1.0091415643692017]\n",
      "[Epoch 2/5] [Batch 327/938] [D loss: 0.5553995966911316] [G loss: 0.9208414554595947]\n",
      "[Epoch 2/5] [Batch 328/938] [D loss: 0.5638579726219177] [G loss: 0.9818527102470398]\n",
      "[Epoch 2/5] [Batch 329/938] [D loss: 0.5703030824661255] [G loss: 1.2782148122787476]\n",
      "[Epoch 2/5] [Batch 330/938] [D loss: 0.5749996900558472] [G loss: 0.8864505290985107]\n",
      "[Epoch 2/5] [Batch 331/938] [D loss: 0.6057701110839844] [G loss: 1.1915099620819092]\n",
      "[Epoch 2/5] [Batch 332/938] [D loss: 0.5767216682434082] [G loss: 0.8170353174209595]\n",
      "[Epoch 2/5] [Batch 333/938] [D loss: 0.6007487773895264] [G loss: 1.1777453422546387]\n",
      "[Epoch 2/5] [Batch 334/938] [D loss: 0.5674765110015869] [G loss: 1.2001323699951172]\n",
      "[Epoch 2/5] [Batch 335/938] [D loss: 0.5676016211509705] [G loss: 0.7306976914405823]\n",
      "[Epoch 2/5] [Batch 336/938] [D loss: 0.6414588689804077] [G loss: 1.204434871673584]\n",
      "[Epoch 2/5] [Batch 337/938] [D loss: 0.6148501634597778] [G loss: 0.997570276260376]\n",
      "[Epoch 2/5] [Batch 338/938] [D loss: 0.6160362958908081] [G loss: 0.8843128085136414]\n",
      "[Epoch 2/5] [Batch 339/938] [D loss: 0.505658745765686] [G loss: 1.158372163772583]\n",
      "[Epoch 2/5] [Batch 340/938] [D loss: 0.6258959770202637] [G loss: 1.0220543146133423]\n",
      "[Epoch 2/5] [Batch 341/938] [D loss: 0.6093012094497681] [G loss: 0.8739324808120728]\n",
      "[Epoch 2/5] [Batch 342/938] [D loss: 0.5605305433273315] [G loss: 1.101949691772461]\n",
      "[Epoch 2/5] [Batch 343/938] [D loss: 0.5957983732223511] [G loss: 1.0920131206512451]\n",
      "[Epoch 2/5] [Batch 344/938] [D loss: 0.6220433712005615] [G loss: 0.8995633125305176]\n",
      "[Epoch 2/5] [Batch 345/938] [D loss: 0.5200666189193726] [G loss: 1.2722036838531494]\n",
      "[Epoch 2/5] [Batch 346/938] [D loss: 0.5933279991149902] [G loss: 0.9505152106285095]\n",
      "[Epoch 2/5] [Batch 347/938] [D loss: 0.5833355188369751] [G loss: 0.7943015694618225]\n",
      "[Epoch 2/5] [Batch 348/938] [D loss: 0.6424508094787598] [G loss: 1.4365928173065186]\n",
      "[Epoch 2/5] [Batch 349/938] [D loss: 0.628501296043396] [G loss: 0.6206730008125305]\n",
      "[Epoch 2/5] [Batch 350/938] [D loss: 0.5800886154174805] [G loss: 1.3239327669143677]\n",
      "[Epoch 2/5] [Batch 351/938] [D loss: 0.5210733413696289] [G loss: 1.0069326162338257]\n",
      "[Epoch 2/5] [Batch 352/938] [D loss: 0.5605398416519165] [G loss: 0.9463147521018982]\n",
      "[Epoch 2/5] [Batch 353/938] [D loss: 0.5375592708587646] [G loss: 1.3709608316421509]\n",
      "[Epoch 2/5] [Batch 354/938] [D loss: 0.5758901238441467] [G loss: 1.0307625532150269]\n",
      "[Epoch 2/5] [Batch 355/938] [D loss: 0.6224336624145508] [G loss: 0.8877622485160828]\n",
      "[Epoch 2/5] [Batch 356/938] [D loss: 0.5771712064743042] [G loss: 0.9587391018867493]\n",
      "[Epoch 2/5] [Batch 357/938] [D loss: 0.6152800917625427] [G loss: 1.0091357231140137]\n",
      "[Epoch 2/5] [Batch 358/938] [D loss: 0.5775991082191467] [G loss: 0.9510329961776733]\n",
      "[Epoch 2/5] [Batch 359/938] [D loss: 0.5896738171577454] [G loss: 1.2588067054748535]\n",
      "[Epoch 2/5] [Batch 360/938] [D loss: 0.5473355054855347] [G loss: 0.9834007024765015]\n",
      "[Epoch 2/5] [Batch 361/938] [D loss: 0.5253129005432129] [G loss: 0.9864785671234131]\n",
      "[Epoch 2/5] [Batch 362/938] [D loss: 0.6342518329620361] [G loss: 1.0195287466049194]\n",
      "[Epoch 2/5] [Batch 363/938] [D loss: 0.5780714154243469] [G loss: 0.9795050024986267]\n",
      "[Epoch 2/5] [Batch 364/938] [D loss: 0.6324076652526855] [G loss: 1.0700998306274414]\n",
      "[Epoch 2/5] [Batch 365/938] [D loss: 0.5786502361297607] [G loss: 0.9409461617469788]\n",
      "[Epoch 2/5] [Batch 366/938] [D loss: 0.655046820640564] [G loss: 1.1740226745605469]\n",
      "[Epoch 2/5] [Batch 367/938] [D loss: 0.5634745359420776] [G loss: 0.8801541328430176]\n",
      "[Epoch 2/5] [Batch 368/938] [D loss: 0.5925387144088745] [G loss: 1.0925238132476807]\n",
      "[Epoch 2/5] [Batch 369/938] [D loss: 0.6033241152763367] [G loss: 1.2541128396987915]\n",
      "[Epoch 2/5] [Batch 370/938] [D loss: 0.5537512898445129] [G loss: 0.9654262661933899]\n",
      "[Epoch 2/5] [Batch 371/938] [D loss: 0.5307623744010925] [G loss: 1.0151103734970093]\n",
      "[Epoch 2/5] [Batch 372/938] [D loss: 0.582298755645752] [G loss: 1.1142995357513428]\n",
      "[Epoch 2/5] [Batch 373/938] [D loss: 0.6025657653808594] [G loss: 0.9885963797569275]\n",
      "[Epoch 2/5] [Batch 374/938] [D loss: 0.5689378976821899] [G loss: 0.9597002863883972]\n",
      "[Epoch 2/5] [Batch 375/938] [D loss: 0.633452296257019] [G loss: 1.0769033432006836]\n",
      "[Epoch 2/5] [Batch 376/938] [D loss: 0.5855563879013062] [G loss: 1.1231440305709839]\n",
      "[Epoch 2/5] [Batch 377/938] [D loss: 0.6120727062225342] [G loss: 0.7045176029205322]\n",
      "[Epoch 2/5] [Batch 378/938] [D loss: 0.6489927768707275] [G loss: 1.1090227365493774]\n",
      "[Epoch 2/5] [Batch 379/938] [D loss: 0.6811994314193726] [G loss: 1.267724633216858]\n",
      "[Epoch 2/5] [Batch 380/938] [D loss: 0.6267014741897583] [G loss: 0.7807943224906921]\n",
      "[Epoch 2/5] [Batch 381/938] [D loss: 0.6041951179504395] [G loss: 0.9918507933616638]\n",
      "[Epoch 2/5] [Batch 382/938] [D loss: 0.5441161394119263] [G loss: 1.069790244102478]\n",
      "[Epoch 2/5] [Batch 383/938] [D loss: 0.5812435746192932] [G loss: 0.9115426540374756]\n",
      "[Epoch 2/5] [Batch 384/938] [D loss: 0.5975350141525269] [G loss: 0.9340873956680298]\n",
      "[Epoch 2/5] [Batch 385/938] [D loss: 0.5679706335067749] [G loss: 0.8825369477272034]\n",
      "[Epoch 2/5] [Batch 386/938] [D loss: 0.590236246585846] [G loss: 1.1401652097702026]\n",
      "[Epoch 2/5] [Batch 387/938] [D loss: 0.5709463357925415] [G loss: 1.066780686378479]\n",
      "[Epoch 2/5] [Batch 388/938] [D loss: 0.5649667978286743] [G loss: 0.8514483571052551]\n",
      "[Epoch 2/5] [Batch 389/938] [D loss: 0.5837043523788452] [G loss: 0.950664758682251]\n",
      "[Epoch 2/5] [Batch 390/938] [D loss: 0.571318507194519] [G loss: 1.113715410232544]\n",
      "[Epoch 2/5] [Batch 391/938] [D loss: 0.5814039707183838] [G loss: 0.7679194211959839]\n",
      "[Epoch 2/5] [Batch 392/938] [D loss: 0.5628936290740967] [G loss: 1.2856321334838867]\n",
      "[Epoch 2/5] [Batch 393/938] [D loss: 0.5811129212379456] [G loss: 1.0597039461135864]\n",
      "[Epoch 2/5] [Batch 394/938] [D loss: 0.5925676226615906] [G loss: 0.647680938243866]\n",
      "[Epoch 2/5] [Batch 395/938] [D loss: 0.5822373628616333] [G loss: 1.1477982997894287]\n",
      "[Epoch 2/5] [Batch 396/938] [D loss: 0.5765577554702759] [G loss: 1.273553729057312]\n",
      "[Epoch 2/5] [Batch 397/938] [D loss: 0.6670929789543152] [G loss: 0.6664409041404724]\n",
      "[Epoch 2/5] [Batch 398/938] [D loss: 0.6139617562294006] [G loss: 1.402130365371704]\n",
      "[Epoch 2/5] [Batch 399/938] [D loss: 0.5580750703811646] [G loss: 1.0317645072937012]\n",
      "[Epoch 2/5] [Batch 400/938] [D loss: 0.596723198890686] [G loss: 0.8538885116577148]\n",
      "[Epoch 2/5] [Batch 401/938] [D loss: 0.5470591187477112] [G loss: 1.0323089361190796]\n",
      "[Epoch 2/5] [Batch 402/938] [D loss: 0.6043378710746765] [G loss: 1.2468427419662476]\n",
      "[Epoch 2/5] [Batch 403/938] [D loss: 0.571033775806427] [G loss: 0.7701689004898071]\n",
      "[Epoch 2/5] [Batch 404/938] [D loss: 0.5561204552650452] [G loss: 0.9933657050132751]\n",
      "[Epoch 2/5] [Batch 405/938] [D loss: 0.5767192840576172] [G loss: 1.098056435585022]\n",
      "[Epoch 2/5] [Batch 406/938] [D loss: 0.5322784185409546] [G loss: 1.2121264934539795]\n",
      "[Epoch 2/5] [Batch 407/938] [D loss: 0.5779401659965515] [G loss: 0.8362984657287598]\n",
      "[Epoch 2/5] [Batch 408/938] [D loss: 0.579162061214447] [G loss: 1.2939097881317139]\n",
      "[Epoch 2/5] [Batch 409/938] [D loss: 0.5442008972167969] [G loss: 0.8468274474143982]\n",
      "[Epoch 2/5] [Batch 410/938] [D loss: 0.5581015348434448] [G loss: 1.0580880641937256]\n",
      "[Epoch 2/5] [Batch 411/938] [D loss: 0.5793601870536804] [G loss: 1.2050920724868774]\n",
      "[Epoch 2/5] [Batch 412/938] [D loss: 0.5664777755737305] [G loss: 0.9891607761383057]\n",
      "[Epoch 2/5] [Batch 413/938] [D loss: 0.5986303091049194] [G loss: 1.053178071975708]\n",
      "[Epoch 2/5] [Batch 414/938] [D loss: 0.5800275802612305] [G loss: 1.213800311088562]\n",
      "[Epoch 2/5] [Batch 415/938] [D loss: 0.5663617849349976] [G loss: 0.8587070107460022]\n",
      "[Epoch 2/5] [Batch 416/938] [D loss: 0.597344160079956] [G loss: 0.9283574223518372]\n",
      "[Epoch 2/5] [Batch 417/938] [D loss: 0.5785382390022278] [G loss: 0.9511691331863403]\n",
      "[Epoch 2/5] [Batch 418/938] [D loss: 0.6109417676925659] [G loss: 0.9390498399734497]\n",
      "[Epoch 2/5] [Batch 419/938] [D loss: 0.6647616624832153] [G loss: 1.3610622882843018]\n",
      "[Epoch 2/5] [Batch 420/938] [D loss: 0.6182254552841187] [G loss: 0.7904515266418457]\n",
      "[Epoch 2/5] [Batch 421/938] [D loss: 0.6401368379592896] [G loss: 1.0035791397094727]\n",
      "[Epoch 2/5] [Batch 422/938] [D loss: 0.593597948551178] [G loss: 0.9659920930862427]\n",
      "[Epoch 2/5] [Batch 423/938] [D loss: 0.5413462519645691] [G loss: 1.003371000289917]\n",
      "[Epoch 2/5] [Batch 424/938] [D loss: 0.5629525780677795] [G loss: 1.0891740322113037]\n",
      "[Epoch 2/5] [Batch 425/938] [D loss: 0.6446660161018372] [G loss: 0.959799587726593]\n",
      "[Epoch 2/5] [Batch 426/938] [D loss: 0.5952326655387878] [G loss: 0.8088236451148987]\n",
      "[Epoch 2/5] [Batch 427/938] [D loss: 0.6218687891960144] [G loss: 1.0982625484466553]\n",
      "[Epoch 2/5] [Batch 428/938] [D loss: 0.571114182472229] [G loss: 1.1840791702270508]\n",
      "[Epoch 2/5] [Batch 429/938] [D loss: 0.6375666856765747] [G loss: 0.8482808470726013]\n",
      "[Epoch 2/5] [Batch 430/938] [D loss: 0.5848091840744019] [G loss: 1.039963722229004]\n",
      "[Epoch 2/5] [Batch 431/938] [D loss: 0.5581139922142029] [G loss: 0.9449460506439209]\n",
      "[Epoch 2/5] [Batch 432/938] [D loss: 0.4933382272720337] [G loss: 0.9603415727615356]\n",
      "[Epoch 2/5] [Batch 433/938] [D loss: 0.5748595595359802] [G loss: 0.9955590963363647]\n",
      "[Epoch 2/5] [Batch 434/938] [D loss: 0.5875519514083862] [G loss: 1.1376829147338867]\n",
      "[Epoch 2/5] [Batch 435/938] [D loss: 0.5862987041473389] [G loss: 1.0413118600845337]\n",
      "[Epoch 2/5] [Batch 436/938] [D loss: 0.557666540145874] [G loss: 1.1707773208618164]\n",
      "[Epoch 2/5] [Batch 437/938] [D loss: 0.539264440536499] [G loss: 0.9996095895767212]\n",
      "[Epoch 2/5] [Batch 438/938] [D loss: 0.5504770278930664] [G loss: 0.8996720314025879]\n",
      "[Epoch 2/5] [Batch 439/938] [D loss: 0.6361745595932007] [G loss: 1.2669395208358765]\n",
      "[Epoch 2/5] [Batch 440/938] [D loss: 0.6407991051673889] [G loss: 0.91650390625]\n",
      "[Epoch 2/5] [Batch 441/938] [D loss: 0.5784735679626465] [G loss: 1.1684818267822266]\n",
      "[Epoch 2/5] [Batch 442/938] [D loss: 0.596078097820282] [G loss: 1.161776065826416]\n",
      "[Epoch 2/5] [Batch 443/938] [D loss: 0.6206653118133545] [G loss: 0.8455737233161926]\n",
      "[Epoch 2/5] [Batch 444/938] [D loss: 0.6480002403259277] [G loss: 1.1027984619140625]\n",
      "[Epoch 2/5] [Batch 445/938] [D loss: 0.5664749145507812] [G loss: 1.0618757009506226]\n",
      "[Epoch 2/5] [Batch 446/938] [D loss: 0.5942188501358032] [G loss: 0.8194934129714966]\n",
      "[Epoch 2/5] [Batch 447/938] [D loss: 0.5752553939819336] [G loss: 1.200675129890442]\n",
      "[Epoch 2/5] [Batch 448/938] [D loss: 0.6394001841545105] [G loss: 0.9613630175590515]\n",
      "[Epoch 2/5] [Batch 449/938] [D loss: 0.6007075309753418] [G loss: 0.8849795460700989]\n",
      "[Epoch 2/5] [Batch 450/938] [D loss: 0.6186665892601013] [G loss: 1.2018996477127075]\n",
      "[Epoch 2/5] [Batch 451/938] [D loss: 0.6062791347503662] [G loss: 0.8274298310279846]\n",
      "[Epoch 2/5] [Batch 452/938] [D loss: 0.6545934081077576] [G loss: 1.289684772491455]\n",
      "[Epoch 2/5] [Batch 453/938] [D loss: 0.5985821485519409] [G loss: 0.6872400641441345]\n",
      "[Epoch 2/5] [Batch 454/938] [D loss: 0.615541398525238] [G loss: 1.2606496810913086]\n",
      "[Epoch 2/5] [Batch 455/938] [D loss: 0.5848945379257202] [G loss: 1.0699982643127441]\n",
      "[Epoch 2/5] [Batch 456/938] [D loss: 0.5757054090499878] [G loss: 0.8907105922698975]\n",
      "[Epoch 2/5] [Batch 457/938] [D loss: 0.5531727075576782] [G loss: 1.0146362781524658]\n",
      "[Epoch 2/5] [Batch 458/938] [D loss: 0.5697190761566162] [G loss: 1.043605089187622]\n",
      "[Epoch 2/5] [Batch 459/938] [D loss: 0.577654242515564] [G loss: 0.7477709650993347]\n",
      "[Epoch 2/5] [Batch 460/938] [D loss: 0.6035634279251099] [G loss: 1.183562159538269]\n",
      "[Epoch 2/5] [Batch 461/938] [D loss: 0.5697831511497498] [G loss: 0.971922755241394]\n",
      "[Epoch 2/5] [Batch 462/938] [D loss: 0.5938949584960938] [G loss: 1.0031388998031616]\n",
      "[Epoch 2/5] [Batch 463/938] [D loss: 0.5709320902824402] [G loss: 1.0337905883789062]\n",
      "[Epoch 2/5] [Batch 464/938] [D loss: 0.5516922473907471] [G loss: 0.9055362939834595]\n",
      "[Epoch 2/5] [Batch 465/938] [D loss: 0.5553001165390015] [G loss: 1.3751786947250366]\n",
      "[Epoch 2/5] [Batch 466/938] [D loss: 0.5404702425003052] [G loss: 1.0366677045822144]\n",
      "[Epoch 2/5] [Batch 467/938] [D loss: 0.623108446598053] [G loss: 0.8793741464614868]\n",
      "[Epoch 2/5] [Batch 468/938] [D loss: 0.5515106916427612] [G loss: 1.237945795059204]\n",
      "[Epoch 2/5] [Batch 469/938] [D loss: 0.5946468114852905] [G loss: 0.8350143432617188]\n",
      "[Epoch 2/5] [Batch 470/938] [D loss: 0.6484154462814331] [G loss: 1.0887084007263184]\n",
      "[Epoch 2/5] [Batch 471/938] [D loss: 0.5474443435668945] [G loss: 1.175180196762085]\n",
      "[Epoch 2/5] [Batch 472/938] [D loss: 0.5621194243431091] [G loss: 0.8422176241874695]\n",
      "[Epoch 2/5] [Batch 473/938] [D loss: 0.6296282410621643] [G loss: 1.2521871328353882]\n",
      "[Epoch 2/5] [Batch 474/938] [D loss: 0.5914964079856873] [G loss: 0.6554017663002014]\n",
      "[Epoch 2/5] [Batch 475/938] [D loss: 0.6197834610939026] [G loss: 1.5864531993865967]\n",
      "[Epoch 2/5] [Batch 476/938] [D loss: 0.5962514281272888] [G loss: 0.8317627310752869]\n",
      "[Epoch 2/5] [Batch 477/938] [D loss: 0.6237980127334595] [G loss: 1.1061750650405884]\n",
      "[Epoch 2/5] [Batch 478/938] [D loss: 0.6299213767051697] [G loss: 1.0700860023498535]\n",
      "[Epoch 2/5] [Batch 479/938] [D loss: 0.5264286994934082] [G loss: 0.7991310358047485]\n",
      "[Epoch 2/5] [Batch 480/938] [D loss: 0.6585185527801514] [G loss: 0.9922599792480469]\n",
      "[Epoch 2/5] [Batch 481/938] [D loss: 0.5789773464202881] [G loss: 1.105708360671997]\n",
      "[Epoch 2/5] [Batch 482/938] [D loss: 0.5774703025817871] [G loss: 0.866186261177063]\n",
      "[Epoch 2/5] [Batch 483/938] [D loss: 0.6095907688140869] [G loss: 0.8641325235366821]\n",
      "[Epoch 2/5] [Batch 484/938] [D loss: 0.618353545665741] [G loss: 1.2708120346069336]\n",
      "[Epoch 2/5] [Batch 485/938] [D loss: 0.5774272680282593] [G loss: 1.0421664714813232]\n",
      "[Epoch 2/5] [Batch 486/938] [D loss: 0.6309195756912231] [G loss: 0.8511477112770081]\n",
      "[Epoch 2/5] [Batch 487/938] [D loss: 0.6471612453460693] [G loss: 0.7663673758506775]\n",
      "[Epoch 2/5] [Batch 488/938] [D loss: 0.6040037870407104] [G loss: 1.0413668155670166]\n",
      "[Epoch 2/5] [Batch 489/938] [D loss: 0.557998538017273] [G loss: 0.9219135642051697]\n",
      "[Epoch 2/5] [Batch 490/938] [D loss: 0.607701301574707] [G loss: 1.046201229095459]\n",
      "[Epoch 2/5] [Batch 491/938] [D loss: 0.5320782661437988] [G loss: 0.987212061882019]\n",
      "[Epoch 2/5] [Batch 492/938] [D loss: 0.5737910866737366] [G loss: 1.0270750522613525]\n",
      "[Epoch 2/5] [Batch 493/938] [D loss: 0.603670597076416] [G loss: 1.0290555953979492]\n",
      "[Epoch 2/5] [Batch 494/938] [D loss: 0.6813013553619385] [G loss: 0.8383282423019409]\n",
      "[Epoch 2/5] [Batch 495/938] [D loss: 0.6205891370773315] [G loss: 0.9835973381996155]\n",
      "[Epoch 2/5] [Batch 496/938] [D loss: 0.5506665706634521] [G loss: 1.0056449174880981]\n",
      "[Epoch 2/5] [Batch 497/938] [D loss: 0.5515648126602173] [G loss: 1.0663518905639648]\n",
      "[Epoch 2/5] [Batch 498/938] [D loss: 0.6127905249595642] [G loss: 1.1529474258422852]\n",
      "[Epoch 2/5] [Batch 499/938] [D loss: 0.6166068911552429] [G loss: 0.8419808745384216]\n",
      "[Epoch 2/5] [Batch 500/938] [D loss: 0.5672407150268555] [G loss: 0.9182658791542053]\n",
      "[Epoch 2/5] [Batch 501/938] [D loss: 0.5111162662506104] [G loss: 1.2723654508590698]\n",
      "[Epoch 2/5] [Batch 502/938] [D loss: 0.5997306108474731] [G loss: 0.8379571437835693]\n",
      "[Epoch 2/5] [Batch 503/938] [D loss: 0.5813674926757812] [G loss: 0.9594128131866455]\n",
      "[Epoch 2/5] [Batch 504/938] [D loss: 0.6132429838180542] [G loss: 1.0488415956497192]\n",
      "[Epoch 2/5] [Batch 505/938] [D loss: 0.6029293537139893] [G loss: 1.0137304067611694]\n",
      "[Epoch 2/5] [Batch 506/938] [D loss: 0.6283190250396729] [G loss: 1.0044934749603271]\n",
      "[Epoch 2/5] [Batch 507/938] [D loss: 0.5885112881660461] [G loss: 1.29347562789917]\n",
      "[Epoch 2/5] [Batch 508/938] [D loss: 0.6795896887779236] [G loss: 0.6748324036598206]\n",
      "[Epoch 2/5] [Batch 509/938] [D loss: 0.5992366075515747] [G loss: 1.2415283918380737]\n",
      "[Epoch 2/5] [Batch 510/938] [D loss: 0.5325925350189209] [G loss: 0.9493552446365356]\n",
      "[Epoch 2/5] [Batch 511/938] [D loss: 0.5896315574645996] [G loss: 0.8086697459220886]\n",
      "[Epoch 2/5] [Batch 512/938] [D loss: 0.5787225365638733] [G loss: 1.0783720016479492]\n",
      "[Epoch 2/5] [Batch 513/938] [D loss: 0.5606255531311035] [G loss: 0.911424458026886]\n",
      "[Epoch 2/5] [Batch 514/938] [D loss: 0.5838856101036072] [G loss: 0.9963615536689758]\n",
      "[Epoch 2/5] [Batch 515/938] [D loss: 0.633406400680542] [G loss: 0.9104757308959961]\n",
      "[Epoch 2/5] [Batch 516/938] [D loss: 0.5597982406616211] [G loss: 0.9905444979667664]\n",
      "[Epoch 2/5] [Batch 517/938] [D loss: 0.5937543511390686] [G loss: 1.01050865650177]\n",
      "[Epoch 2/5] [Batch 518/938] [D loss: 0.5417956709861755] [G loss: 1.0797803401947021]\n",
      "[Epoch 2/5] [Batch 519/938] [D loss: 0.6603379249572754] [G loss: 1.304182767868042]\n",
      "[Epoch 2/5] [Batch 520/938] [D loss: 0.6047325134277344] [G loss: 0.7677152752876282]\n",
      "[Epoch 2/5] [Batch 521/938] [D loss: 0.5850250124931335] [G loss: 1.0926721096038818]\n",
      "[Epoch 2/5] [Batch 522/938] [D loss: 0.634922444820404] [G loss: 1.1374530792236328]\n",
      "[Epoch 2/5] [Batch 523/938] [D loss: 0.583437442779541] [G loss: 0.8698285818099976]\n",
      "[Epoch 2/5] [Batch 524/938] [D loss: 0.5456546545028687] [G loss: 1.123639464378357]\n",
      "[Epoch 2/5] [Batch 525/938] [D loss: 0.5758404731750488] [G loss: 1.0140055418014526]\n",
      "[Epoch 2/5] [Batch 526/938] [D loss: 0.6353158950805664] [G loss: 0.8286623358726501]\n",
      "[Epoch 2/5] [Batch 527/938] [D loss: 0.6388236284255981] [G loss: 1.490641474723816]\n",
      "[Epoch 2/5] [Batch 528/938] [D loss: 0.6692450046539307] [G loss: 0.5692547559738159]\n",
      "[Epoch 2/5] [Batch 529/938] [D loss: 0.6440858244895935] [G loss: 1.0695880651474]\n",
      "[Epoch 2/5] [Batch 530/938] [D loss: 0.6253416538238525] [G loss: 1.1680818796157837]\n",
      "[Epoch 2/5] [Batch 531/938] [D loss: 0.6558613181114197] [G loss: 0.7329512238502502]\n",
      "[Epoch 2/5] [Batch 532/938] [D loss: 0.5429068207740784] [G loss: 0.8493193984031677]\n",
      "[Epoch 2/5] [Batch 533/938] [D loss: 0.5991190075874329] [G loss: 1.0633141994476318]\n",
      "[Epoch 2/5] [Batch 534/938] [D loss: 0.560348391532898] [G loss: 0.8900231122970581]\n",
      "[Epoch 2/5] [Batch 535/938] [D loss: 0.6620675325393677] [G loss: 0.8832916617393494]\n",
      "[Epoch 2/5] [Batch 536/938] [D loss: 0.5518057346343994] [G loss: 1.012824296951294]\n",
      "[Epoch 2/5] [Batch 537/938] [D loss: 0.6473357081413269] [G loss: 0.9070127010345459]\n",
      "[Epoch 2/5] [Batch 538/938] [D loss: 0.5997260212898254] [G loss: 0.8500122427940369]\n",
      "[Epoch 2/5] [Batch 539/938] [D loss: 0.5612944960594177] [G loss: 0.995188295841217]\n",
      "[Epoch 2/5] [Batch 540/938] [D loss: 0.5438333749771118] [G loss: 0.9647178649902344]\n",
      "[Epoch 2/5] [Batch 541/938] [D loss: 0.591869056224823] [G loss: 0.9160921573638916]\n",
      "[Epoch 2/5] [Batch 542/938] [D loss: 0.6232960224151611] [G loss: 1.236976981163025]\n",
      "[Epoch 2/5] [Batch 543/938] [D loss: 0.5660580396652222] [G loss: 0.9135959148406982]\n",
      "[Epoch 2/5] [Batch 544/938] [D loss: 0.6101675629615784] [G loss: 0.8962479829788208]\n",
      "[Epoch 2/5] [Batch 545/938] [D loss: 0.5610169768333435] [G loss: 0.9519200325012207]\n",
      "[Epoch 2/5] [Batch 546/938] [D loss: 0.5825690031051636] [G loss: 1.2408467531204224]\n",
      "[Epoch 2/5] [Batch 547/938] [D loss: 0.579272985458374] [G loss: 0.8076778650283813]\n",
      "[Epoch 2/5] [Batch 548/938] [D loss: 0.5216207504272461] [G loss: 0.9821442365646362]\n",
      "[Epoch 2/5] [Batch 549/938] [D loss: 0.5601062178611755] [G loss: 1.2829121351242065]\n",
      "[Epoch 2/5] [Batch 550/938] [D loss: 0.6276075839996338] [G loss: 1.1115580797195435]\n",
      "[Epoch 2/5] [Batch 551/938] [D loss: 0.6215753555297852] [G loss: 0.750115692615509]\n",
      "[Epoch 2/5] [Batch 552/938] [D loss: 0.59620201587677] [G loss: 1.2306716442108154]\n",
      "[Epoch 2/5] [Batch 553/938] [D loss: 0.5778233408927917] [G loss: 0.958763062953949]\n",
      "[Epoch 2/5] [Batch 554/938] [D loss: 0.6483207941055298] [G loss: 0.7708169221878052]\n",
      "[Epoch 2/5] [Batch 555/938] [D loss: 0.6275381445884705] [G loss: 1.186501145362854]\n",
      "[Epoch 2/5] [Batch 556/938] [D loss: 0.6174441576004028] [G loss: 0.8944442868232727]\n",
      "[Epoch 2/5] [Batch 557/938] [D loss: 0.5927720069885254] [G loss: 1.101647138595581]\n",
      "[Epoch 2/5] [Batch 558/938] [D loss: 0.5569068193435669] [G loss: 1.0332715511322021]\n",
      "[Epoch 2/5] [Batch 559/938] [D loss: 0.6195480227470398] [G loss: 0.7299509048461914]\n",
      "[Epoch 2/5] [Batch 560/938] [D loss: 0.6515130996704102] [G loss: 1.1936978101730347]\n",
      "[Epoch 2/5] [Batch 561/938] [D loss: 0.5637262463569641] [G loss: 0.8897448182106018]\n",
      "[Epoch 2/5] [Batch 562/938] [D loss: 0.5862668752670288] [G loss: 0.8670823574066162]\n",
      "[Epoch 2/5] [Batch 563/938] [D loss: 0.583698034286499] [G loss: 0.997210681438446]\n",
      "[Epoch 2/5] [Batch 564/938] [D loss: 0.5668611526489258] [G loss: 1.0075777769088745]\n",
      "[Epoch 2/5] [Batch 565/938] [D loss: 0.5794255137443542] [G loss: 0.9070174694061279]\n",
      "[Epoch 2/5] [Batch 566/938] [D loss: 0.5678851008415222] [G loss: 1.0869511365890503]\n",
      "[Epoch 2/5] [Batch 567/938] [D loss: 0.6153616905212402] [G loss: 0.8787746429443359]\n",
      "[Epoch 2/5] [Batch 568/938] [D loss: 0.5288586616516113] [G loss: 1.0125148296356201]\n",
      "[Epoch 2/5] [Batch 569/938] [D loss: 0.6049658060073853] [G loss: 1.039090633392334]\n",
      "[Epoch 2/5] [Batch 570/938] [D loss: 0.5339866876602173] [G loss: 0.9945144653320312]\n",
      "[Epoch 2/5] [Batch 571/938] [D loss: 0.5731117725372314] [G loss: 0.9879621863365173]\n",
      "[Epoch 2/5] [Batch 572/938] [D loss: 0.5811747908592224] [G loss: 0.9660435914993286]\n",
      "[Epoch 2/5] [Batch 573/938] [D loss: 0.525166928768158] [G loss: 1.1874370574951172]\n",
      "[Epoch 2/5] [Batch 574/938] [D loss: 0.5631381273269653] [G loss: 0.9528079032897949]\n",
      "[Epoch 2/5] [Batch 575/938] [D loss: 0.615967333316803] [G loss: 0.8327682614326477]\n",
      "[Epoch 2/5] [Batch 576/938] [D loss: 0.619901180267334] [G loss: 1.2224912643432617]\n",
      "[Epoch 2/5] [Batch 577/938] [D loss: 0.6337138414382935] [G loss: 0.6010311245918274]\n",
      "[Epoch 2/5] [Batch 578/938] [D loss: 0.630182147026062] [G loss: 1.4283398389816284]\n",
      "[Epoch 2/5] [Batch 579/938] [D loss: 0.5802379846572876] [G loss: 0.8948267102241516]\n",
      "[Epoch 2/5] [Batch 580/938] [D loss: 0.5962896347045898] [G loss: 0.8242993950843811]\n",
      "[Epoch 2/5] [Batch 581/938] [D loss: 0.6080365180969238] [G loss: 1.2813374996185303]\n",
      "[Epoch 2/5] [Batch 582/938] [D loss: 0.5459502935409546] [G loss: 0.8186988234519958]\n",
      "[Epoch 2/5] [Batch 583/938] [D loss: 0.5927139520645142] [G loss: 1.0751945972442627]\n",
      "[Epoch 2/5] [Batch 584/938] [D loss: 0.5936704874038696] [G loss: 1.301007628440857]\n",
      "[Epoch 2/5] [Batch 585/938] [D loss: 0.5626350045204163] [G loss: 0.938383936882019]\n",
      "[Epoch 2/5] [Batch 586/938] [D loss: 0.5591142773628235] [G loss: 1.1083669662475586]\n",
      "[Epoch 2/5] [Batch 587/938] [D loss: 0.6365749835968018] [G loss: 1.0116136074066162]\n",
      "[Epoch 2/5] [Batch 588/938] [D loss: 0.6452091932296753] [G loss: 0.9596518874168396]\n",
      "[Epoch 2/5] [Batch 589/938] [D loss: 0.5944662094116211] [G loss: 1.0220602750778198]\n",
      "[Epoch 2/5] [Batch 590/938] [D loss: 0.6014149785041809] [G loss: 1.1476631164550781]\n",
      "[Epoch 2/5] [Batch 591/938] [D loss: 0.6094603538513184] [G loss: 0.8094421625137329]\n",
      "[Epoch 2/5] [Batch 592/938] [D loss: 0.5627840161323547] [G loss: 0.980405867099762]\n",
      "[Epoch 2/5] [Batch 593/938] [D loss: 0.6604666709899902] [G loss: 0.8979614973068237]\n",
      "[Epoch 2/5] [Batch 594/938] [D loss: 0.5861092805862427] [G loss: 0.8477511405944824]\n",
      "[Epoch 2/5] [Batch 595/938] [D loss: 0.5898452997207642] [G loss: 1.0795435905456543]\n",
      "[Epoch 2/5] [Batch 596/938] [D loss: 0.5761172771453857] [G loss: 0.8964901566505432]\n",
      "[Epoch 2/5] [Batch 597/938] [D loss: 0.5828229188919067] [G loss: 1.1138763427734375]\n",
      "[Epoch 2/5] [Batch 598/938] [D loss: 0.6002827286720276] [G loss: 0.899362325668335]\n",
      "[Epoch 2/5] [Batch 599/938] [D loss: 0.6724230647087097] [G loss: 0.8464527726173401]\n",
      "[Epoch 2/5] [Batch 600/938] [D loss: 0.6169980764389038] [G loss: 0.8305982947349548]\n",
      "[Epoch 2/5] [Batch 601/938] [D loss: 0.6108538508415222] [G loss: 1.0301048755645752]\n",
      "[Epoch 2/5] [Batch 602/938] [D loss: 0.6469525098800659] [G loss: 1.000209093093872]\n",
      "[Epoch 2/5] [Batch 603/938] [D loss: 0.6475589871406555] [G loss: 0.9710981845855713]\n",
      "[Epoch 2/5] [Batch 604/938] [D loss: 0.5970101952552795] [G loss: 0.8906066417694092]\n",
      "[Epoch 2/5] [Batch 605/938] [D loss: 0.6056119203567505] [G loss: 1.0372340679168701]\n",
      "[Epoch 2/5] [Batch 606/938] [D loss: 0.6034808158874512] [G loss: 0.8698092699050903]\n",
      "[Epoch 2/5] [Batch 607/938] [D loss: 0.5826302766799927] [G loss: 1.0928633213043213]\n",
      "[Epoch 2/5] [Batch 608/938] [D loss: 0.6774589419364929] [G loss: 0.91865074634552]\n",
      "[Epoch 2/5] [Batch 609/938] [D loss: 0.5834802389144897] [G loss: 0.8237110376358032]\n",
      "[Epoch 2/5] [Batch 610/938] [D loss: 0.5783101320266724] [G loss: 1.0958445072174072]\n",
      "[Epoch 2/5] [Batch 611/938] [D loss: 0.5737588405609131] [G loss: 1.0774335861206055]\n",
      "[Epoch 2/5] [Batch 612/938] [D loss: 0.6527997255325317] [G loss: 0.8713239431381226]\n",
      "[Epoch 2/5] [Batch 613/938] [D loss: 0.619951069355011] [G loss: 1.136522889137268]\n",
      "[Epoch 2/5] [Batch 614/938] [D loss: 0.6140176057815552] [G loss: 0.9602558016777039]\n",
      "[Epoch 2/5] [Batch 615/938] [D loss: 0.5631792545318604] [G loss: 0.9926690459251404]\n",
      "[Epoch 2/5] [Batch 616/938] [D loss: 0.611253023147583] [G loss: 1.0609327554702759]\n",
      "[Epoch 2/5] [Batch 617/938] [D loss: 0.6457874774932861] [G loss: 0.8578083515167236]\n",
      "[Epoch 2/5] [Batch 618/938] [D loss: 0.6185388565063477] [G loss: 1.026186227798462]\n",
      "[Epoch 2/5] [Batch 619/938] [D loss: 0.626158595085144] [G loss: 1.0368661880493164]\n",
      "[Epoch 2/5] [Batch 620/938] [D loss: 0.5924661755561829] [G loss: 1.0713255405426025]\n",
      "[Epoch 2/5] [Batch 621/938] [D loss: 0.6175775527954102] [G loss: 0.7995572686195374]\n",
      "[Epoch 2/5] [Batch 622/938] [D loss: 0.5656365156173706] [G loss: 1.0507452487945557]\n",
      "[Epoch 2/5] [Batch 623/938] [D loss: 0.5614595413208008] [G loss: 1.0407259464263916]\n",
      "[Epoch 2/5] [Batch 624/938] [D loss: 0.6726714372634888] [G loss: 1.07710862159729]\n",
      "[Epoch 2/5] [Batch 625/938] [D loss: 0.5953001976013184] [G loss: 1.0727827548980713]\n",
      "[Epoch 2/5] [Batch 626/938] [D loss: 0.6134111285209656] [G loss: 1.0168852806091309]\n",
      "[Epoch 2/5] [Batch 627/938] [D loss: 0.5945628881454468] [G loss: 0.8923778533935547]\n",
      "[Epoch 2/5] [Batch 628/938] [D loss: 0.6433491706848145] [G loss: 1.375244140625]\n",
      "[Epoch 2/5] [Batch 629/938] [D loss: 0.6719000935554504] [G loss: 0.5627404451370239]\n",
      "[Epoch 2/5] [Batch 630/938] [D loss: 0.5962163209915161] [G loss: 1.179688572883606]\n",
      "[Epoch 2/5] [Batch 631/938] [D loss: 0.5907547473907471] [G loss: 1.1639862060546875]\n",
      "[Epoch 2/5] [Batch 632/938] [D loss: 0.6082398891448975] [G loss: 0.7384176850318909]\n",
      "[Epoch 2/5] [Batch 633/938] [D loss: 0.6355904340744019] [G loss: 0.8887671232223511]\n",
      "[Epoch 2/5] [Batch 634/938] [D loss: 0.5998510718345642] [G loss: 0.9683802127838135]\n",
      "[Epoch 2/5] [Batch 635/938] [D loss: 0.5948185920715332] [G loss: 0.8754180073738098]\n",
      "[Epoch 2/5] [Batch 636/938] [D loss: 0.6412469148635864] [G loss: 1.1056872606277466]\n",
      "[Epoch 2/5] [Batch 637/938] [D loss: 0.5802831053733826] [G loss: 0.8748170137405396]\n",
      "[Epoch 2/5] [Batch 638/938] [D loss: 0.5675882697105408] [G loss: 0.9327908754348755]\n",
      "[Epoch 2/5] [Batch 639/938] [D loss: 0.5784049034118652] [G loss: 0.9231815338134766]\n",
      "[Epoch 2/5] [Batch 640/938] [D loss: 0.5556997656822205] [G loss: 1.0444867610931396]\n",
      "[Epoch 2/5] [Batch 641/938] [D loss: 0.5152044892311096] [G loss: 0.9914330840110779]\n",
      "[Epoch 2/5] [Batch 642/938] [D loss: 0.6066219806671143] [G loss: 0.8630475401878357]\n",
      "[Epoch 2/5] [Batch 643/938] [D loss: 0.5935459733009338] [G loss: 1.2551906108856201]\n",
      "[Epoch 2/5] [Batch 644/938] [D loss: 0.6214762926101685] [G loss: 1.012656331062317]\n",
      "[Epoch 2/5] [Batch 645/938] [D loss: 0.5649062395095825] [G loss: 1.0944565534591675]\n",
      "[Epoch 2/5] [Batch 646/938] [D loss: 0.567391037940979] [G loss: 1.07892906665802]\n",
      "[Epoch 2/5] [Batch 647/938] [D loss: 0.5864543914794922] [G loss: 0.7588496804237366]\n",
      "[Epoch 2/5] [Batch 648/938] [D loss: 0.6412160396575928] [G loss: 1.3829848766326904]\n",
      "[Epoch 2/5] [Batch 649/938] [D loss: 0.5803101062774658] [G loss: 0.9019482135772705]\n",
      "[Epoch 2/5] [Batch 650/938] [D loss: 0.6172429323196411] [G loss: 0.7816639542579651]\n",
      "[Epoch 2/5] [Batch 651/938] [D loss: 0.5869630575180054] [G loss: 1.2736141681671143]\n",
      "[Epoch 2/5] [Batch 652/938] [D loss: 0.6295466423034668] [G loss: 0.997326135635376]\n",
      "[Epoch 2/5] [Batch 653/938] [D loss: 0.5468082427978516] [G loss: 1.0472114086151123]\n",
      "[Epoch 2/5] [Batch 654/938] [D loss: 0.5885576009750366] [G loss: 0.774383544921875]\n",
      "[Epoch 2/5] [Batch 655/938] [D loss: 0.627364456653595] [G loss: 1.21311354637146]\n",
      "[Epoch 2/5] [Batch 656/938] [D loss: 0.6120961308479309] [G loss: 0.9201385378837585]\n",
      "[Epoch 2/5] [Batch 657/938] [D loss: 0.6182718276977539] [G loss: 0.9023086428642273]\n",
      "[Epoch 2/5] [Batch 658/938] [D loss: 0.6295599937438965] [G loss: 1.1876240968704224]\n",
      "[Epoch 2/5] [Batch 659/938] [D loss: 0.5943467617034912] [G loss: 0.8740211725234985]\n",
      "[Epoch 2/5] [Batch 660/938] [D loss: 0.5530619621276855] [G loss: 1.1405608654022217]\n",
      "[Epoch 2/5] [Batch 661/938] [D loss: 0.5655269026756287] [G loss: 1.12554931640625]\n",
      "[Epoch 2/5] [Batch 662/938] [D loss: 0.6274176836013794] [G loss: 0.6648970246315002]\n",
      "[Epoch 2/5] [Batch 663/938] [D loss: 0.6612542271614075] [G loss: 1.4286707639694214]\n",
      "[Epoch 2/5] [Batch 664/938] [D loss: 0.5726089477539062] [G loss: 0.8887343406677246]\n",
      "[Epoch 2/5] [Batch 665/938] [D loss: 0.5932888984680176] [G loss: 0.9509326219558716]\n",
      "[Epoch 2/5] [Batch 666/938] [D loss: 0.5293940901756287] [G loss: 1.027326226234436]\n",
      "[Epoch 2/5] [Batch 667/938] [D loss: 0.5540847778320312] [G loss: 0.9498671293258667]\n",
      "[Epoch 2/5] [Batch 668/938] [D loss: 0.6206526160240173] [G loss: 1.206203818321228]\n",
      "[Epoch 2/5] [Batch 669/938] [D loss: 0.6019718050956726] [G loss: 0.8619727492332458]\n",
      "[Epoch 2/5] [Batch 670/938] [D loss: 0.6034034490585327] [G loss: 1.0229341983795166]\n",
      "[Epoch 2/5] [Batch 671/938] [D loss: 0.5827863812446594] [G loss: 0.961828351020813]\n",
      "[Epoch 2/5] [Batch 672/938] [D loss: 0.5698525905609131] [G loss: 0.9922198057174683]\n",
      "[Epoch 2/5] [Batch 673/938] [D loss: 0.6013027429580688] [G loss: 1.2163653373718262]\n",
      "[Epoch 2/5] [Batch 674/938] [D loss: 0.5614726543426514] [G loss: 0.9346504211425781]\n",
      "[Epoch 2/5] [Batch 675/938] [D loss: 0.5137145519256592] [G loss: 1.2518864870071411]\n",
      "[Epoch 2/5] [Batch 676/938] [D loss: 0.5803004503250122] [G loss: 0.8590965270996094]\n",
      "[Epoch 2/5] [Batch 677/938] [D loss: 0.6090323328971863] [G loss: 1.2267041206359863]\n",
      "[Epoch 2/5] [Batch 678/938] [D loss: 0.6149070262908936] [G loss: 1.192710041999817]\n",
      "[Epoch 2/5] [Batch 679/938] [D loss: 0.6555517911911011] [G loss: 0.8000333309173584]\n",
      "[Epoch 2/5] [Batch 680/938] [D loss: 0.5390450358390808] [G loss: 0.9219200015068054]\n",
      "[Epoch 2/5] [Batch 681/938] [D loss: 0.6314428448677063] [G loss: 1.3946914672851562]\n",
      "[Epoch 2/5] [Batch 682/938] [D loss: 0.60663902759552] [G loss: 0.813106894493103]\n",
      "[Epoch 2/5] [Batch 683/938] [D loss: 0.6012959480285645] [G loss: 0.8955254554748535]\n",
      "[Epoch 2/5] [Batch 684/938] [D loss: 0.6232624053955078] [G loss: 1.3904975652694702]\n",
      "[Epoch 2/5] [Batch 685/938] [D loss: 0.5925726890563965] [G loss: 0.7578936815261841]\n",
      "[Epoch 2/5] [Batch 686/938] [D loss: 0.6115714311599731] [G loss: 0.897010326385498]\n",
      "[Epoch 2/5] [Batch 687/938] [D loss: 0.6145182847976685] [G loss: 0.9556848406791687]\n",
      "[Epoch 2/5] [Batch 688/938] [D loss: 0.6632716655731201] [G loss: 1.0581138134002686]\n",
      "[Epoch 2/5] [Batch 689/938] [D loss: 0.6085613965988159] [G loss: 0.8388037085533142]\n",
      "[Epoch 2/5] [Batch 690/938] [D loss: 0.5675745010375977] [G loss: 0.8939211964607239]\n",
      "[Epoch 2/5] [Batch 691/938] [D loss: 0.6050134897232056] [G loss: 0.9297393560409546]\n",
      "[Epoch 2/5] [Batch 692/938] [D loss: 0.5465472936630249] [G loss: 1.172635555267334]\n",
      "[Epoch 2/5] [Batch 693/938] [D loss: 0.5973315238952637] [G loss: 1.0859497785568237]\n",
      "[Epoch 2/5] [Batch 694/938] [D loss: 0.5852160453796387] [G loss: 0.9394248723983765]\n",
      "[Epoch 2/5] [Batch 695/938] [D loss: 0.5587314367294312] [G loss: 0.9410804510116577]\n",
      "[Epoch 2/5] [Batch 696/938] [D loss: 0.6259123682975769] [G loss: 0.894039511680603]\n",
      "[Epoch 2/5] [Batch 697/938] [D loss: 0.5459999442100525] [G loss: 1.0298861265182495]\n",
      "[Epoch 2/5] [Batch 698/938] [D loss: 0.6710637211799622] [G loss: 0.9173623323440552]\n",
      "[Epoch 2/5] [Batch 699/938] [D loss: 0.5734553337097168] [G loss: 1.3380602598190308]\n",
      "[Epoch 2/5] [Batch 700/938] [D loss: 0.5652296543121338] [G loss: 0.9568644165992737]\n",
      "[Epoch 2/5] [Batch 701/938] [D loss: 0.6491174101829529] [G loss: 0.8372398614883423]\n",
      "[Epoch 2/5] [Batch 702/938] [D loss: 0.6203489899635315] [G loss: 1.156267523765564]\n",
      "[Epoch 2/5] [Batch 703/938] [D loss: 0.5808507800102234] [G loss: 0.870879590511322]\n",
      "[Epoch 2/5] [Batch 704/938] [D loss: 0.5694141387939453] [G loss: 0.8914332389831543]\n",
      "[Epoch 2/5] [Batch 705/938] [D loss: 0.6742452383041382] [G loss: 1.1337217092514038]\n",
      "[Epoch 2/5] [Batch 706/938] [D loss: 0.592435359954834] [G loss: 0.7157667279243469]\n",
      "[Epoch 2/5] [Batch 707/938] [D loss: 0.6493311524391174] [G loss: 0.8983655571937561]\n",
      "[Epoch 2/5] [Batch 708/938] [D loss: 0.6498650312423706] [G loss: 1.0803699493408203]\n",
      "[Epoch 2/5] [Batch 709/938] [D loss: 0.6161562204360962] [G loss: 0.8492686748504639]\n",
      "[Epoch 2/5] [Batch 710/938] [D loss: 0.5775395631790161] [G loss: 1.094702124595642]\n",
      "[Epoch 2/5] [Batch 711/938] [D loss: 0.6318162679672241] [G loss: 1.1807070970535278]\n",
      "[Epoch 2/5] [Batch 712/938] [D loss: 0.5976170301437378] [G loss: 1.0101318359375]\n",
      "[Epoch 2/5] [Batch 713/938] [D loss: 0.610305905342102] [G loss: 0.960826575756073]\n",
      "[Epoch 2/5] [Batch 714/938] [D loss: 0.5667938590049744] [G loss: 1.0836750268936157]\n",
      "[Epoch 2/5] [Batch 715/938] [D loss: 0.6273462772369385] [G loss: 0.8266937732696533]\n",
      "[Epoch 2/5] [Batch 716/938] [D loss: 0.6312843561172485] [G loss: 0.7778911590576172]\n",
      "[Epoch 2/5] [Batch 717/938] [D loss: 0.622235894203186] [G loss: 1.3000190258026123]\n",
      "[Epoch 2/5] [Batch 718/938] [D loss: 0.5406152009963989] [G loss: 0.8976074457168579]\n",
      "[Epoch 2/5] [Batch 719/938] [D loss: 0.5990667343139648] [G loss: 0.9904958009719849]\n",
      "[Epoch 2/5] [Batch 720/938] [D loss: 0.6567000150680542] [G loss: 1.2017853260040283]\n",
      "[Epoch 2/5] [Batch 721/938] [D loss: 0.5812151432037354] [G loss: 0.6730966567993164]\n",
      "[Epoch 2/5] [Batch 722/938] [D loss: 0.6074620485305786] [G loss: 1.1546066999435425]\n",
      "[Epoch 2/5] [Batch 723/938] [D loss: 0.6184866428375244] [G loss: 0.7291673421859741]\n",
      "[Epoch 2/5] [Batch 724/938] [D loss: 0.6180477142333984] [G loss: 0.9013997316360474]\n",
      "[Epoch 2/5] [Batch 725/938] [D loss: 0.6054472923278809] [G loss: 0.9291390776634216]\n",
      "[Epoch 2/5] [Batch 726/938] [D loss: 0.5687721967697144] [G loss: 0.966938853263855]\n",
      "[Epoch 2/5] [Batch 727/938] [D loss: 0.5994985699653625] [G loss: 0.9481925964355469]\n",
      "[Epoch 2/5] [Batch 728/938] [D loss: 0.6005297899246216] [G loss: 1.0864449739456177]\n",
      "[Epoch 2/5] [Batch 729/938] [D loss: 0.5516431331634521] [G loss: 0.8952735066413879]\n",
      "[Epoch 2/5] [Batch 730/938] [D loss: 0.5968426465988159] [G loss: 1.0080897808074951]\n",
      "[Epoch 2/5] [Batch 731/938] [D loss: 0.5867262482643127] [G loss: 0.8607332706451416]\n",
      "[Epoch 2/5] [Batch 732/938] [D loss: 0.6404276490211487] [G loss: 1.1197195053100586]\n",
      "[Epoch 2/5] [Batch 733/938] [D loss: 0.5878853797912598] [G loss: 0.6792623996734619]\n",
      "[Epoch 2/5] [Batch 734/938] [D loss: 0.5675523281097412] [G loss: 1.3219817876815796]\n",
      "[Epoch 2/5] [Batch 735/938] [D loss: 0.6344740390777588] [G loss: 0.8537195324897766]\n",
      "[Epoch 2/5] [Batch 736/938] [D loss: 0.5980484485626221] [G loss: 0.936843752861023]\n",
      "[Epoch 2/5] [Batch 737/938] [D loss: 0.5638847947120667] [G loss: 0.8661535382270813]\n",
      "[Epoch 2/5] [Batch 738/938] [D loss: 0.6155439615249634] [G loss: 1.056408166885376]\n",
      "[Epoch 2/5] [Batch 739/938] [D loss: 0.6320334672927856] [G loss: 1.0352071523666382]\n",
      "[Epoch 2/5] [Batch 740/938] [D loss: 0.5742641687393188] [G loss: 0.8598048686981201]\n",
      "[Epoch 2/5] [Batch 741/938] [D loss: 0.6056210398674011] [G loss: 1.1774464845657349]\n",
      "[Epoch 2/5] [Batch 742/938] [D loss: 0.6375133991241455] [G loss: 0.8511950373649597]\n",
      "[Epoch 2/5] [Batch 743/938] [D loss: 0.6056063771247864] [G loss: 1.2052364349365234]\n",
      "[Epoch 2/5] [Batch 744/938] [D loss: 0.6284193992614746] [G loss: 0.7044142484664917]\n",
      "[Epoch 2/5] [Batch 745/938] [D loss: 0.5864154100418091] [G loss: 1.036213755607605]\n",
      "[Epoch 2/5] [Batch 746/938] [D loss: 0.5842556953430176] [G loss: 1.1054527759552002]\n",
      "[Epoch 2/5] [Batch 747/938] [D loss: 0.5680686235427856] [G loss: 0.8122766613960266]\n",
      "[Epoch 2/5] [Batch 748/938] [D loss: 0.5806468725204468] [G loss: 0.995911717414856]\n",
      "[Epoch 2/5] [Batch 749/938] [D loss: 0.632140576839447] [G loss: 0.8050740361213684]\n",
      "[Epoch 2/5] [Batch 750/938] [D loss: 0.6178390979766846] [G loss: 1.0094795227050781]\n",
      "[Epoch 2/5] [Batch 751/938] [D loss: 0.5929203629493713] [G loss: 1.009446620941162]\n",
      "[Epoch 2/5] [Batch 752/938] [D loss: 0.49824386835098267] [G loss: 0.8580837249755859]\n",
      "[Epoch 2/5] [Batch 753/938] [D loss: 0.6010777950286865] [G loss: 0.8722056150436401]\n",
      "[Epoch 2/5] [Batch 754/938] [D loss: 0.5644865036010742] [G loss: 1.0287964344024658]\n",
      "[Epoch 2/5] [Batch 755/938] [D loss: 0.5942987203598022] [G loss: 0.9160876274108887]\n",
      "[Epoch 2/5] [Batch 756/938] [D loss: 0.5546300411224365] [G loss: 1.3013463020324707]\n",
      "[Epoch 2/5] [Batch 757/938] [D loss: 0.5760685205459595] [G loss: 0.9339370131492615]\n",
      "[Epoch 2/5] [Batch 758/938] [D loss: 0.5600477457046509] [G loss: 1.0492666959762573]\n",
      "[Epoch 2/5] [Batch 759/938] [D loss: 0.5978591442108154] [G loss: 1.2095811367034912]\n",
      "[Epoch 2/5] [Batch 760/938] [D loss: 0.6152017116546631] [G loss: 0.7267101407051086]\n",
      "[Epoch 2/5] [Batch 761/938] [D loss: 0.5557965040206909] [G loss: 1.2182339429855347]\n",
      "[Epoch 2/5] [Batch 762/938] [D loss: 0.579728364944458] [G loss: 0.9999207854270935]\n",
      "[Epoch 2/5] [Batch 763/938] [D loss: 0.6353462934494019] [G loss: 0.7857322692871094]\n",
      "[Epoch 2/5] [Batch 764/938] [D loss: 0.5592244267463684] [G loss: 1.2192641496658325]\n",
      "[Epoch 2/5] [Batch 765/938] [D loss: 0.5833597183227539] [G loss: 1.0373003482818604]\n",
      "[Epoch 2/5] [Batch 766/938] [D loss: 0.5979057550430298] [G loss: 1.124845266342163]\n",
      "[Epoch 2/5] [Batch 767/938] [D loss: 0.554375171661377] [G loss: 0.866916298866272]\n",
      "[Epoch 2/5] [Batch 768/938] [D loss: 0.6162300109863281] [G loss: 0.8438915014266968]\n",
      "[Epoch 2/5] [Batch 769/938] [D loss: 0.6501529216766357] [G loss: 0.9897518157958984]\n",
      "[Epoch 2/5] [Batch 770/938] [D loss: 0.5787055492401123] [G loss: 0.9272354245185852]\n",
      "[Epoch 2/5] [Batch 771/938] [D loss: 0.5856455564498901] [G loss: 1.0191407203674316]\n",
      "[Epoch 2/5] [Batch 772/938] [D loss: 0.5956368446350098] [G loss: 1.1020234823226929]\n",
      "[Epoch 2/5] [Batch 773/938] [D loss: 0.6327441334724426] [G loss: 0.9218519926071167]\n",
      "[Epoch 2/5] [Batch 774/938] [D loss: 0.6484310626983643] [G loss: 1.171461820602417]\n",
      "[Epoch 2/5] [Batch 775/938] [D loss: 0.6227002143859863] [G loss: 1.01460862159729]\n",
      "[Epoch 2/5] [Batch 776/938] [D loss: 0.6078298091888428] [G loss: 0.8007564544677734]\n",
      "[Epoch 2/5] [Batch 777/938] [D loss: 0.6359352469444275] [G loss: 1.1274561882019043]\n",
      "[Epoch 2/5] [Batch 778/938] [D loss: 0.5936294794082642] [G loss: 0.9565683007240295]\n",
      "[Epoch 2/5] [Batch 779/938] [D loss: 0.6158608198165894] [G loss: 0.932378351688385]\n",
      "[Epoch 2/5] [Batch 780/938] [D loss: 0.5959214568138123] [G loss: 1.0979708433151245]\n",
      "[Epoch 2/5] [Batch 781/938] [D loss: 0.6146060824394226] [G loss: 0.7694723606109619]\n",
      "[Epoch 2/5] [Batch 782/938] [D loss: 0.571752667427063] [G loss: 1.0026986598968506]\n",
      "[Epoch 2/5] [Batch 783/938] [D loss: 0.6121041774749756] [G loss: 1.045779824256897]\n",
      "[Epoch 2/5] [Batch 784/938] [D loss: 0.558349609375] [G loss: 0.9703124761581421]\n",
      "[Epoch 2/5] [Batch 785/938] [D loss: 0.6093235015869141] [G loss: 0.9576704502105713]\n",
      "[Epoch 2/5] [Batch 786/938] [D loss: 0.5954506397247314] [G loss: 1.0631386041641235]\n",
      "[Epoch 2/5] [Batch 787/938] [D loss: 0.5935665369033813] [G loss: 1.2023043632507324]\n",
      "[Epoch 2/5] [Batch 788/938] [D loss: 0.6107484698295593] [G loss: 0.8225470781326294]\n",
      "[Epoch 2/5] [Batch 789/938] [D loss: 0.5907585620880127] [G loss: 0.9450594186782837]\n",
      "[Epoch 2/5] [Batch 790/938] [D loss: 0.6177449226379395] [G loss: 1.2918474674224854]\n",
      "[Epoch 2/5] [Batch 791/938] [D loss: 0.6133483648300171] [G loss: 0.7030693292617798]\n",
      "[Epoch 2/5] [Batch 792/938] [D loss: 0.5319539308547974] [G loss: 1.1838810443878174]\n",
      "[Epoch 2/5] [Batch 793/938] [D loss: 0.5482044219970703] [G loss: 1.0714364051818848]\n",
      "[Epoch 2/5] [Batch 794/938] [D loss: 0.5851503014564514] [G loss: 0.9325534105300903]\n",
      "[Epoch 2/5] [Batch 795/938] [D loss: 0.5926055312156677] [G loss: 1.080877423286438]\n",
      "[Epoch 2/5] [Batch 796/938] [D loss: 0.6719515323638916] [G loss: 1.0236672163009644]\n",
      "[Epoch 2/5] [Batch 797/938] [D loss: 0.5582304000854492] [G loss: 0.7899799346923828]\n",
      "[Epoch 2/5] [Batch 798/938] [D loss: 0.6230456233024597] [G loss: 1.2475813627243042]\n",
      "[Epoch 2/5] [Batch 799/938] [D loss: 0.6655334830284119] [G loss: 0.7584880590438843]\n",
      "[Epoch 2/5] [Batch 800/938] [D loss: 0.6654984951019287] [G loss: 1.3086585998535156]\n",
      "[Epoch 2/5] [Batch 801/938] [D loss: 0.543226420879364] [G loss: 0.9735608100891113]\n",
      "[Epoch 2/5] [Batch 802/938] [D loss: 0.6104588508605957] [G loss: 0.7826006412506104]\n",
      "[Epoch 2/5] [Batch 803/938] [D loss: 0.5808064937591553] [G loss: 1.0438486337661743]\n",
      "[Epoch 2/5] [Batch 804/938] [D loss: 0.6162999868392944] [G loss: 1.049605131149292]\n",
      "[Epoch 2/5] [Batch 805/938] [D loss: 0.6334859132766724] [G loss: 0.8566851615905762]\n",
      "[Epoch 2/5] [Batch 806/938] [D loss: 0.6402667760848999] [G loss: 1.224562644958496]\n",
      "[Epoch 2/5] [Batch 807/938] [D loss: 0.5586515665054321] [G loss: 0.9660192131996155]\n",
      "[Epoch 2/5] [Batch 808/938] [D loss: 0.6594141721725464] [G loss: 0.7579572200775146]\n",
      "[Epoch 2/5] [Batch 809/938] [D loss: 0.5778008699417114] [G loss: 1.056881070137024]\n",
      "[Epoch 2/5] [Batch 810/938] [D loss: 0.6588122248649597] [G loss: 1.1671844720840454]\n",
      "[Epoch 2/5] [Batch 811/938] [D loss: 0.5561310648918152] [G loss: 0.8054237961769104]\n",
      "[Epoch 2/5] [Batch 812/938] [D loss: 0.6124534606933594] [G loss: 0.852185070514679]\n",
      "[Epoch 2/5] [Batch 813/938] [D loss: 0.55352783203125] [G loss: 1.2563139200210571]\n",
      "[Epoch 2/5] [Batch 814/938] [D loss: 0.6397860646247864] [G loss: 0.8425689339637756]\n",
      "[Epoch 2/5] [Batch 815/938] [D loss: 0.5928514003753662] [G loss: 0.9233090281486511]\n",
      "[Epoch 2/5] [Batch 816/938] [D loss: 0.6101593971252441] [G loss: 1.1896114349365234]\n",
      "[Epoch 2/5] [Batch 817/938] [D loss: 0.5702921152114868] [G loss: 0.89089435338974]\n",
      "[Epoch 2/5] [Batch 818/938] [D loss: 0.5810175538063049] [G loss: 0.9707379937171936]\n",
      "[Epoch 2/5] [Batch 819/938] [D loss: 0.5846642851829529] [G loss: 0.9648723602294922]\n",
      "[Epoch 2/5] [Batch 820/938] [D loss: 0.5797422528266907] [G loss: 0.9250780940055847]\n",
      "[Epoch 2/5] [Batch 821/938] [D loss: 0.61951744556427] [G loss: 0.6898623108863831]\n",
      "[Epoch 2/5] [Batch 822/938] [D loss: 0.5895546674728394] [G loss: 1.0771183967590332]\n",
      "[Epoch 2/5] [Batch 823/938] [D loss: 0.6411845088005066] [G loss: 0.99532151222229]\n",
      "[Epoch 2/5] [Batch 824/938] [D loss: 0.5125920176506042] [G loss: 0.8808777332305908]\n",
      "[Epoch 2/5] [Batch 825/938] [D loss: 0.5788172483444214] [G loss: 0.932701051235199]\n",
      "[Epoch 2/5] [Batch 826/938] [D loss: 0.553164541721344] [G loss: 1.0939418077468872]\n",
      "[Epoch 2/5] [Batch 827/938] [D loss: 0.514299213886261] [G loss: 1.0416855812072754]\n",
      "[Epoch 2/5] [Batch 828/938] [D loss: 0.630247175693512] [G loss: 1.185499906539917]\n",
      "[Epoch 2/5] [Batch 829/938] [D loss: 0.5999773740768433] [G loss: 1.0013480186462402]\n",
      "[Epoch 2/5] [Batch 830/938] [D loss: 0.6267046928405762] [G loss: 0.8167794346809387]\n",
      "[Epoch 2/5] [Batch 831/938] [D loss: 0.6487749814987183] [G loss: 1.0457286834716797]\n",
      "[Epoch 2/5] [Batch 832/938] [D loss: 0.580504834651947] [G loss: 1.0004210472106934]\n",
      "[Epoch 2/5] [Batch 833/938] [D loss: 0.5690300464630127] [G loss: 1.0530009269714355]\n",
      "[Epoch 2/5] [Batch 834/938] [D loss: 0.6327592134475708] [G loss: 1.0192161798477173]\n",
      "[Epoch 2/5] [Batch 835/938] [D loss: 0.5926459431648254] [G loss: 1.084465503692627]\n",
      "[Epoch 2/5] [Batch 836/938] [D loss: 0.6419459581375122] [G loss: 0.7850253582000732]\n",
      "[Epoch 2/5] [Batch 837/938] [D loss: 0.5692359209060669] [G loss: 1.0055431127548218]\n",
      "[Epoch 2/5] [Batch 838/938] [D loss: 0.5697939395904541] [G loss: 1.0532588958740234]\n",
      "[Epoch 2/5] [Batch 839/938] [D loss: 0.5607061982154846] [G loss: 0.9074422121047974]\n",
      "[Epoch 2/5] [Batch 840/938] [D loss: 0.579357922077179] [G loss: 0.9203349351882935]\n",
      "[Epoch 2/5] [Batch 841/938] [D loss: 0.5613541603088379] [G loss: 0.9840719699859619]\n",
      "[Epoch 2/5] [Batch 842/938] [D loss: 0.598283052444458] [G loss: 1.0115442276000977]\n",
      "[Epoch 2/5] [Batch 843/938] [D loss: 0.6059800386428833] [G loss: 0.9318328499794006]\n",
      "[Epoch 2/5] [Batch 844/938] [D loss: 0.557955265045166] [G loss: 0.9370644688606262]\n",
      "[Epoch 2/5] [Batch 845/938] [D loss: 0.6143039464950562] [G loss: 1.0897542238235474]\n",
      "[Epoch 2/5] [Batch 846/938] [D loss: 0.6931707859039307] [G loss: 0.9277169704437256]\n",
      "[Epoch 2/5] [Batch 847/938] [D loss: 0.5705547332763672] [G loss: 1.131838321685791]\n",
      "[Epoch 2/5] [Batch 848/938] [D loss: 0.622533917427063] [G loss: 0.9759767055511475]\n",
      "[Epoch 2/5] [Batch 849/938] [D loss: 0.6098724603652954] [G loss: 0.6432670950889587]\n",
      "[Epoch 2/5] [Batch 850/938] [D loss: 0.6128566265106201] [G loss: 1.0563039779663086]\n",
      "[Epoch 2/5] [Batch 851/938] [D loss: 0.5798147916793823] [G loss: 0.9207245707511902]\n",
      "[Epoch 2/5] [Batch 852/938] [D loss: 0.6681766510009766] [G loss: 1.1177387237548828]\n",
      "[Epoch 2/5] [Batch 853/938] [D loss: 0.5814502239227295] [G loss: 0.8294310569763184]\n",
      "[Epoch 2/5] [Batch 854/938] [D loss: 0.6531036496162415] [G loss: 1.1522618532180786]\n",
      "[Epoch 2/5] [Batch 855/938] [D loss: 0.6300480365753174] [G loss: 0.8710651397705078]\n",
      "[Epoch 2/5] [Batch 856/938] [D loss: 0.5650652050971985] [G loss: 0.952175498008728]\n",
      "[Epoch 2/5] [Batch 857/938] [D loss: 0.6058986186981201] [G loss: 0.9267712235450745]\n",
      "[Epoch 2/5] [Batch 858/938] [D loss: 0.6362998485565186] [G loss: 0.914035439491272]\n",
      "[Epoch 2/5] [Batch 859/938] [D loss: 0.5708379745483398] [G loss: 1.3099825382232666]\n",
      "[Epoch 2/5] [Batch 860/938] [D loss: 0.5864355564117432] [G loss: 0.9623123407363892]\n",
      "[Epoch 2/5] [Batch 861/938] [D loss: 0.6304919719696045] [G loss: 0.9104079008102417]\n",
      "[Epoch 2/5] [Batch 862/938] [D loss: 0.5823771357536316] [G loss: 1.01615571975708]\n",
      "[Epoch 2/5] [Batch 863/938] [D loss: 0.6307234764099121] [G loss: 0.9282316565513611]\n",
      "[Epoch 2/5] [Batch 864/938] [D loss: 0.6208181381225586] [G loss: 0.9765171408653259]\n",
      "[Epoch 2/5] [Batch 865/938] [D loss: 0.5896977186203003] [G loss: 1.0808665752410889]\n",
      "[Epoch 2/5] [Batch 866/938] [D loss: 0.6245924234390259] [G loss: 0.9303382039070129]\n",
      "[Epoch 2/5] [Batch 867/938] [D loss: 0.5469571352005005] [G loss: 0.8543635010719299]\n",
      "[Epoch 2/5] [Batch 868/938] [D loss: 0.6252493262290955] [G loss: 0.9340701103210449]\n",
      "[Epoch 2/5] [Batch 869/938] [D loss: 0.5521141290664673] [G loss: 0.9107645153999329]\n",
      "[Epoch 2/5] [Batch 870/938] [D loss: 0.5778411626815796] [G loss: 1.0725094079971313]\n",
      "[Epoch 2/5] [Batch 871/938] [D loss: 0.6057785749435425] [G loss: 0.9712461233139038]\n",
      "[Epoch 2/5] [Batch 872/938] [D loss: 0.577261745929718] [G loss: 0.810431718826294]\n",
      "[Epoch 2/5] [Batch 873/938] [D loss: 0.6101342439651489] [G loss: 1.452524185180664]\n",
      "[Epoch 2/5] [Batch 874/938] [D loss: 0.5927907228469849] [G loss: 0.9463127851486206]\n",
      "[Epoch 2/5] [Batch 875/938] [D loss: 0.5949485898017883] [G loss: 0.8285459876060486]\n",
      "[Epoch 2/5] [Batch 876/938] [D loss: 0.5958994626998901] [G loss: 0.959020733833313]\n",
      "[Epoch 2/5] [Batch 877/938] [D loss: 0.5742964744567871] [G loss: 1.0392329692840576]\n",
      "[Epoch 2/5] [Batch 878/938] [D loss: 0.6310656070709229] [G loss: 0.8017413020133972]\n",
      "[Epoch 2/5] [Batch 879/938] [D loss: 0.5962944626808167] [G loss: 1.0704917907714844]\n",
      "[Epoch 2/5] [Batch 880/938] [D loss: 0.6075266599655151] [G loss: 1.0348812341690063]\n",
      "[Epoch 2/5] [Batch 881/938] [D loss: 0.6270859837532043] [G loss: 0.8632887601852417]\n",
      "[Epoch 2/5] [Batch 882/938] [D loss: 0.6436944007873535] [G loss: 1.056925892829895]\n",
      "[Epoch 2/5] [Batch 883/938] [D loss: 0.6053731441497803] [G loss: 0.9775687456130981]\n",
      "[Epoch 2/5] [Batch 884/938] [D loss: 0.5902460813522339] [G loss: 0.9566546082496643]\n",
      "[Epoch 2/5] [Batch 885/938] [D loss: 0.5824201107025146] [G loss: 0.953338623046875]\n",
      "[Epoch 2/5] [Batch 886/938] [D loss: 0.5421030521392822] [G loss: 1.085160732269287]\n",
      "[Epoch 2/5] [Batch 887/938] [D loss: 0.5491855144500732] [G loss: 1.5151729583740234]\n",
      "[Epoch 2/5] [Batch 888/938] [D loss: 0.5877946019172668] [G loss: 0.7753196954727173]\n",
      "[Epoch 2/5] [Batch 889/938] [D loss: 0.603002667427063] [G loss: 1.0681902170181274]\n",
      "[Epoch 2/5] [Batch 890/938] [D loss: 0.6294659376144409] [G loss: 0.8919217586517334]\n",
      "[Epoch 2/5] [Batch 891/938] [D loss: 0.6158304214477539] [G loss: 1.2919657230377197]\n",
      "[Epoch 2/5] [Batch 892/938] [D loss: 0.5715324878692627] [G loss: 0.6937022805213928]\n",
      "[Epoch 2/5] [Batch 893/938] [D loss: 0.6106116771697998] [G loss: 1.1897697448730469]\n",
      "[Epoch 2/5] [Batch 894/938] [D loss: 0.5670142769813538] [G loss: 0.9715651869773865]\n",
      "[Epoch 2/5] [Batch 895/938] [D loss: 0.5971348285675049] [G loss: 0.9931343793869019]\n",
      "[Epoch 2/5] [Batch 896/938] [D loss: 0.6012806296348572] [G loss: 0.9488989114761353]\n",
      "[Epoch 2/5] [Batch 897/938] [D loss: 0.5932924747467041] [G loss: 0.9809428453445435]\n",
      "[Epoch 2/5] [Batch 898/938] [D loss: 0.5682355165481567] [G loss: 1.324142575263977]\n",
      "[Epoch 2/5] [Batch 899/938] [D loss: 0.6437529921531677] [G loss: 0.7965537309646606]\n",
      "[Epoch 2/5] [Batch 900/938] [D loss: 0.5666412115097046] [G loss: 1.0173428058624268]\n",
      "[Epoch 2/5] [Batch 901/938] [D loss: 0.6086909174919128] [G loss: 1.0442333221435547]\n",
      "[Epoch 2/5] [Batch 902/938] [D loss: 0.6599593162536621] [G loss: 1.109621524810791]\n",
      "[Epoch 2/5] [Batch 903/938] [D loss: 0.6297522187232971] [G loss: 0.7949930429458618]\n",
      "[Epoch 2/5] [Batch 904/938] [D loss: 0.6148879528045654] [G loss: 1.2918362617492676]\n",
      "[Epoch 2/5] [Batch 905/938] [D loss: 0.6213822364807129] [G loss: 0.8042144179344177]\n",
      "[Epoch 2/5] [Batch 906/938] [D loss: 0.5895777344703674] [G loss: 0.6896703839302063]\n",
      "[Epoch 2/5] [Batch 907/938] [D loss: 0.6442698836326599] [G loss: 1.1654527187347412]\n",
      "[Epoch 2/5] [Batch 908/938] [D loss: 0.6231385469436646] [G loss: 0.8930615782737732]\n",
      "[Epoch 2/5] [Batch 909/938] [D loss: 0.584527850151062] [G loss: 0.9964028596878052]\n",
      "[Epoch 2/5] [Batch 910/938] [D loss: 0.5559868216514587] [G loss: 0.8942171335220337]\n",
      "[Epoch 2/5] [Batch 911/938] [D loss: 0.6539415121078491] [G loss: 1.073586344718933]\n",
      "[Epoch 2/5] [Batch 912/938] [D loss: 0.5694140791893005] [G loss: 0.7741661667823792]\n",
      "[Epoch 2/5] [Batch 913/938] [D loss: 0.5917640924453735] [G loss: 0.8560958504676819]\n",
      "[Epoch 2/5] [Batch 914/938] [D loss: 0.5613948702812195] [G loss: 0.9870527386665344]\n",
      "[Epoch 2/5] [Batch 915/938] [D loss: 0.6354150772094727] [G loss: 0.9944946765899658]\n",
      "[Epoch 2/5] [Batch 916/938] [D loss: 0.6155409812927246] [G loss: 0.922982931137085]\n",
      "[Epoch 2/5] [Batch 917/938] [D loss: 0.648835301399231] [G loss: 0.7288659811019897]\n",
      "[Epoch 2/5] [Batch 918/938] [D loss: 0.6235673427581787] [G loss: 1.3156869411468506]\n",
      "[Epoch 2/5] [Batch 919/938] [D loss: 0.6016941070556641] [G loss: 0.9758087396621704]\n",
      "[Epoch 2/5] [Batch 920/938] [D loss: 0.6075435876846313] [G loss: 0.9884578585624695]\n",
      "[Epoch 2/5] [Batch 921/938] [D loss: 0.6054192781448364] [G loss: 1.181028127670288]\n",
      "[Epoch 2/5] [Batch 922/938] [D loss: 0.6498974561691284] [G loss: 0.6021203994750977]\n",
      "[Epoch 2/5] [Batch 923/938] [D loss: 0.5948430299758911] [G loss: 1.1801350116729736]\n",
      "[Epoch 2/5] [Batch 924/938] [D loss: 0.5724997520446777] [G loss: 1.0533781051635742]\n",
      "[Epoch 2/5] [Batch 925/938] [D loss: 0.5941222906112671] [G loss: 0.7072879076004028]\n",
      "[Epoch 2/5] [Batch 926/938] [D loss: 0.6220057010650635] [G loss: 1.0180836915969849]\n",
      "[Epoch 2/5] [Batch 927/938] [D loss: 0.6183569431304932] [G loss: 1.097872257232666]\n",
      "[Epoch 2/5] [Batch 928/938] [D loss: 0.5980599522590637] [G loss: 0.8426700830459595]\n",
      "[Epoch 2/5] [Batch 929/938] [D loss: 0.5957468748092651] [G loss: 0.8784700036048889]\n",
      "[Epoch 2/5] [Batch 930/938] [D loss: 0.6375385522842407] [G loss: 0.9674801230430603]\n",
      "[Epoch 2/5] [Batch 931/938] [D loss: 0.5975298285484314] [G loss: 1.0177669525146484]\n",
      "[Epoch 2/5] [Batch 932/938] [D loss: 0.5901094675064087] [G loss: 0.9312595129013062]\n",
      "[Epoch 2/5] [Batch 933/938] [D loss: 0.5572870373725891] [G loss: 0.9579737782478333]\n",
      "[Epoch 2/5] [Batch 934/938] [D loss: 0.614129900932312] [G loss: 0.9532182216644287]\n",
      "[Epoch 2/5] [Batch 935/938] [D loss: 0.5590077638626099] [G loss: 1.07362961769104]\n",
      "[Epoch 2/5] [Batch 936/938] [D loss: 0.5879321098327637] [G loss: 0.8878297209739685]\n",
      "[Epoch 2/5] [Batch 937/938] [D loss: 0.5876826047897339] [G loss: 1.139857292175293]\n",
      "[Epoch 3/5] [Batch 0/938] [D loss: 0.5444653630256653] [G loss: 0.9140084385871887]\n",
      "[Epoch 3/5] [Batch 1/938] [D loss: 0.6014000773429871] [G loss: 1.056318759918213]\n",
      "[Epoch 3/5] [Batch 2/938] [D loss: 0.5945423245429993] [G loss: 1.2460718154907227]\n",
      "[Epoch 3/5] [Batch 3/938] [D loss: 0.6778697371482849] [G loss: 0.6803048253059387]\n",
      "[Epoch 3/5] [Batch 4/938] [D loss: 0.5628277063369751] [G loss: 1.3855693340301514]\n",
      "[Epoch 3/5] [Batch 5/938] [D loss: 0.5796493291854858] [G loss: 0.9431589245796204]\n",
      "[Epoch 3/5] [Batch 6/938] [D loss: 0.6229723691940308] [G loss: 0.7574681043624878]\n",
      "[Epoch 3/5] [Batch 7/938] [D loss: 0.5867714881896973] [G loss: 1.2329652309417725]\n",
      "[Epoch 3/5] [Batch 8/938] [D loss: 0.6214632987976074] [G loss: 0.6852351427078247]\n",
      "[Epoch 3/5] [Batch 9/938] [D loss: 0.5816696882247925] [G loss: 0.9158346056938171]\n",
      "[Epoch 3/5] [Batch 10/938] [D loss: 0.5795823931694031] [G loss: 1.057738184928894]\n",
      "[Epoch 3/5] [Batch 11/938] [D loss: 0.6196988821029663] [G loss: 0.9118085503578186]\n",
      "[Epoch 3/5] [Batch 12/938] [D loss: 0.5347107648849487] [G loss: 0.9866775274276733]\n",
      "[Epoch 3/5] [Batch 13/938] [D loss: 0.5767735242843628] [G loss: 1.143937110900879]\n",
      "[Epoch 3/5] [Batch 14/938] [D loss: 0.6168364882469177] [G loss: 1.0049331188201904]\n",
      "[Epoch 3/5] [Batch 15/938] [D loss: 0.6712493896484375] [G loss: 1.1546849012374878]\n",
      "[Epoch 3/5] [Batch 16/938] [D loss: 0.6323829889297485] [G loss: 0.7870042324066162]\n",
      "[Epoch 3/5] [Batch 17/938] [D loss: 0.5935168266296387] [G loss: 1.0025972127914429]\n",
      "[Epoch 3/5] [Batch 18/938] [D loss: 0.6007466316223145] [G loss: 1.0465314388275146]\n",
      "[Epoch 3/5] [Batch 19/938] [D loss: 0.6121600866317749] [G loss: 0.7971853017807007]\n",
      "[Epoch 3/5] [Batch 20/938] [D loss: 0.6691610813140869] [G loss: 1.1639527082443237]\n",
      "[Epoch 3/5] [Batch 21/938] [D loss: 0.5789746046066284] [G loss: 0.8193116784095764]\n",
      "[Epoch 3/5] [Batch 22/938] [D loss: 0.6709888577461243] [G loss: 0.7969545125961304]\n",
      "[Epoch 3/5] [Batch 23/938] [D loss: 0.6203768253326416] [G loss: 1.3047637939453125]\n",
      "[Epoch 3/5] [Batch 24/938] [D loss: 0.6002035737037659] [G loss: 0.858392059803009]\n",
      "[Epoch 3/5] [Batch 25/938] [D loss: 0.642564058303833] [G loss: 0.7871116399765015]\n",
      "[Epoch 3/5] [Batch 26/938] [D loss: 0.6031909584999084] [G loss: 1.020849585533142]\n",
      "[Epoch 3/5] [Batch 27/938] [D loss: 0.5891279578208923] [G loss: 1.1500983238220215]\n",
      "[Epoch 3/5] [Batch 28/938] [D loss: 0.6412107348442078] [G loss: 0.6278374195098877]\n",
      "[Epoch 3/5] [Batch 29/938] [D loss: 0.5425431728363037] [G loss: 1.0769567489624023]\n",
      "[Epoch 3/5] [Batch 30/938] [D loss: 0.62642502784729] [G loss: 1.1972169876098633]\n",
      "[Epoch 3/5] [Batch 31/938] [D loss: 0.6150456070899963] [G loss: 0.8506424427032471]\n",
      "[Epoch 3/5] [Batch 32/938] [D loss: 0.6330841779708862] [G loss: 1.0217033624649048]\n",
      "[Epoch 3/5] [Batch 33/938] [D loss: 0.6154530048370361] [G loss: 1.1933139562606812]\n",
      "[Epoch 3/5] [Batch 34/938] [D loss: 0.6478850245475769] [G loss: 0.8104008436203003]\n",
      "[Epoch 3/5] [Batch 35/938] [D loss: 0.5429441928863525] [G loss: 0.9185168743133545]\n",
      "[Epoch 3/5] [Batch 36/938] [D loss: 0.5768090486526489] [G loss: 0.8882394433021545]\n",
      "[Epoch 3/5] [Batch 37/938] [D loss: 0.6056243181228638] [G loss: 0.9764278531074524]\n",
      "[Epoch 3/5] [Batch 38/938] [D loss: 0.5786583423614502] [G loss: 0.8990927934646606]\n",
      "[Epoch 3/5] [Batch 39/938] [D loss: 0.6048414707183838] [G loss: 1.163728952407837]\n",
      "[Epoch 3/5] [Batch 40/938] [D loss: 0.5832773447036743] [G loss: 0.7174309492111206]\n",
      "[Epoch 3/5] [Batch 41/938] [D loss: 0.6283410787582397] [G loss: 1.0147377252578735]\n",
      "[Epoch 3/5] [Batch 42/938] [D loss: 0.520953893661499] [G loss: 0.8335633873939514]\n",
      "[Epoch 3/5] [Batch 43/938] [D loss: 0.5887055397033691] [G loss: 1.1229639053344727]\n",
      "[Epoch 3/5] [Batch 44/938] [D loss: 0.5470804572105408] [G loss: 0.9191096425056458]\n",
      "[Epoch 3/5] [Batch 45/938] [D loss: 0.5724315047264099] [G loss: 0.924836277961731]\n",
      "[Epoch 3/5] [Batch 46/938] [D loss: 0.5756964683532715] [G loss: 1.0513180494308472]\n",
      "[Epoch 3/5] [Batch 47/938] [D loss: 0.5909673571586609] [G loss: 0.9588033556938171]\n",
      "[Epoch 3/5] [Batch 48/938] [D loss: 0.6342501640319824] [G loss: 1.2764276266098022]\n",
      "[Epoch 3/5] [Batch 49/938] [D loss: 0.5514518022537231] [G loss: 0.9269161224365234]\n",
      "[Epoch 3/5] [Batch 50/938] [D loss: 0.6184792518615723] [G loss: 0.986156702041626]\n",
      "[Epoch 3/5] [Batch 51/938] [D loss: 0.6309341192245483] [G loss: 0.8785293102264404]\n",
      "[Epoch 3/5] [Batch 52/938] [D loss: 0.634528636932373] [G loss: 1.0840320587158203]\n",
      "[Epoch 3/5] [Batch 53/938] [D loss: 0.6008922457695007] [G loss: 0.7842069268226624]\n",
      "[Epoch 3/5] [Batch 54/938] [D loss: 0.6146644949913025] [G loss: 1.19120454788208]\n",
      "[Epoch 3/5] [Batch 55/938] [D loss: 0.5843867063522339] [G loss: 0.7809842824935913]\n",
      "[Epoch 3/5] [Batch 56/938] [D loss: 0.6526073217391968] [G loss: 1.2899774312973022]\n",
      "[Epoch 3/5] [Batch 57/938] [D loss: 0.5801914930343628] [G loss: 0.9454044699668884]\n",
      "[Epoch 3/5] [Batch 58/938] [D loss: 0.5963624715805054] [G loss: 0.8817931413650513]\n",
      "[Epoch 3/5] [Batch 59/938] [D loss: 0.616887629032135] [G loss: 1.0900782346725464]\n",
      "[Epoch 3/5] [Batch 60/938] [D loss: 0.5921052694320679] [G loss: 1.0451680421829224]\n",
      "[Epoch 3/5] [Batch 61/938] [D loss: 0.6274871826171875] [G loss: 0.6872266530990601]\n",
      "[Epoch 3/5] [Batch 62/938] [D loss: 0.6351946592330933] [G loss: 1.2124942541122437]\n",
      "[Epoch 3/5] [Batch 63/938] [D loss: 0.5732041597366333] [G loss: 1.0232667922973633]\n",
      "[Epoch 3/5] [Batch 64/938] [D loss: 0.5805407166481018] [G loss: 0.8612990379333496]\n",
      "[Epoch 3/5] [Batch 65/938] [D loss: 0.5703015327453613] [G loss: 1.0442506074905396]\n",
      "[Epoch 3/5] [Batch 66/938] [D loss: 0.59258633852005] [G loss: 1.1319175958633423]\n",
      "[Epoch 3/5] [Batch 67/938] [D loss: 0.5251356363296509] [G loss: 0.8762813210487366]\n",
      "[Epoch 3/5] [Batch 68/938] [D loss: 0.5798271894454956] [G loss: 0.928349494934082]\n",
      "[Epoch 3/5] [Batch 69/938] [D loss: 0.6146550178527832] [G loss: 1.0976649522781372]\n",
      "[Epoch 3/5] [Batch 70/938] [D loss: 0.5953842401504517] [G loss: 0.8364036083221436]\n",
      "[Epoch 3/5] [Batch 71/938] [D loss: 0.6343361735343933] [G loss: 1.4162662029266357]\n",
      "[Epoch 3/5] [Batch 72/938] [D loss: 0.6335150003433228] [G loss: 0.7126134634017944]\n",
      "[Epoch 3/5] [Batch 73/938] [D loss: 0.5731416344642639] [G loss: 1.1071780920028687]\n",
      "[Epoch 3/5] [Batch 74/938] [D loss: 0.5453803539276123] [G loss: 0.8203912973403931]\n",
      "[Epoch 3/5] [Batch 75/938] [D loss: 0.548078715801239] [G loss: 1.1217912435531616]\n",
      "[Epoch 3/5] [Batch 76/938] [D loss: 0.5837522149085999] [G loss: 0.9115010499954224]\n",
      "[Epoch 3/5] [Batch 77/938] [D loss: 0.6293281316757202] [G loss: 1.0894404649734497]\n",
      "[Epoch 3/5] [Batch 78/938] [D loss: 0.6416285037994385] [G loss: 0.996771514415741]\n",
      "[Epoch 3/5] [Batch 79/938] [D loss: 0.5684365034103394] [G loss: 0.9247804880142212]\n",
      "[Epoch 3/5] [Batch 80/938] [D loss: 0.5826222896575928] [G loss: 0.9615983963012695]\n",
      "[Epoch 3/5] [Batch 81/938] [D loss: 0.6022221446037292] [G loss: 0.9241090416908264]\n",
      "[Epoch 3/5] [Batch 82/938] [D loss: 0.5971326231956482] [G loss: 1.0384353399276733]\n",
      "[Epoch 3/5] [Batch 83/938] [D loss: 0.6275922060012817] [G loss: 0.8378627896308899]\n",
      "[Epoch 3/5] [Batch 84/938] [D loss: 0.5991898775100708] [G loss: 0.9560381174087524]\n",
      "[Epoch 3/5] [Batch 85/938] [D loss: 0.6092804670333862] [G loss: 1.1018846035003662]\n",
      "[Epoch 3/5] [Batch 86/938] [D loss: 0.5890583992004395] [G loss: 0.9281107783317566]\n",
      "[Epoch 3/5] [Batch 87/938] [D loss: 0.5993247032165527] [G loss: 1.0203847885131836]\n",
      "[Epoch 3/5] [Batch 88/938] [D loss: 0.5727210640907288] [G loss: 0.9450049996376038]\n",
      "[Epoch 3/5] [Batch 89/938] [D loss: 0.5479083061218262] [G loss: 0.9176346063613892]\n",
      "[Epoch 3/5] [Batch 90/938] [D loss: 0.6278414726257324] [G loss: 1.2013801336288452]\n",
      "[Epoch 3/5] [Batch 91/938] [D loss: 0.5844374895095825] [G loss: 0.9896102547645569]\n",
      "[Epoch 3/5] [Batch 92/938] [D loss: 0.5710378289222717] [G loss: 0.8834583759307861]\n",
      "[Epoch 3/5] [Batch 93/938] [D loss: 0.5935933589935303] [G loss: 0.9305797219276428]\n",
      "[Epoch 3/5] [Batch 94/938] [D loss: 0.6199431419372559] [G loss: 1.0407447814941406]\n",
      "[Epoch 3/5] [Batch 95/938] [D loss: 0.6160761117935181] [G loss: 0.893488883972168]\n",
      "[Epoch 3/5] [Batch 96/938] [D loss: 0.605388879776001] [G loss: 1.0025283098220825]\n",
      "[Epoch 3/5] [Batch 97/938] [D loss: 0.608896791934967] [G loss: 1.1417834758758545]\n",
      "[Epoch 3/5] [Batch 98/938] [D loss: 0.6157732009887695] [G loss: 0.6015778183937073]\n",
      "[Epoch 3/5] [Batch 99/938] [D loss: 0.600900411605835] [G loss: 1.2300007343292236]\n",
      "[Epoch 3/5] [Batch 100/938] [D loss: 0.657813310623169] [G loss: 1.056901216506958]\n",
      "[Epoch 3/5] [Batch 101/938] [D loss: 0.6109046936035156] [G loss: 0.674208402633667]\n",
      "[Epoch 3/5] [Batch 102/938] [D loss: 0.5956657528877258] [G loss: 1.1881643533706665]\n",
      "[Epoch 3/5] [Batch 103/938] [D loss: 0.597411036491394] [G loss: 0.9165979027748108]\n",
      "[Epoch 3/5] [Batch 104/938] [D loss: 0.5849040746688843] [G loss: 0.9910754561424255]\n",
      "[Epoch 3/5] [Batch 105/938] [D loss: 0.5941735506057739] [G loss: 0.8269690275192261]\n",
      "[Epoch 3/5] [Batch 106/938] [D loss: 0.6191101670265198] [G loss: 0.9858153462409973]\n",
      "[Epoch 3/5] [Batch 107/938] [D loss: 0.6168883442878723] [G loss: 1.1389355659484863]\n",
      "[Epoch 3/5] [Batch 108/938] [D loss: 0.593246579170227] [G loss: 0.7216720581054688]\n",
      "[Epoch 3/5] [Batch 109/938] [D loss: 0.5870094299316406] [G loss: 1.1283901929855347]\n",
      "[Epoch 3/5] [Batch 110/938] [D loss: 0.6124019622802734] [G loss: 0.8960381746292114]\n",
      "[Epoch 3/5] [Batch 111/938] [D loss: 0.5941175222396851] [G loss: 1.0659481287002563]\n",
      "[Epoch 3/5] [Batch 112/938] [D loss: 0.5449919700622559] [G loss: 1.154881238937378]\n",
      "[Epoch 3/5] [Batch 113/938] [D loss: 0.6099718809127808] [G loss: 0.9637637138366699]\n",
      "[Epoch 3/5] [Batch 114/938] [D loss: 0.6347429752349854] [G loss: 1.1747198104858398]\n",
      "[Epoch 3/5] [Batch 115/938] [D loss: 0.6042219996452332] [G loss: 0.8640269041061401]\n",
      "[Epoch 3/5] [Batch 116/938] [D loss: 0.6614761352539062] [G loss: 1.0522485971450806]\n",
      "[Epoch 3/5] [Batch 117/938] [D loss: 0.6322749257087708] [G loss: 0.8329312801361084]\n",
      "[Epoch 3/5] [Batch 118/938] [D loss: 0.5495469570159912] [G loss: 1.0518345832824707]\n",
      "[Epoch 3/5] [Batch 119/938] [D loss: 0.6178514361381531] [G loss: 0.88807612657547]\n",
      "[Epoch 3/5] [Batch 120/938] [D loss: 0.616511344909668] [G loss: 0.8501701354980469]\n",
      "[Epoch 3/5] [Batch 121/938] [D loss: 0.6298781037330627] [G loss: 1.2446788549423218]\n",
      "[Epoch 3/5] [Batch 122/938] [D loss: 0.5725642442703247] [G loss: 0.9564001560211182]\n",
      "[Epoch 3/5] [Batch 123/938] [D loss: 0.6601976156234741] [G loss: 0.9395614862442017]\n",
      "[Epoch 3/5] [Batch 124/938] [D loss: 0.5497006177902222] [G loss: 0.9219684600830078]\n",
      "[Epoch 3/5] [Batch 125/938] [D loss: 0.6058294177055359] [G loss: 0.8565881252288818]\n",
      "[Epoch 3/5] [Batch 126/938] [D loss: 0.6408637762069702] [G loss: 1.122179627418518]\n",
      "[Epoch 3/5] [Batch 127/938] [D loss: 0.5685062408447266] [G loss: 0.9200965166091919]\n",
      "[Epoch 3/5] [Batch 128/938] [D loss: 0.5850300788879395] [G loss: 0.8540304899215698]\n",
      "[Epoch 3/5] [Batch 129/938] [D loss: 0.5942560434341431] [G loss: 0.9067480564117432]\n",
      "[Epoch 3/5] [Batch 130/938] [D loss: 0.6205612421035767] [G loss: 1.3156836032867432]\n",
      "[Epoch 3/5] [Batch 131/938] [D loss: 0.6020850539207458] [G loss: 0.7048945426940918]\n",
      "[Epoch 3/5] [Batch 132/938] [D loss: 0.5525060892105103] [G loss: 1.268499732017517]\n",
      "[Epoch 3/5] [Batch 133/938] [D loss: 0.5836559534072876] [G loss: 0.9568450450897217]\n",
      "[Epoch 3/5] [Batch 134/938] [D loss: 0.6283310651779175] [G loss: 1.0914115905761719]\n",
      "[Epoch 3/5] [Batch 135/938] [D loss: 0.6414716839790344] [G loss: 0.6393882036209106]\n",
      "[Epoch 3/5] [Batch 136/938] [D loss: 0.6063095927238464] [G loss: 1.2780802249908447]\n",
      "[Epoch 3/5] [Batch 137/938] [D loss: 0.6141220927238464] [G loss: 1.167504906654358]\n",
      "[Epoch 3/5] [Batch 138/938] [D loss: 0.6025509238243103] [G loss: 0.6873296499252319]\n",
      "[Epoch 3/5] [Batch 139/938] [D loss: 0.5912827253341675] [G loss: 1.161203384399414]\n",
      "[Epoch 3/5] [Batch 140/938] [D loss: 0.5119055509567261] [G loss: 0.9043731093406677]\n",
      "[Epoch 3/5] [Batch 141/938] [D loss: 0.591193675994873] [G loss: 0.8886996507644653]\n",
      "[Epoch 3/5] [Batch 142/938] [D loss: 0.605428159236908] [G loss: 0.9525586366653442]\n",
      "[Epoch 3/5] [Batch 143/938] [D loss: 0.5645030736923218] [G loss: 1.0505990982055664]\n",
      "[Epoch 3/5] [Batch 144/938] [D loss: 0.5716412663459778] [G loss: 0.8654578328132629]\n",
      "[Epoch 3/5] [Batch 145/938] [D loss: 0.6156413555145264] [G loss: 0.8714171051979065]\n",
      "[Epoch 3/5] [Batch 146/938] [D loss: 0.5393145084381104] [G loss: 1.1782112121582031]\n",
      "[Epoch 3/5] [Batch 147/938] [D loss: 0.6032769680023193] [G loss: 1.013923168182373]\n",
      "[Epoch 3/5] [Batch 148/938] [D loss: 0.5593236684799194] [G loss: 1.214337944984436]\n",
      "[Epoch 3/5] [Batch 149/938] [D loss: 0.6074892282485962] [G loss: 0.7377625107765198]\n",
      "[Epoch 3/5] [Batch 150/938] [D loss: 0.6936060190200806] [G loss: 1.3669511079788208]\n",
      "[Epoch 3/5] [Batch 151/938] [D loss: 0.6261414289474487] [G loss: 0.6227397918701172]\n",
      "[Epoch 3/5] [Batch 152/938] [D loss: 0.6411064863204956] [G loss: 1.4503874778747559]\n",
      "[Epoch 3/5] [Batch 153/938] [D loss: 0.6256303191184998] [G loss: 0.7218811511993408]\n",
      "[Epoch 3/5] [Batch 154/938] [D loss: 0.5657044649124146] [G loss: 0.8849219083786011]\n",
      "[Epoch 3/5] [Batch 155/938] [D loss: 0.6226162910461426] [G loss: 0.9398912191390991]\n",
      "[Epoch 3/5] [Batch 156/938] [D loss: 0.6129133105278015] [G loss: 0.9510486125946045]\n",
      "[Epoch 3/5] [Batch 157/938] [D loss: 0.6096150875091553] [G loss: 0.9736977815628052]\n",
      "[Epoch 3/5] [Batch 158/938] [D loss: 0.5813157558441162] [G loss: 0.8654257655143738]\n",
      "[Epoch 3/5] [Batch 159/938] [D loss: 0.5373234748840332] [G loss: 1.0074169635772705]\n",
      "[Epoch 3/5] [Batch 160/938] [D loss: 0.6610497236251831] [G loss: 0.8710092902183533]\n",
      "[Epoch 3/5] [Batch 161/938] [D loss: 0.5628089904785156] [G loss: 1.0677320957183838]\n",
      "[Epoch 3/5] [Batch 162/938] [D loss: 0.624657154083252] [G loss: 0.9767666459083557]\n",
      "[Epoch 3/5] [Batch 163/938] [D loss: 0.6132323741912842] [G loss: 0.7746636867523193]\n",
      "[Epoch 3/5] [Batch 164/938] [D loss: 0.6330215930938721] [G loss: 1.1550499200820923]\n",
      "[Epoch 3/5] [Batch 165/938] [D loss: 0.5467657446861267] [G loss: 0.7953399419784546]\n",
      "[Epoch 3/5] [Batch 166/938] [D loss: 0.5953714847564697] [G loss: 0.9443642497062683]\n",
      "[Epoch 3/5] [Batch 167/938] [D loss: 0.574809193611145] [G loss: 0.9239448308944702]\n",
      "[Epoch 3/5] [Batch 168/938] [D loss: 0.5490593910217285] [G loss: 1.0528078079223633]\n",
      "[Epoch 3/5] [Batch 169/938] [D loss: 0.5967980623245239] [G loss: 1.0239672660827637]\n",
      "[Epoch 3/5] [Batch 170/938] [D loss: 0.6498976945877075] [G loss: 0.8910637497901917]\n",
      "[Epoch 3/5] [Batch 171/938] [D loss: 0.5966999530792236] [G loss: 1.1172235012054443]\n",
      "[Epoch 3/5] [Batch 172/938] [D loss: 0.5676543712615967] [G loss: 0.9982346296310425]\n",
      "[Epoch 3/5] [Batch 173/938] [D loss: 0.6129892468452454] [G loss: 1.0522692203521729]\n",
      "[Epoch 3/5] [Batch 174/938] [D loss: 0.5881340503692627] [G loss: 1.1441272497177124]\n",
      "[Epoch 3/5] [Batch 175/938] [D loss: 0.6145347356796265] [G loss: 0.8328219056129456]\n",
      "[Epoch 3/5] [Batch 176/938] [D loss: 0.6229351758956909] [G loss: 0.8262253403663635]\n",
      "[Epoch 3/5] [Batch 177/938] [D loss: 0.6147075891494751] [G loss: 1.125589370727539]\n",
      "[Epoch 3/5] [Batch 178/938] [D loss: 0.5755395889282227] [G loss: 0.9809778332710266]\n",
      "[Epoch 3/5] [Batch 179/938] [D loss: 0.5758271217346191] [G loss: 1.0354676246643066]\n",
      "[Epoch 3/5] [Batch 180/938] [D loss: 0.5995982885360718] [G loss: 0.8366824984550476]\n",
      "[Epoch 3/5] [Batch 181/938] [D loss: 0.6514269709587097] [G loss: 1.4162379503250122]\n",
      "[Epoch 3/5] [Batch 182/938] [D loss: 0.6116420030593872] [G loss: 0.6502390503883362]\n",
      "[Epoch 3/5] [Batch 183/938] [D loss: 0.605475127696991] [G loss: 1.2224491834640503]\n",
      "[Epoch 3/5] [Batch 184/938] [D loss: 0.5780102014541626] [G loss: 0.8668200373649597]\n",
      "[Epoch 3/5] [Batch 185/938] [D loss: 0.6149373054504395] [G loss: 0.7662092447280884]\n",
      "[Epoch 3/5] [Batch 186/938] [D loss: 0.6505551338195801] [G loss: 1.5558476448059082]\n",
      "[Epoch 3/5] [Batch 187/938] [D loss: 0.6232337951660156] [G loss: 0.6135704517364502]\n",
      "[Epoch 3/5] [Batch 188/938] [D loss: 0.5927883982658386] [G loss: 0.8863953948020935]\n",
      "[Epoch 3/5] [Batch 189/938] [D loss: 0.599101185798645] [G loss: 1.2176140546798706]\n",
      "[Epoch 3/5] [Batch 190/938] [D loss: 0.605852484703064] [G loss: 0.9532268643379211]\n",
      "[Epoch 3/5] [Batch 191/938] [D loss: 0.6001777648925781] [G loss: 0.7091231942176819]\n",
      "[Epoch 3/5] [Batch 192/938] [D loss: 0.5945844650268555] [G loss: 0.8943418860435486]\n",
      "[Epoch 3/5] [Batch 193/938] [D loss: 0.5993509888648987] [G loss: 1.0046136379241943]\n",
      "[Epoch 3/5] [Batch 194/938] [D loss: 0.6160585880279541] [G loss: 0.9927773475646973]\n",
      "[Epoch 3/5] [Batch 195/938] [D loss: 0.5673115849494934] [G loss: 0.8370171785354614]\n",
      "[Epoch 3/5] [Batch 196/938] [D loss: 0.6117473840713501] [G loss: 0.8100197315216064]\n",
      "[Epoch 3/5] [Batch 197/938] [D loss: 0.6267435550689697] [G loss: 1.1180871725082397]\n",
      "[Epoch 3/5] [Batch 198/938] [D loss: 0.6210060119628906] [G loss: 0.7154008150100708]\n",
      "[Epoch 3/5] [Batch 199/938] [D loss: 0.6036170721054077] [G loss: 1.1347804069519043]\n",
      "[Epoch 3/5] [Batch 200/938] [D loss: 0.55193030834198] [G loss: 0.9124250411987305]\n",
      "[Epoch 3/5] [Batch 201/938] [D loss: 0.5647449493408203] [G loss: 0.9161921739578247]\n",
      "[Epoch 3/5] [Batch 202/938] [D loss: 0.5963879823684692] [G loss: 1.1432815790176392]\n",
      "[Epoch 3/5] [Batch 203/938] [D loss: 0.6007014513015747] [G loss: 0.9756860733032227]\n",
      "[Epoch 3/5] [Batch 204/938] [D loss: 0.5882357358932495] [G loss: 1.0853484869003296]\n",
      "[Epoch 3/5] [Batch 205/938] [D loss: 0.5873348116874695] [G loss: 0.8719282746315002]\n",
      "[Epoch 3/5] [Batch 206/938] [D loss: 0.6452436447143555] [G loss: 1.3390840291976929]\n",
      "[Epoch 3/5] [Batch 207/938] [D loss: 0.6184384226799011] [G loss: 0.7440189719200134]\n",
      "[Epoch 3/5] [Batch 208/938] [D loss: 0.6223626732826233] [G loss: 0.9450158476829529]\n",
      "[Epoch 3/5] [Batch 209/938] [D loss: 0.5912560224533081] [G loss: 1.1259782314300537]\n",
      "[Epoch 3/5] [Batch 210/938] [D loss: 0.6026549339294434] [G loss: 0.8272039890289307]\n",
      "[Epoch 3/5] [Batch 211/938] [D loss: 0.592492938041687] [G loss: 0.9114533066749573]\n",
      "[Epoch 3/5] [Batch 212/938] [D loss: 0.6145068407058716] [G loss: 0.93632972240448]\n",
      "[Epoch 3/5] [Batch 213/938] [D loss: 0.5606964826583862] [G loss: 0.942805826663971]\n",
      "[Epoch 3/5] [Batch 214/938] [D loss: 0.558303952217102] [G loss: 0.9253052473068237]\n",
      "[Epoch 3/5] [Batch 215/938] [D loss: 0.568942666053772] [G loss: 1.0330299139022827]\n",
      "[Epoch 3/5] [Batch 216/938] [D loss: 0.6108512282371521] [G loss: 1.178126335144043]\n",
      "[Epoch 3/5] [Batch 217/938] [D loss: 0.6163443922996521] [G loss: 0.8211922645568848]\n",
      "[Epoch 3/5] [Batch 218/938] [D loss: 0.6498484015464783] [G loss: 1.1866772174835205]\n",
      "[Epoch 3/5] [Batch 219/938] [D loss: 0.6676124334335327] [G loss: 0.7747982144355774]\n",
      "[Epoch 3/5] [Batch 220/938] [D loss: 0.6376432776451111] [G loss: 0.9619231224060059]\n",
      "[Epoch 3/5] [Batch 221/938] [D loss: 0.5950368642807007] [G loss: 0.9554367661476135]\n",
      "[Epoch 3/5] [Batch 222/938] [D loss: 0.6260467171669006] [G loss: 0.9925223588943481]\n",
      "[Epoch 3/5] [Batch 223/938] [D loss: 0.6485940217971802] [G loss: 0.7828501462936401]\n",
      "[Epoch 3/5] [Batch 224/938] [D loss: 0.6262807846069336] [G loss: 1.197799563407898]\n",
      "[Epoch 3/5] [Batch 225/938] [D loss: 0.6195009350776672] [G loss: 0.7789304256439209]\n",
      "[Epoch 3/5] [Batch 226/938] [D loss: 0.6117279529571533] [G loss: 0.8472949862480164]\n",
      "[Epoch 3/5] [Batch 227/938] [D loss: 0.572873592376709] [G loss: 1.086739182472229]\n",
      "[Epoch 3/5] [Batch 228/938] [D loss: 0.6268596649169922] [G loss: 0.918120801448822]\n",
      "[Epoch 3/5] [Batch 229/938] [D loss: 0.6560142040252686] [G loss: 0.8312050104141235]\n",
      "[Epoch 3/5] [Batch 230/938] [D loss: 0.5970828533172607] [G loss: 0.9471958875656128]\n",
      "[Epoch 3/5] [Batch 231/938] [D loss: 0.6052165627479553] [G loss: 0.923651397228241]\n",
      "[Epoch 3/5] [Batch 232/938] [D loss: 0.5807185769081116] [G loss: 1.0354058742523193]\n",
      "[Epoch 3/5] [Batch 233/938] [D loss: 0.6071075201034546] [G loss: 0.9761157035827637]\n",
      "[Epoch 3/5] [Batch 234/938] [D loss: 0.6256198883056641] [G loss: 0.8393691182136536]\n",
      "[Epoch 3/5] [Batch 235/938] [D loss: 0.5815690755844116] [G loss: 1.057819128036499]\n",
      "[Epoch 3/5] [Batch 236/938] [D loss: 0.5858045816421509] [G loss: 0.9264734983444214]\n",
      "[Epoch 3/5] [Batch 237/938] [D loss: 0.5545755624771118] [G loss: 1.0242855548858643]\n",
      "[Epoch 3/5] [Batch 238/938] [D loss: 0.6526546478271484] [G loss: 1.0859367847442627]\n",
      "[Epoch 3/5] [Batch 239/938] [D loss: 0.61802738904953] [G loss: 0.9796226024627686]\n",
      "[Epoch 3/5] [Batch 240/938] [D loss: 0.6652741432189941] [G loss: 0.9653729200363159]\n",
      "[Epoch 3/5] [Batch 241/938] [D loss: 0.6204261779785156] [G loss: 0.7961615920066833]\n",
      "[Epoch 3/5] [Batch 242/938] [D loss: 0.5312628746032715] [G loss: 0.9766965508460999]\n",
      "[Epoch 3/5] [Batch 243/938] [D loss: 0.5888310670852661] [G loss: 1.000072956085205]\n",
      "[Epoch 3/5] [Batch 244/938] [D loss: 0.5663979053497314] [G loss: 1.0856486558914185]\n",
      "[Epoch 3/5] [Batch 245/938] [D loss: 0.6097490787506104] [G loss: 1.0141181945800781]\n",
      "[Epoch 3/5] [Batch 246/938] [D loss: 0.6291484236717224] [G loss: 0.784546434879303]\n",
      "[Epoch 3/5] [Batch 247/938] [D loss: 0.6288063526153564] [G loss: 1.11712646484375]\n",
      "[Epoch 3/5] [Batch 248/938] [D loss: 0.5754181742668152] [G loss: 0.8217246532440186]\n",
      "[Epoch 3/5] [Batch 249/938] [D loss: 0.6365655660629272] [G loss: 1.0815534591674805]\n",
      "[Epoch 3/5] [Batch 250/938] [D loss: 0.6110009551048279] [G loss: 0.8431702256202698]\n",
      "[Epoch 3/5] [Batch 251/938] [D loss: 0.5655930042266846] [G loss: 1.1839368343353271]\n",
      "[Epoch 3/5] [Batch 252/938] [D loss: 0.591741681098938] [G loss: 0.8797941207885742]\n",
      "[Epoch 3/5] [Batch 253/938] [D loss: 0.6381196975708008] [G loss: 0.8996835947036743]\n",
      "[Epoch 3/5] [Batch 254/938] [D loss: 0.5970263481140137] [G loss: 1.2968813180923462]\n",
      "[Epoch 3/5] [Batch 255/938] [D loss: 0.6360449194908142] [G loss: 0.6492445468902588]\n",
      "[Epoch 3/5] [Batch 256/938] [D loss: 0.5748591423034668] [G loss: 1.2634446620941162]\n",
      "[Epoch 3/5] [Batch 257/938] [D loss: 0.5648808479309082] [G loss: 0.914714515209198]\n",
      "[Epoch 3/5] [Batch 258/938] [D loss: 0.6367954015731812] [G loss: 0.7735006213188171]\n",
      "[Epoch 3/5] [Batch 259/938] [D loss: 0.5995855331420898] [G loss: 1.1453830003738403]\n",
      "[Epoch 3/5] [Batch 260/938] [D loss: 0.5692620277404785] [G loss: 0.883407711982727]\n",
      "[Epoch 3/5] [Batch 261/938] [D loss: 0.6601545810699463] [G loss: 0.9742130041122437]\n",
      "[Epoch 3/5] [Batch 262/938] [D loss: 0.5960572957992554] [G loss: 1.0541934967041016]\n",
      "[Epoch 3/5] [Batch 263/938] [D loss: 0.6798129081726074] [G loss: 0.7480881214141846]\n",
      "[Epoch 3/5] [Batch 264/938] [D loss: 0.5992618799209595] [G loss: 1.084251880645752]\n",
      "[Epoch 3/5] [Batch 265/938] [D loss: 0.5987788438796997] [G loss: 1.0010416507720947]\n",
      "[Epoch 3/5] [Batch 266/938] [D loss: 0.600766122341156] [G loss: 0.8999211192131042]\n",
      "[Epoch 3/5] [Batch 267/938] [D loss: 0.6089446544647217] [G loss: 0.9165409207344055]\n",
      "[Epoch 3/5] [Batch 268/938] [D loss: 0.6069609522819519] [G loss: 0.930435061454773]\n",
      "[Epoch 3/5] [Batch 269/938] [D loss: 0.631468653678894] [G loss: 1.1537015438079834]\n",
      "[Epoch 3/5] [Batch 270/938] [D loss: 0.611737847328186] [G loss: 0.7051798105239868]\n",
      "[Epoch 3/5] [Batch 271/938] [D loss: 0.5804443955421448] [G loss: 1.0131759643554688]\n",
      "[Epoch 3/5] [Batch 272/938] [D loss: 0.5993338823318481] [G loss: 1.006630301475525]\n",
      "[Epoch 3/5] [Batch 273/938] [D loss: 0.6176753044128418] [G loss: 0.8656949996948242]\n",
      "[Epoch 3/5] [Batch 274/938] [D loss: 0.5621875524520874] [G loss: 1.1298471689224243]\n",
      "[Epoch 3/5] [Batch 275/938] [D loss: 0.6165835857391357] [G loss: 1.1733566522598267]\n",
      "[Epoch 3/5] [Batch 276/938] [D loss: 0.6165786981582642] [G loss: 0.6833975315093994]\n",
      "[Epoch 3/5] [Batch 277/938] [D loss: 0.5722415447235107] [G loss: 0.9581526517868042]\n",
      "[Epoch 3/5] [Batch 278/938] [D loss: 0.6412193179130554] [G loss: 1.1760226488113403]\n",
      "[Epoch 3/5] [Batch 279/938] [D loss: 0.6440388560295105] [G loss: 0.7147904634475708]\n",
      "[Epoch 3/5] [Batch 280/938] [D loss: 0.5965406894683838] [G loss: 0.9729263782501221]\n",
      "[Epoch 3/5] [Batch 281/938] [D loss: 0.6065059900283813] [G loss: 1.0087499618530273]\n",
      "[Epoch 3/5] [Batch 282/938] [D loss: 0.6205170154571533] [G loss: 0.7958402037620544]\n",
      "[Epoch 3/5] [Batch 283/938] [D loss: 0.5499851703643799] [G loss: 1.0784802436828613]\n",
      "[Epoch 3/5] [Batch 284/938] [D loss: 0.5687917470932007] [G loss: 1.0183613300323486]\n",
      "[Epoch 3/5] [Batch 285/938] [D loss: 0.6110495328903198] [G loss: 0.9625898599624634]\n",
      "[Epoch 3/5] [Batch 286/938] [D loss: 0.629673182964325] [G loss: 1.2077878713607788]\n",
      "[Epoch 3/5] [Batch 287/938] [D loss: 0.5728675723075867] [G loss: 0.8610788583755493]\n",
      "[Epoch 3/5] [Batch 288/938] [D loss: 0.6290215253829956] [G loss: 1.0660837888717651]\n",
      "[Epoch 3/5] [Batch 289/938] [D loss: 0.6025331020355225] [G loss: 0.8962313532829285]\n",
      "[Epoch 3/5] [Batch 290/938] [D loss: 0.5875672101974487] [G loss: 0.9354783892631531]\n",
      "[Epoch 3/5] [Batch 291/938] [D loss: 0.6547302007675171] [G loss: 0.9077778458595276]\n",
      "[Epoch 3/5] [Batch 292/938] [D loss: 0.5942636728286743] [G loss: 0.880774974822998]\n",
      "[Epoch 3/5] [Batch 293/938] [D loss: 0.6410714983940125] [G loss: 1.0193233489990234]\n",
      "[Epoch 3/5] [Batch 294/938] [D loss: 0.5479567050933838] [G loss: 0.8442775011062622]\n",
      "[Epoch 3/5] [Batch 295/938] [D loss: 0.6228405237197876] [G loss: 0.839928925037384]\n",
      "[Epoch 3/5] [Batch 296/938] [D loss: 0.5786252617835999] [G loss: 0.9469509124755859]\n",
      "[Epoch 3/5] [Batch 297/938] [D loss: 0.5825421810150146] [G loss: 1.1202398538589478]\n",
      "[Epoch 3/5] [Batch 298/938] [D loss: 0.5965534448623657] [G loss: 0.9797040224075317]\n",
      "[Epoch 3/5] [Batch 299/938] [D loss: 0.6223493218421936] [G loss: 0.8691737055778503]\n",
      "[Epoch 3/5] [Batch 300/938] [D loss: 0.6682231426239014] [G loss: 1.049702525138855]\n",
      "[Epoch 3/5] [Batch 301/938] [D loss: 0.5689760446548462] [G loss: 1.14547860622406]\n",
      "[Epoch 3/5] [Batch 302/938] [D loss: 0.6180026531219482] [G loss: 0.9430220127105713]\n",
      "[Epoch 3/5] [Batch 303/938] [D loss: 0.6025533676147461] [G loss: 0.8988116979598999]\n",
      "[Epoch 3/5] [Batch 304/938] [D loss: 0.6426871418952942] [G loss: 1.0802088975906372]\n",
      "[Epoch 3/5] [Batch 305/938] [D loss: 0.6570760011672974] [G loss: 0.8544731736183167]\n",
      "[Epoch 3/5] [Batch 306/938] [D loss: 0.6428091526031494] [G loss: 0.9352017045021057]\n",
      "[Epoch 3/5] [Batch 307/938] [D loss: 0.6537794470787048] [G loss: 1.0815354585647583]\n",
      "[Epoch 3/5] [Batch 308/938] [D loss: 0.5774011611938477] [G loss: 0.8185413479804993]\n",
      "[Epoch 3/5] [Batch 309/938] [D loss: 0.5789254903793335] [G loss: 0.9468675255775452]\n",
      "[Epoch 3/5] [Batch 310/938] [D loss: 0.615435004234314] [G loss: 1.0862104892730713]\n",
      "[Epoch 3/5] [Batch 311/938] [D loss: 0.6470097899436951] [G loss: 0.6949380040168762]\n",
      "[Epoch 3/5] [Batch 312/938] [D loss: 0.5775805115699768] [G loss: 1.1446478366851807]\n",
      "[Epoch 3/5] [Batch 313/938] [D loss: 0.6504506468772888] [G loss: 0.7242786288261414]\n",
      "[Epoch 3/5] [Batch 314/938] [D loss: 0.643136739730835] [G loss: 0.9738386869430542]\n",
      "[Epoch 3/5] [Batch 315/938] [D loss: 0.6014478206634521] [G loss: 0.9266298413276672]\n",
      "[Epoch 3/5] [Batch 316/938] [D loss: 0.6099408268928528] [G loss: 0.7627090215682983]\n",
      "[Epoch 3/5] [Batch 317/938] [D loss: 0.6009822487831116] [G loss: 0.9450143575668335]\n",
      "[Epoch 3/5] [Batch 318/938] [D loss: 0.6057592630386353] [G loss: 0.9129186868667603]\n",
      "[Epoch 3/5] [Batch 319/938] [D loss: 0.6344156861305237] [G loss: 0.9593971967697144]\n",
      "[Epoch 3/5] [Batch 320/938] [D loss: 0.6004071235656738] [G loss: 0.9221546649932861]\n",
      "[Epoch 3/5] [Batch 321/938] [D loss: 0.5120615363121033] [G loss: 1.0840879678726196]\n",
      "[Epoch 3/5] [Batch 322/938] [D loss: 0.6500008702278137] [G loss: 0.923351526260376]\n",
      "[Epoch 3/5] [Batch 323/938] [D loss: 0.6104859113693237] [G loss: 0.7933648824691772]\n",
      "[Epoch 3/5] [Batch 324/938] [D loss: 0.5115800499916077] [G loss: 1.0533243417739868]\n",
      "[Epoch 3/5] [Batch 325/938] [D loss: 0.6232499480247498] [G loss: 0.7620249390602112]\n",
      "[Epoch 3/5] [Batch 326/938] [D loss: 0.5928632020950317] [G loss: 1.0390663146972656]\n",
      "[Epoch 3/5] [Batch 327/938] [D loss: 0.6140480637550354] [G loss: 1.2892006635665894]\n",
      "[Epoch 3/5] [Batch 328/938] [D loss: 0.6210422515869141] [G loss: 0.7590455412864685]\n",
      "[Epoch 3/5] [Batch 329/938] [D loss: 0.616439938545227] [G loss: 1.084252119064331]\n",
      "[Epoch 3/5] [Batch 330/938] [D loss: 0.6278724670410156] [G loss: 0.9267576932907104]\n",
      "[Epoch 3/5] [Batch 331/938] [D loss: 0.6217276453971863] [G loss: 0.7964822053909302]\n",
      "[Epoch 3/5] [Batch 332/938] [D loss: 0.6316118836402893] [G loss: 1.099729061126709]\n",
      "[Epoch 3/5] [Batch 333/938] [D loss: 0.5978344082832336] [G loss: 0.7468900680541992]\n",
      "[Epoch 3/5] [Batch 334/938] [D loss: 0.5961326956748962] [G loss: 0.9621837735176086]\n",
      "[Epoch 3/5] [Batch 335/938] [D loss: 0.5295425653457642] [G loss: 1.0286930799484253]\n",
      "[Epoch 3/5] [Batch 336/938] [D loss: 0.5983200669288635] [G loss: 0.9247347712516785]\n",
      "[Epoch 3/5] [Batch 337/938] [D loss: 0.5905277729034424] [G loss: 1.0010452270507812]\n",
      "[Epoch 3/5] [Batch 338/938] [D loss: 0.6086231470108032] [G loss: 1.0031462907791138]\n",
      "[Epoch 3/5] [Batch 339/938] [D loss: 0.6547046899795532] [G loss: 0.7698125243186951]\n",
      "[Epoch 3/5] [Batch 340/938] [D loss: 0.6413406133651733] [G loss: 1.3017176389694214]\n",
      "[Epoch 3/5] [Batch 341/938] [D loss: 0.5616346001625061] [G loss: 0.9967963099479675]\n",
      "[Epoch 3/5] [Batch 342/938] [D loss: 0.6020427942276001] [G loss: 0.858672022819519]\n",
      "[Epoch 3/5] [Batch 343/938] [D loss: 0.5492749810218811] [G loss: 1.1712592840194702]\n",
      "[Epoch 3/5] [Batch 344/938] [D loss: 0.6370683908462524] [G loss: 0.8606414794921875]\n",
      "[Epoch 3/5] [Batch 345/938] [D loss: 0.6445763111114502] [G loss: 0.9304829239845276]\n",
      "[Epoch 3/5] [Batch 346/938] [D loss: 0.6023674011230469] [G loss: 0.9702035188674927]\n",
      "[Epoch 3/5] [Batch 347/938] [D loss: 0.5806372761726379] [G loss: 0.9438720345497131]\n",
      "[Epoch 3/5] [Batch 348/938] [D loss: 0.5756162405014038] [G loss: 0.872891366481781]\n",
      "[Epoch 3/5] [Batch 349/938] [D loss: 0.6080302000045776] [G loss: 1.1074268817901611]\n",
      "[Epoch 3/5] [Batch 350/938] [D loss: 0.6337316036224365] [G loss: 0.8906269073486328]\n",
      "[Epoch 3/5] [Batch 351/938] [D loss: 0.6566734313964844] [G loss: 1.2144575119018555]\n",
      "[Epoch 3/5] [Batch 352/938] [D loss: 0.5959371328353882] [G loss: 0.8574665784835815]\n",
      "[Epoch 3/5] [Batch 353/938] [D loss: 0.5749826431274414] [G loss: 0.9118605852127075]\n",
      "[Epoch 3/5] [Batch 354/938] [D loss: 0.6761596202850342] [G loss: 0.9554910063743591]\n",
      "[Epoch 3/5] [Batch 355/938] [D loss: 0.6038784980773926] [G loss: 0.9010953903198242]\n",
      "[Epoch 3/5] [Batch 356/938] [D loss: 0.6414332389831543] [G loss: 0.956533670425415]\n",
      "[Epoch 3/5] [Batch 357/938] [D loss: 0.6440213918685913] [G loss: 0.8546048402786255]\n",
      "[Epoch 3/5] [Batch 358/938] [D loss: 0.5806056261062622] [G loss: 0.7637647986412048]\n",
      "[Epoch 3/5] [Batch 359/938] [D loss: 0.5384193658828735] [G loss: 0.9534890651702881]\n",
      "[Epoch 3/5] [Batch 360/938] [D loss: 0.5965183973312378] [G loss: 1.1961205005645752]\n",
      "[Epoch 3/5] [Batch 361/938] [D loss: 0.6375644207000732] [G loss: 0.7972403168678284]\n",
      "[Epoch 3/5] [Batch 362/938] [D loss: 0.6146124601364136] [G loss: 0.9729527831077576]\n",
      "[Epoch 3/5] [Batch 363/938] [D loss: 0.6553821563720703] [G loss: 0.8526350259780884]\n",
      "[Epoch 3/5] [Batch 364/938] [D loss: 0.5970386862754822] [G loss: 0.9950189590454102]\n",
      "[Epoch 3/5] [Batch 365/938] [D loss: 0.6137566566467285] [G loss: 1.0007230043411255]\n",
      "[Epoch 3/5] [Batch 366/938] [D loss: 0.6283830404281616] [G loss: 0.9053996205329895]\n",
      "[Epoch 3/5] [Batch 367/938] [D loss: 0.6015552282333374] [G loss: 1.1364660263061523]\n",
      "[Epoch 3/5] [Batch 368/938] [D loss: 0.5442397594451904] [G loss: 0.9394167065620422]\n",
      "[Epoch 3/5] [Batch 369/938] [D loss: 0.6429612040519714] [G loss: 0.828405499458313]\n",
      "[Epoch 3/5] [Batch 370/938] [D loss: 0.6560662984848022] [G loss: 1.086405873298645]\n",
      "[Epoch 3/5] [Batch 371/938] [D loss: 0.6182805895805359] [G loss: 0.9077789783477783]\n",
      "[Epoch 3/5] [Batch 372/938] [D loss: 0.6810553073883057] [G loss: 0.8563821315765381]\n",
      "[Epoch 3/5] [Batch 373/938] [D loss: 0.5549606680870056] [G loss: 1.0406097173690796]\n",
      "[Epoch 3/5] [Batch 374/938] [D loss: 0.6564633846282959] [G loss: 1.1304881572723389]\n",
      "[Epoch 3/5] [Batch 375/938] [D loss: 0.6476073265075684] [G loss: 0.6667447686195374]\n",
      "[Epoch 3/5] [Batch 376/938] [D loss: 0.5815340280532837] [G loss: 1.0604578256607056]\n",
      "[Epoch 3/5] [Batch 377/938] [D loss: 0.5580068230628967] [G loss: 0.8853242993354797]\n",
      "[Epoch 3/5] [Batch 378/938] [D loss: 0.5704866647720337] [G loss: 0.8561425805091858]\n",
      "[Epoch 3/5] [Batch 379/938] [D loss: 0.6191703677177429] [G loss: 0.9844748973846436]\n",
      "[Epoch 3/5] [Batch 380/938] [D loss: 0.6490639448165894] [G loss: 0.7468506097793579]\n",
      "[Epoch 3/5] [Batch 381/938] [D loss: 0.6241923570632935] [G loss: 1.0753037929534912]\n",
      "[Epoch 3/5] [Batch 382/938] [D loss: 0.6229315996170044] [G loss: 0.8007265329360962]\n",
      "[Epoch 3/5] [Batch 383/938] [D loss: 0.5891069173812866] [G loss: 0.8496289253234863]\n",
      "[Epoch 3/5] [Batch 384/938] [D loss: 0.6325081586837769] [G loss: 1.0257019996643066]\n",
      "[Epoch 3/5] [Batch 385/938] [D loss: 0.6669296622276306] [G loss: 0.9333417415618896]\n",
      "[Epoch 3/5] [Batch 386/938] [D loss: 0.6216464042663574] [G loss: 0.8171879649162292]\n",
      "[Epoch 3/5] [Batch 387/938] [D loss: 0.6289137005805969] [G loss: 0.7591679692268372]\n",
      "[Epoch 3/5] [Batch 388/938] [D loss: 0.624812126159668] [G loss: 0.9893069267272949]\n",
      "[Epoch 3/5] [Batch 389/938] [D loss: 0.6050921678543091] [G loss: 0.8974655270576477]\n",
      "[Epoch 3/5] [Batch 390/938] [D loss: 0.6508671641349792] [G loss: 0.8941085338592529]\n",
      "[Epoch 3/5] [Batch 391/938] [D loss: 0.6170576810836792] [G loss: 0.7771175503730774]\n",
      "[Epoch 3/5] [Batch 392/938] [D loss: 0.6163095235824585] [G loss: 1.0582860708236694]\n",
      "[Epoch 3/5] [Batch 393/938] [D loss: 0.5669268369674683] [G loss: 1.0092312097549438]\n",
      "[Epoch 3/5] [Batch 394/938] [D loss: 0.6358814239501953] [G loss: 0.9183577299118042]\n",
      "[Epoch 3/5] [Batch 395/938] [D loss: 0.6119346618652344] [G loss: 1.039048671722412]\n",
      "[Epoch 3/5] [Batch 396/938] [D loss: 0.5679185390472412] [G loss: 1.063366174697876]\n",
      "[Epoch 3/5] [Batch 397/938] [D loss: 0.5858563780784607] [G loss: 0.9675743579864502]\n",
      "[Epoch 3/5] [Batch 398/938] [D loss: 0.6155898571014404] [G loss: 0.7908210158348083]\n",
      "[Epoch 3/5] [Batch 399/938] [D loss: 0.6373998522758484] [G loss: 0.9808200597763062]\n",
      "[Epoch 3/5] [Batch 400/938] [D loss: 0.6034724116325378] [G loss: 0.8221628069877625]\n",
      "[Epoch 3/5] [Batch 401/938] [D loss: 0.6322605013847351] [G loss: 0.9141542911529541]\n",
      "[Epoch 3/5] [Batch 402/938] [D loss: 0.6016950607299805] [G loss: 0.8999339938163757]\n",
      "[Epoch 3/5] [Batch 403/938] [D loss: 0.5860185027122498] [G loss: 0.9253638982772827]\n",
      "[Epoch 3/5] [Batch 404/938] [D loss: 0.628882884979248] [G loss: 0.9974750280380249]\n",
      "[Epoch 3/5] [Batch 405/938] [D loss: 0.6778479814529419] [G loss: 0.7946354746818542]\n",
      "[Epoch 3/5] [Batch 406/938] [D loss: 0.6330267190933228] [G loss: 1.0837138891220093]\n",
      "[Epoch 3/5] [Batch 407/938] [D loss: 0.6303738355636597] [G loss: 0.8306143283843994]\n",
      "[Epoch 3/5] [Batch 408/938] [D loss: 0.6178489923477173] [G loss: 0.9023088216781616]\n",
      "[Epoch 3/5] [Batch 409/938] [D loss: 0.6140537858009338] [G loss: 1.0981909036636353]\n",
      "[Epoch 3/5] [Batch 410/938] [D loss: 0.6034354567527771] [G loss: 0.7093411087989807]\n",
      "[Epoch 3/5] [Batch 411/938] [D loss: 0.6155157685279846] [G loss: 0.9466754794120789]\n",
      "[Epoch 3/5] [Batch 412/938] [D loss: 0.6016015410423279] [G loss: 0.8894388675689697]\n",
      "[Epoch 3/5] [Batch 413/938] [D loss: 0.5390349626541138] [G loss: 0.7198092937469482]\n",
      "[Epoch 3/5] [Batch 414/938] [D loss: 0.5853836536407471] [G loss: 1.0284337997436523]\n",
      "[Epoch 3/5] [Batch 415/938] [D loss: 0.6391026973724365] [G loss: 1.0430221557617188]\n",
      "[Epoch 3/5] [Batch 416/938] [D loss: 0.579997181892395] [G loss: 0.8148664236068726]\n",
      "[Epoch 3/5] [Batch 417/938] [D loss: 0.5733149647712708] [G loss: 0.8535904884338379]\n",
      "[Epoch 3/5] [Batch 418/938] [D loss: 0.6221067905426025] [G loss: 0.9786550402641296]\n",
      "[Epoch 3/5] [Batch 419/938] [D loss: 0.5971486568450928] [G loss: 0.9785062074661255]\n",
      "[Epoch 3/5] [Batch 420/938] [D loss: 0.5533050298690796] [G loss: 1.0563571453094482]\n",
      "[Epoch 3/5] [Batch 421/938] [D loss: 0.5963934659957886] [G loss: 1.1173338890075684]\n",
      "[Epoch 3/5] [Batch 422/938] [D loss: 0.6456996202468872] [G loss: 0.6695334315299988]\n",
      "[Epoch 3/5] [Batch 423/938] [D loss: 0.6949784755706787] [G loss: 1.4684675931930542]\n",
      "[Epoch 3/5] [Batch 424/938] [D loss: 0.6830794215202332] [G loss: 0.5859463214874268]\n",
      "[Epoch 3/5] [Batch 425/938] [D loss: 0.624192476272583] [G loss: 0.9461908340454102]\n",
      "[Epoch 3/5] [Batch 426/938] [D loss: 0.573161780834198] [G loss: 0.9938819408416748]\n",
      "[Epoch 3/5] [Batch 427/938] [D loss: 0.6497856974601746] [G loss: 0.848915696144104]\n",
      "[Epoch 3/5] [Batch 428/938] [D loss: 0.6225229501724243] [G loss: 0.8954112529754639]\n",
      "[Epoch 3/5] [Batch 429/938] [D loss: 0.639443039894104] [G loss: 1.045820713043213]\n",
      "[Epoch 3/5] [Batch 430/938] [D loss: 0.5984377861022949] [G loss: 0.884113073348999]\n",
      "[Epoch 3/5] [Batch 431/938] [D loss: 0.6609209775924683] [G loss: 0.8290541768074036]\n",
      "[Epoch 3/5] [Batch 432/938] [D loss: 0.5986194014549255] [G loss: 1.162062168121338]\n",
      "[Epoch 3/5] [Batch 433/938] [D loss: 0.656735897064209] [G loss: 0.8841281533241272]\n",
      "[Epoch 3/5] [Batch 434/938] [D loss: 0.6520940065383911] [G loss: 0.8209736943244934]\n",
      "[Epoch 3/5] [Batch 435/938] [D loss: 0.5831494927406311] [G loss: 1.1533076763153076]\n",
      "[Epoch 3/5] [Batch 436/938] [D loss: 0.5607202649116516] [G loss: 0.817816436290741]\n",
      "[Epoch 3/5] [Batch 437/938] [D loss: 0.6063856482505798] [G loss: 0.9589107036590576]\n",
      "[Epoch 3/5] [Batch 438/938] [D loss: 0.6065884828567505] [G loss: 1.050338625907898]\n",
      "[Epoch 3/5] [Batch 439/938] [D loss: 0.5612672567367554] [G loss: 0.8980085849761963]\n",
      "[Epoch 3/5] [Batch 440/938] [D loss: 0.5937730073928833] [G loss: 0.9357412457466125]\n",
      "[Epoch 3/5] [Batch 441/938] [D loss: 0.5786986351013184] [G loss: 0.9861162900924683]\n",
      "[Epoch 3/5] [Batch 442/938] [D loss: 0.6069999933242798] [G loss: 1.0867910385131836]\n",
      "[Epoch 3/5] [Batch 443/938] [D loss: 0.6802834272384644] [G loss: 1.0398706197738647]\n",
      "[Epoch 3/5] [Batch 444/938] [D loss: 0.6006309390068054] [G loss: 0.6929789781570435]\n",
      "[Epoch 3/5] [Batch 445/938] [D loss: 0.6201345920562744] [G loss: 0.8929364681243896]\n",
      "[Epoch 3/5] [Batch 446/938] [D loss: 0.6030139923095703] [G loss: 1.1168124675750732]\n",
      "[Epoch 3/5] [Batch 447/938] [D loss: 0.6283947825431824] [G loss: 0.8028557300567627]\n",
      "[Epoch 3/5] [Batch 448/938] [D loss: 0.5602758526802063] [G loss: 1.0313677787780762]\n",
      "[Epoch 3/5] [Batch 449/938] [D loss: 0.5983612537384033] [G loss: 0.9055400490760803]\n",
      "[Epoch 3/5] [Batch 450/938] [D loss: 0.6197928786277771] [G loss: 1.051679253578186]\n",
      "[Epoch 3/5] [Batch 451/938] [D loss: 0.651760995388031] [G loss: 0.7297208905220032]\n",
      "[Epoch 3/5] [Batch 452/938] [D loss: 0.6396457552909851] [G loss: 1.062203288078308]\n",
      "[Epoch 3/5] [Batch 453/938] [D loss: 0.6068158745765686] [G loss: 0.8313207030296326]\n",
      "[Epoch 3/5] [Batch 454/938] [D loss: 0.5735917091369629] [G loss: 0.8750646114349365]\n",
      "[Epoch 3/5] [Batch 455/938] [D loss: 0.5952304005622864] [G loss: 1.1078393459320068]\n",
      "[Epoch 3/5] [Batch 456/938] [D loss: 0.6016775369644165] [G loss: 0.9054980278015137]\n",
      "[Epoch 3/5] [Batch 457/938] [D loss: 0.5851002931594849] [G loss: 0.8842390179634094]\n",
      "[Epoch 3/5] [Batch 458/938] [D loss: 0.6409103870391846] [G loss: 1.0699083805084229]\n",
      "[Epoch 3/5] [Batch 459/938] [D loss: 0.6526379585266113] [G loss: 0.9638010263442993]\n",
      "[Epoch 3/5] [Batch 460/938] [D loss: 0.684715747833252] [G loss: 0.8217571377754211]\n",
      "[Epoch 3/5] [Batch 461/938] [D loss: 0.6124849319458008] [G loss: 0.9363210201263428]\n",
      "[Epoch 3/5] [Batch 462/938] [D loss: 0.599402904510498] [G loss: 0.9699804186820984]\n",
      "[Epoch 3/5] [Batch 463/938] [D loss: 0.5505261421203613] [G loss: 0.9301624298095703]\n",
      "[Epoch 3/5] [Batch 464/938] [D loss: 0.6014602184295654] [G loss: 0.8975713849067688]\n",
      "[Epoch 3/5] [Batch 465/938] [D loss: 0.5965313911437988] [G loss: 1.2235028743743896]\n",
      "[Epoch 3/5] [Batch 466/938] [D loss: 0.6252503991127014] [G loss: 0.7265878319740295]\n",
      "[Epoch 3/5] [Batch 467/938] [D loss: 0.6024669408798218] [G loss: 1.3092947006225586]\n",
      "[Epoch 3/5] [Batch 468/938] [D loss: 0.6094107627868652] [G loss: 1.0223170518875122]\n",
      "[Epoch 3/5] [Batch 469/938] [D loss: 0.6194638013839722] [G loss: 0.7243253588676453]\n",
      "[Epoch 3/5] [Batch 470/938] [D loss: 0.649036169052124] [G loss: 0.933208703994751]\n",
      "[Epoch 3/5] [Batch 471/938] [D loss: 0.5778954029083252] [G loss: 1.345253348350525]\n",
      "[Epoch 3/5] [Batch 472/938] [D loss: 0.6241301894187927] [G loss: 0.6366883516311646]\n",
      "[Epoch 3/5] [Batch 473/938] [D loss: 0.5719889998435974] [G loss: 0.9520067572593689]\n",
      "[Epoch 3/5] [Batch 474/938] [D loss: 0.6113663911819458] [G loss: 1.2096500396728516]\n",
      "[Epoch 3/5] [Batch 475/938] [D loss: 0.603441059589386] [G loss: 0.7597140073776245]\n",
      "[Epoch 3/5] [Batch 476/938] [D loss: 0.6316291093826294] [G loss: 0.94222092628479]\n",
      "[Epoch 3/5] [Batch 477/938] [D loss: 0.6021403670310974] [G loss: 0.9835827946662903]\n",
      "[Epoch 3/5] [Batch 478/938] [D loss: 0.6033911108970642] [G loss: 1.001069188117981]\n",
      "[Epoch 3/5] [Batch 479/938] [D loss: 0.5775559544563293] [G loss: 0.8049225807189941]\n",
      "[Epoch 3/5] [Batch 480/938] [D loss: 0.5924698114395142] [G loss: 0.8242402672767639]\n",
      "[Epoch 3/5] [Batch 481/938] [D loss: 0.5922183394432068] [G loss: 1.0854973793029785]\n",
      "[Epoch 3/5] [Batch 482/938] [D loss: 0.594485878944397] [G loss: 0.8140580058097839]\n",
      "[Epoch 3/5] [Batch 483/938] [D loss: 0.6019801497459412] [G loss: 1.057737946510315]\n",
      "[Epoch 3/5] [Batch 484/938] [D loss: 0.5889583826065063] [G loss: 0.9640733003616333]\n",
      "[Epoch 3/5] [Batch 485/938] [D loss: 0.5655907392501831] [G loss: 0.9425540566444397]\n",
      "[Epoch 3/5] [Batch 486/938] [D loss: 0.6064175367355347] [G loss: 1.1156578063964844]\n",
      "[Epoch 3/5] [Batch 487/938] [D loss: 0.5941896438598633] [G loss: 0.7669875025749207]\n",
      "[Epoch 3/5] [Batch 488/938] [D loss: 0.5536278486251831] [G loss: 1.024721622467041]\n",
      "[Epoch 3/5] [Batch 489/938] [D loss: 0.564757227897644] [G loss: 1.1317213773727417]\n",
      "[Epoch 3/5] [Batch 490/938] [D loss: 0.7044602632522583] [G loss: 0.5007096529006958]\n",
      "[Epoch 3/5] [Batch 491/938] [D loss: 0.6583195328712463] [G loss: 1.6133627891540527]\n",
      "[Epoch 3/5] [Batch 492/938] [D loss: 0.6740622520446777] [G loss: 0.779248833656311]\n",
      "[Epoch 3/5] [Batch 493/938] [D loss: 0.6126664280891418] [G loss: 0.8375213742256165]\n",
      "[Epoch 3/5] [Batch 494/938] [D loss: 0.6081283092498779] [G loss: 0.9136964678764343]\n",
      "[Epoch 3/5] [Batch 495/938] [D loss: 0.5663295984268188] [G loss: 1.0771071910858154]\n",
      "[Epoch 3/5] [Batch 496/938] [D loss: 0.6281619071960449] [G loss: 0.7411648631095886]\n",
      "[Epoch 3/5] [Batch 497/938] [D loss: 0.6308313012123108] [G loss: 0.8204201459884644]\n",
      "[Epoch 3/5] [Batch 498/938] [D loss: 0.6120955944061279] [G loss: 1.0049121379852295]\n",
      "[Epoch 3/5] [Batch 499/938] [D loss: 0.6032407283782959] [G loss: 0.9974609613418579]\n",
      "[Epoch 3/5] [Batch 500/938] [D loss: 0.6522195339202881] [G loss: 0.6953942179679871]\n",
      "[Epoch 3/5] [Batch 501/938] [D loss: 0.570166826248169] [G loss: 1.1432831287384033]\n",
      "[Epoch 3/5] [Batch 502/938] [D loss: 0.5944099426269531] [G loss: 1.0360751152038574]\n",
      "[Epoch 3/5] [Batch 503/938] [D loss: 0.6431561708450317] [G loss: 0.877139151096344]\n",
      "[Epoch 3/5] [Batch 504/938] [D loss: 0.5858978033065796] [G loss: 0.9989632368087769]\n",
      "[Epoch 3/5] [Batch 505/938] [D loss: 0.5422606468200684] [G loss: 0.9240131378173828]\n",
      "[Epoch 3/5] [Batch 506/938] [D loss: 0.5910325050354004] [G loss: 1.0747032165527344]\n",
      "[Epoch 3/5] [Batch 507/938] [D loss: 0.633118748664856] [G loss: 0.8808346390724182]\n",
      "[Epoch 3/5] [Batch 508/938] [D loss: 0.5667777061462402] [G loss: 0.989652693271637]\n",
      "[Epoch 3/5] [Batch 509/938] [D loss: 0.615654706954956] [G loss: 0.844852864742279]\n",
      "[Epoch 3/5] [Batch 510/938] [D loss: 0.6003715395927429] [G loss: 1.0439932346343994]\n",
      "[Epoch 3/5] [Batch 511/938] [D loss: 0.6254315972328186] [G loss: 0.7663391828536987]\n",
      "[Epoch 3/5] [Batch 512/938] [D loss: 0.6093962788581848] [G loss: 0.8489193916320801]\n",
      "[Epoch 3/5] [Batch 513/938] [D loss: 0.6285933256149292] [G loss: 1.278726577758789]\n",
      "[Epoch 3/5] [Batch 514/938] [D loss: 0.6101400852203369] [G loss: 0.6902214884757996]\n",
      "[Epoch 3/5] [Batch 515/938] [D loss: 0.5875905752182007] [G loss: 1.0636308193206787]\n",
      "[Epoch 3/5] [Batch 516/938] [D loss: 0.5692278742790222] [G loss: 1.040681004524231]\n",
      "[Epoch 3/5] [Batch 517/938] [D loss: 0.6129390597343445] [G loss: 0.8675237894058228]\n",
      "[Epoch 3/5] [Batch 518/938] [D loss: 0.6705976128578186] [G loss: 0.9091023206710815]\n",
      "[Epoch 3/5] [Batch 519/938] [D loss: 0.6346904039382935] [G loss: 1.0270551443099976]\n",
      "[Epoch 3/5] [Batch 520/938] [D loss: 0.6287365555763245] [G loss: 0.8294770121574402]\n",
      "[Epoch 3/5] [Batch 521/938] [D loss: 0.6385349035263062] [G loss: 1.056809425354004]\n",
      "[Epoch 3/5] [Batch 522/938] [D loss: 0.6118460893630981] [G loss: 0.8125584125518799]\n",
      "[Epoch 3/5] [Batch 523/938] [D loss: 0.5678628087043762] [G loss: 0.8716838359832764]\n",
      "[Epoch 3/5] [Batch 524/938] [D loss: 0.5720344185829163] [G loss: 1.1327451467514038]\n",
      "[Epoch 3/5] [Batch 525/938] [D loss: 0.5971795916557312] [G loss: 0.9471575021743774]\n",
      "[Epoch 3/5] [Batch 526/938] [D loss: 0.6461920738220215] [G loss: 0.7075147032737732]\n",
      "[Epoch 3/5] [Batch 527/938] [D loss: 0.5437546968460083] [G loss: 1.3784123659133911]\n",
      "[Epoch 3/5] [Batch 528/938] [D loss: 0.5938830971717834] [G loss: 0.8437674045562744]\n",
      "[Epoch 3/5] [Batch 529/938] [D loss: 0.5852422714233398] [G loss: 1.0623401403427124]\n",
      "[Epoch 3/5] [Batch 530/938] [D loss: 0.6385399103164673] [G loss: 0.9975225329399109]\n",
      "[Epoch 3/5] [Batch 531/938] [D loss: 0.5879226326942444] [G loss: 0.8806300163269043]\n",
      "[Epoch 3/5] [Batch 532/938] [D loss: 0.6307745575904846] [G loss: 0.8891478180885315]\n",
      "[Epoch 3/5] [Batch 533/938] [D loss: 0.6060621738433838] [G loss: 1.0152467489242554]\n",
      "[Epoch 3/5] [Batch 534/938] [D loss: 0.650814414024353] [G loss: 0.863037645816803]\n",
      "[Epoch 3/5] [Batch 535/938] [D loss: 0.6238176822662354] [G loss: 1.0210996866226196]\n",
      "[Epoch 3/5] [Batch 536/938] [D loss: 0.5838064551353455] [G loss: 1.0686085224151611]\n",
      "[Epoch 3/5] [Batch 537/938] [D loss: 0.5650064945220947] [G loss: 0.9303671717643738]\n",
      "[Epoch 3/5] [Batch 538/938] [D loss: 0.5961180925369263] [G loss: 1.0229876041412354]\n",
      "[Epoch 3/5] [Batch 539/938] [D loss: 0.6573129296302795] [G loss: 0.9179232716560364]\n",
      "[Epoch 3/5] [Batch 540/938] [D loss: 0.5940868854522705] [G loss: 1.0040881633758545]\n",
      "[Epoch 3/5] [Batch 541/938] [D loss: 0.5901261568069458] [G loss: 0.7789114117622375]\n",
      "[Epoch 3/5] [Batch 542/938] [D loss: 0.5544045567512512] [G loss: 0.8609053492546082]\n",
      "[Epoch 3/5] [Batch 543/938] [D loss: 0.5951062440872192] [G loss: 1.017026424407959]\n",
      "[Epoch 3/5] [Batch 544/938] [D loss: 0.6166825294494629] [G loss: 1.0347459316253662]\n",
      "[Epoch 3/5] [Batch 545/938] [D loss: 0.5985133647918701] [G loss: 0.8315854072570801]\n",
      "[Epoch 3/5] [Batch 546/938] [D loss: 0.5470583438873291] [G loss: 1.051944375038147]\n",
      "[Epoch 3/5] [Batch 547/938] [D loss: 0.632593035697937] [G loss: 1.1290754079818726]\n",
      "[Epoch 3/5] [Batch 548/938] [D loss: 0.5616959929466248] [G loss: 0.792991042137146]\n",
      "[Epoch 3/5] [Batch 549/938] [D loss: 0.600808322429657] [G loss: 1.0903904438018799]\n",
      "[Epoch 3/5] [Batch 550/938] [D loss: 0.572527289390564] [G loss: 0.8137918710708618]\n",
      "[Epoch 3/5] [Batch 551/938] [D loss: 0.6078202724456787] [G loss: 1.1464829444885254]\n",
      "[Epoch 3/5] [Batch 552/938] [D loss: 0.6214203238487244] [G loss: 1.0225365161895752]\n",
      "[Epoch 3/5] [Batch 553/938] [D loss: 0.6326851844787598] [G loss: 1.0211273431777954]\n",
      "[Epoch 3/5] [Batch 554/938] [D loss: 0.6022788286209106] [G loss: 0.8417589068412781]\n",
      "[Epoch 3/5] [Batch 555/938] [D loss: 0.6194989681243896] [G loss: 0.8793923258781433]\n",
      "[Epoch 3/5] [Batch 556/938] [D loss: 0.5477606058120728] [G loss: 1.018344521522522]\n",
      "[Epoch 3/5] [Batch 557/938] [D loss: 0.6501820087432861] [G loss: 1.018338680267334]\n",
      "[Epoch 3/5] [Batch 558/938] [D loss: 0.5759999752044678] [G loss: 0.654609203338623]\n",
      "[Epoch 3/5] [Batch 559/938] [D loss: 0.5788402557373047] [G loss: 0.9678753018379211]\n",
      "[Epoch 3/5] [Batch 560/938] [D loss: 0.6568496823310852] [G loss: 0.9368562698364258]\n",
      "[Epoch 3/5] [Batch 561/938] [D loss: 0.6109046339988708] [G loss: 0.9668709635734558]\n",
      "[Epoch 3/5] [Batch 562/938] [D loss: 0.5700100660324097] [G loss: 1.036351203918457]\n",
      "[Epoch 3/5] [Batch 563/938] [D loss: 0.6023386716842651] [G loss: 0.9219013452529907]\n",
      "[Epoch 3/5] [Batch 564/938] [D loss: 0.6225624084472656] [G loss: 0.8835275173187256]\n",
      "[Epoch 3/5] [Batch 565/938] [D loss: 0.6191484928131104] [G loss: 0.8856656551361084]\n",
      "[Epoch 3/5] [Batch 566/938] [D loss: 0.6227694749832153] [G loss: 1.3445335626602173]\n",
      "[Epoch 3/5] [Batch 567/938] [D loss: 0.570004403591156] [G loss: 0.8582061529159546]\n",
      "[Epoch 3/5] [Batch 568/938] [D loss: 0.6268112063407898] [G loss: 0.8349381685256958]\n",
      "[Epoch 3/5] [Batch 569/938] [D loss: 0.5500378608703613] [G loss: 1.128204107284546]\n",
      "[Epoch 3/5] [Batch 570/938] [D loss: 0.5997121930122375] [G loss: 0.7238298654556274]\n",
      "[Epoch 3/5] [Batch 571/938] [D loss: 0.6225165128707886] [G loss: 1.1390507221221924]\n",
      "[Epoch 3/5] [Batch 572/938] [D loss: 0.6121175289154053] [G loss: 0.9377943873405457]\n",
      "[Epoch 3/5] [Batch 573/938] [D loss: 0.655098557472229] [G loss: 0.8839038610458374]\n",
      "[Epoch 3/5] [Batch 574/938] [D loss: 0.5733174085617065] [G loss: 0.9874451160430908]\n",
      "[Epoch 3/5] [Batch 575/938] [D loss: 0.5876991152763367] [G loss: 0.9050213098526001]\n",
      "[Epoch 3/5] [Batch 576/938] [D loss: 0.6117169857025146] [G loss: 0.8976590037345886]\n",
      "[Epoch 3/5] [Batch 577/938] [D loss: 0.602149248123169] [G loss: 1.11033034324646]\n",
      "[Epoch 3/5] [Batch 578/938] [D loss: 0.6226153373718262] [G loss: 0.9107269644737244]\n",
      "[Epoch 3/5] [Batch 579/938] [D loss: 0.6377277374267578] [G loss: 1.0305134057998657]\n",
      "[Epoch 3/5] [Batch 580/938] [D loss: 0.635292649269104] [G loss: 0.6888605356216431]\n",
      "[Epoch 3/5] [Batch 581/938] [D loss: 0.6165149211883545] [G loss: 1.197680950164795]\n",
      "[Epoch 3/5] [Batch 582/938] [D loss: 0.6278356313705444] [G loss: 0.8764731287956238]\n",
      "[Epoch 3/5] [Batch 583/938] [D loss: 0.6043598651885986] [G loss: 0.9031338691711426]\n",
      "[Epoch 3/5] [Batch 584/938] [D loss: 0.6489963531494141] [G loss: 1.0740251541137695]\n",
      "[Epoch 3/5] [Batch 585/938] [D loss: 0.5637531280517578] [G loss: 0.8429927229881287]\n",
      "[Epoch 3/5] [Batch 586/938] [D loss: 0.5718134045600891] [G loss: 0.8786627054214478]\n",
      "[Epoch 3/5] [Batch 587/938] [D loss: 0.5813677906990051] [G loss: 1.0951440334320068]\n",
      "[Epoch 3/5] [Batch 588/938] [D loss: 0.6174996495246887] [G loss: 0.8216772675514221]\n",
      "[Epoch 3/5] [Batch 589/938] [D loss: 0.607764482498169] [G loss: 0.9301648139953613]\n",
      "[Epoch 3/5] [Batch 590/938] [D loss: 0.5779432654380798] [G loss: 0.9928491115570068]\n",
      "[Epoch 3/5] [Batch 591/938] [D loss: 0.5623112916946411] [G loss: 0.944354772567749]\n",
      "[Epoch 3/5] [Batch 592/938] [D loss: 0.6144524216651917] [G loss: 0.7532115578651428]\n",
      "[Epoch 3/5] [Batch 593/938] [D loss: 0.6161884069442749] [G loss: 1.2550280094146729]\n",
      "[Epoch 3/5] [Batch 594/938] [D loss: 0.6123473644256592] [G loss: 0.6599680781364441]\n",
      "[Epoch 3/5] [Batch 595/938] [D loss: 0.5716196298599243] [G loss: 1.1535605192184448]\n",
      "[Epoch 3/5] [Batch 596/938] [D loss: 0.6169685125350952] [G loss: 0.9521355032920837]\n",
      "[Epoch 3/5] [Batch 597/938] [D loss: 0.6125974655151367] [G loss: 0.8877100348472595]\n",
      "[Epoch 3/5] [Batch 598/938] [D loss: 0.6379252672195435] [G loss: 0.9291703104972839]\n",
      "[Epoch 3/5] [Batch 599/938] [D loss: 0.5949949622154236] [G loss: 1.0664774179458618]\n",
      "[Epoch 3/5] [Batch 600/938] [D loss: 0.5854472517967224] [G loss: 0.8791483640670776]\n",
      "[Epoch 3/5] [Batch 601/938] [D loss: 0.5895848274230957] [G loss: 1.0930192470550537]\n",
      "[Epoch 3/5] [Batch 602/938] [D loss: 0.6601729393005371] [G loss: 0.9720458984375]\n",
      "[Epoch 3/5] [Batch 603/938] [D loss: 0.6065235137939453] [G loss: 0.956946074962616]\n",
      "[Epoch 3/5] [Batch 604/938] [D loss: 0.6066819429397583] [G loss: 0.9804655313491821]\n",
      "[Epoch 3/5] [Batch 605/938] [D loss: 0.5815232992172241] [G loss: 0.9653874039649963]\n",
      "[Epoch 3/5] [Batch 606/938] [D loss: 0.6091027855873108] [G loss: 0.9222751259803772]\n",
      "[Epoch 3/5] [Batch 607/938] [D loss: 0.6332226395606995] [G loss: 0.6640341281890869]\n",
      "[Epoch 3/5] [Batch 608/938] [D loss: 0.6760221123695374] [G loss: 1.5369311571121216]\n",
      "[Epoch 3/5] [Batch 609/938] [D loss: 0.6732041239738464] [G loss: 0.6570663452148438]\n",
      "[Epoch 3/5] [Batch 610/938] [D loss: 0.5620751976966858] [G loss: 0.9540730118751526]\n",
      "[Epoch 3/5] [Batch 611/938] [D loss: 0.6012024879455566] [G loss: 0.9818214178085327]\n",
      "[Epoch 3/5] [Batch 612/938] [D loss: 0.5930333733558655] [G loss: 0.8533260822296143]\n",
      "[Epoch 3/5] [Batch 613/938] [D loss: 0.6102077960968018] [G loss: 0.9203218817710876]\n",
      "[Epoch 3/5] [Batch 614/938] [D loss: 0.5743865966796875] [G loss: 0.8978142738342285]\n",
      "[Epoch 3/5] [Batch 615/938] [D loss: 0.5757359862327576] [G loss: 0.9511551856994629]\n",
      "[Epoch 3/5] [Batch 616/938] [D loss: 0.612635612487793] [G loss: 0.8336201906204224]\n",
      "[Epoch 3/5] [Batch 617/938] [D loss: 0.6167111992835999] [G loss: 1.037461519241333]\n",
      "[Epoch 3/5] [Batch 618/938] [D loss: 0.5893748998641968] [G loss: 1.034179925918579]\n",
      "[Epoch 3/5] [Batch 619/938] [D loss: 0.6333560347557068] [G loss: 0.9193407297134399]\n",
      "[Epoch 3/5] [Batch 620/938] [D loss: 0.5751510262489319] [G loss: 1.0562492609024048]\n",
      "[Epoch 3/5] [Batch 621/938] [D loss: 0.6104156970977783] [G loss: 0.9892561435699463]\n",
      "[Epoch 3/5] [Batch 622/938] [D loss: 0.6273133754730225] [G loss: 0.7510012984275818]\n",
      "[Epoch 3/5] [Batch 623/938] [D loss: 0.6071758270263672] [G loss: 1.1298454999923706]\n",
      "[Epoch 3/5] [Batch 624/938] [D loss: 0.6550401449203491] [G loss: 0.9815795421600342]\n",
      "[Epoch 3/5] [Batch 625/938] [D loss: 0.6538224220275879] [G loss: 1.0106239318847656]\n",
      "[Epoch 3/5] [Batch 626/938] [D loss: 0.5937869548797607] [G loss: 0.7599508166313171]\n",
      "[Epoch 3/5] [Batch 627/938] [D loss: 0.6395781636238098] [G loss: 1.053430199623108]\n",
      "[Epoch 3/5] [Batch 628/938] [D loss: 0.6310684680938721] [G loss: 0.9540313482284546]\n",
      "[Epoch 3/5] [Batch 629/938] [D loss: 0.6322866678237915] [G loss: 0.833410382270813]\n",
      "[Epoch 3/5] [Batch 630/938] [D loss: 0.6082873940467834] [G loss: 0.9466794729232788]\n",
      "[Epoch 3/5] [Batch 631/938] [D loss: 0.6138426065444946] [G loss: 1.010398507118225]\n",
      "[Epoch 3/5] [Batch 632/938] [D loss: 0.6108079552650452] [G loss: 1.1342726945877075]\n",
      "[Epoch 3/5] [Batch 633/938] [D loss: 0.6283642053604126] [G loss: 0.7317852973937988]\n",
      "[Epoch 3/5] [Batch 634/938] [D loss: 0.6442604064941406] [G loss: 1.2521867752075195]\n",
      "[Epoch 3/5] [Batch 635/938] [D loss: 0.564970850944519] [G loss: 0.8254357576370239]\n",
      "[Epoch 3/5] [Batch 636/938] [D loss: 0.6236236095428467] [G loss: 0.8469706773757935]\n",
      "[Epoch 3/5] [Batch 637/938] [D loss: 0.5365943312644958] [G loss: 1.2039114236831665]\n",
      "[Epoch 3/5] [Batch 638/938] [D loss: 0.5871527194976807] [G loss: 0.8366209268569946]\n",
      "[Epoch 3/5] [Batch 639/938] [D loss: 0.6319502592086792] [G loss: 1.0314617156982422]\n",
      "[Epoch 3/5] [Batch 640/938] [D loss: 0.6432358622550964] [G loss: 1.1094361543655396]\n",
      "[Epoch 3/5] [Batch 641/938] [D loss: 0.5886756181716919] [G loss: 0.671801745891571]\n",
      "[Epoch 3/5] [Batch 642/938] [D loss: 0.6277573108673096] [G loss: 1.0237061977386475]\n",
      "[Epoch 3/5] [Batch 643/938] [D loss: 0.5879431366920471] [G loss: 1.102860450744629]\n",
      "[Epoch 3/5] [Batch 644/938] [D loss: 0.6036525964736938] [G loss: 0.6945680975914001]\n",
      "[Epoch 3/5] [Batch 645/938] [D loss: 0.6903907060623169] [G loss: 1.4192664623260498]\n",
      "[Epoch 3/5] [Batch 646/938] [D loss: 0.6891185641288757] [G loss: 0.723770022392273]\n",
      "[Epoch 3/5] [Batch 647/938] [D loss: 0.5913338661193848] [G loss: 0.8990687131881714]\n",
      "[Epoch 3/5] [Batch 648/938] [D loss: 0.6284937858581543] [G loss: 1.0171723365783691]\n",
      "[Epoch 3/5] [Batch 649/938] [D loss: 0.6704871654510498] [G loss: 0.870495080947876]\n",
      "[Epoch 3/5] [Batch 650/938] [D loss: 0.634251594543457] [G loss: 0.9591574668884277]\n",
      "[Epoch 3/5] [Batch 651/938] [D loss: 0.5603951811790466] [G loss: 1.0279568433761597]\n",
      "[Epoch 3/5] [Batch 652/938] [D loss: 0.6104497909545898] [G loss: 0.8794952630996704]\n",
      "[Epoch 3/5] [Batch 653/938] [D loss: 0.6374087333679199] [G loss: 0.9173989295959473]\n",
      "[Epoch 3/5] [Batch 654/938] [D loss: 0.6743298768997192] [G loss: 1.0195516347885132]\n",
      "[Epoch 3/5] [Batch 655/938] [D loss: 0.6222090125083923] [G loss: 0.6820263266563416]\n",
      "[Epoch 3/5] [Batch 656/938] [D loss: 0.6365726590156555] [G loss: 1.098694920539856]\n",
      "[Epoch 3/5] [Batch 657/938] [D loss: 0.6134868860244751] [G loss: 0.8234210014343262]\n",
      "[Epoch 3/5] [Batch 658/938] [D loss: 0.6093628406524658] [G loss: 0.7792350053787231]\n",
      "[Epoch 3/5] [Batch 659/938] [D loss: 0.6311155557632446] [G loss: 1.0735273361206055]\n",
      "[Epoch 3/5] [Batch 660/938] [D loss: 0.6120558977127075] [G loss: 0.943155825138092]\n",
      "[Epoch 3/5] [Batch 661/938] [D loss: 0.604759156703949] [G loss: 0.876928985118866]\n",
      "[Epoch 3/5] [Batch 662/938] [D loss: 0.599379301071167] [G loss: 0.928536593914032]\n",
      "[Epoch 3/5] [Batch 663/938] [D loss: 0.6062629818916321] [G loss: 1.0801202058792114]\n",
      "[Epoch 3/5] [Batch 664/938] [D loss: 0.6121144890785217] [G loss: 0.7461903095245361]\n",
      "[Epoch 3/5] [Batch 665/938] [D loss: 0.642786979675293] [G loss: 0.8900376558303833]\n",
      "[Epoch 3/5] [Batch 666/938] [D loss: 0.5754088163375854] [G loss: 1.0000919103622437]\n",
      "[Epoch 3/5] [Batch 667/938] [D loss: 0.5817912817001343] [G loss: 0.9027291536331177]\n",
      "[Epoch 3/5] [Batch 668/938] [D loss: 0.5381355285644531] [G loss: 0.8436498045921326]\n",
      "[Epoch 3/5] [Batch 669/938] [D loss: 0.6321237087249756] [G loss: 1.1065394878387451]\n",
      "[Epoch 3/5] [Batch 670/938] [D loss: 0.650757372379303] [G loss: 0.9434792399406433]\n",
      "[Epoch 3/5] [Batch 671/938] [D loss: 0.5858452320098877] [G loss: 0.9405858516693115]\n",
      "[Epoch 3/5] [Batch 672/938] [D loss: 0.628646969795227] [G loss: 0.9509455561637878]\n",
      "[Epoch 3/5] [Batch 673/938] [D loss: 0.5825738906860352] [G loss: 0.8523736000061035]\n",
      "[Epoch 3/5] [Batch 674/938] [D loss: 0.6565994620323181] [G loss: 1.093027114868164]\n",
      "[Epoch 3/5] [Batch 675/938] [D loss: 0.6171311140060425] [G loss: 0.9917524456977844]\n",
      "[Epoch 3/5] [Batch 676/938] [D loss: 0.6308534145355225] [G loss: 0.6668404340744019]\n",
      "[Epoch 3/5] [Batch 677/938] [D loss: 0.6317943334579468] [G loss: 1.0418777465820312]\n",
      "[Epoch 3/5] [Batch 678/938] [D loss: 0.650585412979126] [G loss: 0.7804491519927979]\n",
      "[Epoch 3/5] [Batch 679/938] [D loss: 0.5928328037261963] [G loss: 1.0424156188964844]\n",
      "[Epoch 3/5] [Batch 680/938] [D loss: 0.5965202450752258] [G loss: 0.8187158703804016]\n",
      "[Epoch 3/5] [Batch 681/938] [D loss: 0.6578153371810913] [G loss: 0.9229476451873779]\n",
      "[Epoch 3/5] [Batch 682/938] [D loss: 0.6015430688858032] [G loss: 0.9056167602539062]\n",
      "[Epoch 3/5] [Batch 683/938] [D loss: 0.6239351034164429] [G loss: 0.9080647230148315]\n",
      "[Epoch 3/5] [Batch 684/938] [D loss: 0.5945744514465332] [G loss: 1.0845134258270264]\n",
      "[Epoch 3/5] [Batch 685/938] [D loss: 0.6213171482086182] [G loss: 0.820410430431366]\n",
      "[Epoch 3/5] [Batch 686/938] [D loss: 0.6472831964492798] [G loss: 0.7468053698539734]\n",
      "[Epoch 3/5] [Batch 687/938] [D loss: 0.5662326812744141] [G loss: 1.044809341430664]\n",
      "[Epoch 3/5] [Batch 688/938] [D loss: 0.5834505558013916] [G loss: 0.9658020734786987]\n",
      "[Epoch 3/5] [Batch 689/938] [D loss: 0.5676536560058594] [G loss: 1.0778065919876099]\n",
      "[Epoch 3/5] [Batch 690/938] [D loss: 0.6267139315605164] [G loss: 1.1438348293304443]\n",
      "[Epoch 3/5] [Batch 691/938] [D loss: 0.5970029234886169] [G loss: 0.7896826863288879]\n",
      "[Epoch 3/5] [Batch 692/938] [D loss: 0.5779989957809448] [G loss: 1.107229471206665]\n",
      "[Epoch 3/5] [Batch 693/938] [D loss: 0.5982818603515625] [G loss: 0.9789819717407227]\n",
      "[Epoch 3/5] [Batch 694/938] [D loss: 0.6407520771026611] [G loss: 0.9863625764846802]\n",
      "[Epoch 3/5] [Batch 695/938] [D loss: 0.59516441822052] [G loss: 0.9604219198226929]\n",
      "[Epoch 3/5] [Batch 696/938] [D loss: 0.6531169414520264] [G loss: 0.909826397895813]\n",
      "[Epoch 3/5] [Batch 697/938] [D loss: 0.6343613862991333] [G loss: 0.8759311437606812]\n",
      "[Epoch 3/5] [Batch 698/938] [D loss: 0.657194197177887] [G loss: 1.2414981126785278]\n",
      "[Epoch 3/5] [Batch 699/938] [D loss: 0.6260629296302795] [G loss: 0.6086060404777527]\n",
      "[Epoch 3/5] [Batch 700/938] [D loss: 0.5703737735748291] [G loss: 1.0371904373168945]\n",
      "[Epoch 3/5] [Batch 701/938] [D loss: 0.5838358402252197] [G loss: 0.9738146066665649]\n",
      "[Epoch 3/5] [Batch 702/938] [D loss: 0.5942047834396362] [G loss: 0.7346174716949463]\n",
      "[Epoch 3/5] [Batch 703/938] [D loss: 0.609371542930603] [G loss: 1.0052696466445923]\n",
      "[Epoch 3/5] [Batch 704/938] [D loss: 0.5739436149597168] [G loss: 0.8799853920936584]\n",
      "[Epoch 3/5] [Batch 705/938] [D loss: 0.5951641798019409] [G loss: 0.9135716557502747]\n",
      "[Epoch 3/5] [Batch 706/938] [D loss: 0.6442142724990845] [G loss: 0.9098906517028809]\n",
      "[Epoch 3/5] [Batch 707/938] [D loss: 0.6257238388061523] [G loss: 0.8548534512519836]\n",
      "[Epoch 3/5] [Batch 708/938] [D loss: 0.5670733451843262] [G loss: 0.8339692950248718]\n",
      "[Epoch 3/5] [Batch 709/938] [D loss: 0.6226720213890076] [G loss: 0.9299864172935486]\n",
      "[Epoch 3/5] [Batch 710/938] [D loss: 0.5754625797271729] [G loss: 0.9924712181091309]\n",
      "[Epoch 3/5] [Batch 711/938] [D loss: 0.601363480091095] [G loss: 0.8901273012161255]\n",
      "[Epoch 3/5] [Batch 712/938] [D loss: 0.5923053026199341] [G loss: 1.151525855064392]\n",
      "[Epoch 3/5] [Batch 713/938] [D loss: 0.6378054022789001] [G loss: 0.8992737531661987]\n",
      "[Epoch 3/5] [Batch 714/938] [D loss: 0.6124678254127502] [G loss: 0.9163774251937866]\n",
      "[Epoch 3/5] [Batch 715/938] [D loss: 0.7057626843452454] [G loss: 1.1603305339813232]\n",
      "[Epoch 3/5] [Batch 716/938] [D loss: 0.5702865123748779] [G loss: 0.9679896831512451]\n",
      "[Epoch 3/5] [Batch 717/938] [D loss: 0.5684038400650024] [G loss: 1.017890453338623]\n",
      "[Epoch 3/5] [Batch 718/938] [D loss: 0.6270842552185059] [G loss: 0.8674395084381104]\n",
      "[Epoch 3/5] [Batch 719/938] [D loss: 0.5810271501541138] [G loss: 0.9494097232818604]\n",
      "[Epoch 3/5] [Batch 720/938] [D loss: 0.67896568775177] [G loss: 1.1108074188232422]\n",
      "[Epoch 3/5] [Batch 721/938] [D loss: 0.6496370434761047] [G loss: 0.737080991268158]\n",
      "[Epoch 3/5] [Batch 722/938] [D loss: 0.6489757895469666] [G loss: 1.068610668182373]\n",
      "[Epoch 3/5] [Batch 723/938] [D loss: 0.6091562509536743] [G loss: 0.9985890984535217]\n",
      "[Epoch 3/5] [Batch 724/938] [D loss: 0.650676965713501] [G loss: 0.8346422910690308]\n",
      "[Epoch 3/5] [Batch 725/938] [D loss: 0.6159530878067017] [G loss: 1.0374794006347656]\n",
      "[Epoch 3/5] [Batch 726/938] [D loss: 0.6276637315750122] [G loss: 0.7653197050094604]\n",
      "[Epoch 3/5] [Batch 727/938] [D loss: 0.6164580583572388] [G loss: 0.9742754101753235]\n",
      "[Epoch 3/5] [Batch 728/938] [D loss: 0.6060129404067993] [G loss: 0.9148975610733032]\n",
      "[Epoch 3/5] [Batch 729/938] [D loss: 0.5907598733901978] [G loss: 0.76844322681427]\n",
      "[Epoch 3/5] [Batch 730/938] [D loss: 0.6098219752311707] [G loss: 1.0785480737686157]\n",
      "[Epoch 3/5] [Batch 731/938] [D loss: 0.6026090383529663] [G loss: 0.8855943083763123]\n",
      "[Epoch 3/5] [Batch 732/938] [D loss: 0.6104815006256104] [G loss: 0.9230135083198547]\n",
      "[Epoch 3/5] [Batch 733/938] [D loss: 0.6165006160736084] [G loss: 0.9502280354499817]\n",
      "[Epoch 3/5] [Batch 734/938] [D loss: 0.6029078960418701] [G loss: 0.9287383556365967]\n",
      "[Epoch 3/5] [Batch 735/938] [D loss: 0.6598832607269287] [G loss: 0.8396004438400269]\n",
      "[Epoch 3/5] [Batch 736/938] [D loss: 0.6034010648727417] [G loss: 1.0072791576385498]\n",
      "[Epoch 3/5] [Batch 737/938] [D loss: 0.5894417762756348] [G loss: 0.9506163597106934]\n",
      "[Epoch 3/5] [Batch 738/938] [D loss: 0.5927139520645142] [G loss: 0.9538412094116211]\n",
      "[Epoch 3/5] [Batch 739/938] [D loss: 0.5842815041542053] [G loss: 0.8423871397972107]\n",
      "[Epoch 3/5] [Batch 740/938] [D loss: 0.5308870077133179] [G loss: 1.0324561595916748]\n",
      "[Epoch 3/5] [Batch 741/938] [D loss: 0.5651060938835144] [G loss: 1.0256009101867676]\n",
      "[Epoch 3/5] [Batch 742/938] [D loss: 0.6283524036407471] [G loss: 0.59853196144104]\n",
      "[Epoch 3/5] [Batch 743/938] [D loss: 0.5833343267440796] [G loss: 1.2882180213928223]\n",
      "[Epoch 3/5] [Batch 744/938] [D loss: 0.6497541666030884] [G loss: 0.8614650964736938]\n",
      "[Epoch 3/5] [Batch 745/938] [D loss: 0.6366534233093262] [G loss: 0.746368408203125]\n",
      "[Epoch 3/5] [Batch 746/938] [D loss: 0.6386400461196899] [G loss: 1.212515115737915]\n",
      "[Epoch 3/5] [Batch 747/938] [D loss: 0.5720692276954651] [G loss: 0.9003888368606567]\n",
      "[Epoch 3/5] [Batch 748/938] [D loss: 0.5840891599655151] [G loss: 0.7630808353424072]\n",
      "[Epoch 3/5] [Batch 749/938] [D loss: 0.5906746983528137] [G loss: 0.9435473680496216]\n",
      "[Epoch 3/5] [Batch 750/938] [D loss: 0.6245059370994568] [G loss: 1.09657883644104]\n",
      "[Epoch 3/5] [Batch 751/938] [D loss: 0.6477263569831848] [G loss: 0.7403936386108398]\n",
      "[Epoch 3/5] [Batch 752/938] [D loss: 0.6308446526527405] [G loss: 1.0682191848754883]\n",
      "[Epoch 3/5] [Batch 753/938] [D loss: 0.599307656288147] [G loss: 0.8566392064094543]\n",
      "[Epoch 3/5] [Batch 754/938] [D loss: 0.5749505758285522] [G loss: 0.9237144589424133]\n",
      "[Epoch 3/5] [Batch 755/938] [D loss: 0.5525276064872742] [G loss: 0.9518038630485535]\n",
      "[Epoch 3/5] [Batch 756/938] [D loss: 0.6481389999389648] [G loss: 0.9141114950180054]\n",
      "[Epoch 3/5] [Batch 757/938] [D loss: 0.6038268208503723] [G loss: 0.8194025754928589]\n",
      "[Epoch 3/5] [Batch 758/938] [D loss: 0.5867873430252075] [G loss: 1.14801025390625]\n",
      "[Epoch 3/5] [Batch 759/938] [D loss: 0.6358712315559387] [G loss: 0.886019766330719]\n",
      "[Epoch 3/5] [Batch 760/938] [D loss: 0.5632537007331848] [G loss: 0.9702974557876587]\n",
      "[Epoch 3/5] [Batch 761/938] [D loss: 0.6359924077987671] [G loss: 0.9211254119873047]\n",
      "[Epoch 3/5] [Batch 762/938] [D loss: 0.6442919969558716] [G loss: 0.9019051790237427]\n",
      "[Epoch 3/5] [Batch 763/938] [D loss: 0.6802195906639099] [G loss: 0.767785370349884]\n",
      "[Epoch 3/5] [Batch 764/938] [D loss: 0.6480680704116821] [G loss: 1.1786069869995117]\n",
      "[Epoch 3/5] [Batch 765/938] [D loss: 0.6155368089675903] [G loss: 0.7840959429740906]\n",
      "[Epoch 3/5] [Batch 766/938] [D loss: 0.5863351821899414] [G loss: 1.1422979831695557]\n",
      "[Epoch 3/5] [Batch 767/938] [D loss: 0.6247343420982361] [G loss: 1.1154444217681885]\n",
      "[Epoch 3/5] [Batch 768/938] [D loss: 0.5848382115364075] [G loss: 0.7760446071624756]\n",
      "[Epoch 3/5] [Batch 769/938] [D loss: 0.5858651399612427] [G loss: 0.8159129619598389]\n",
      "[Epoch 3/5] [Batch 770/938] [D loss: 0.625822126865387] [G loss: 1.232799768447876]\n",
      "[Epoch 3/5] [Batch 771/938] [D loss: 0.6267355680465698] [G loss: 0.8127254247665405]\n",
      "[Epoch 3/5] [Batch 772/938] [D loss: 0.6006746292114258] [G loss: 0.9496820569038391]\n",
      "[Epoch 3/5] [Batch 773/938] [D loss: 0.6197181940078735] [G loss: 0.9598740339279175]\n",
      "[Epoch 3/5] [Batch 774/938] [D loss: 0.5961909294128418] [G loss: 0.7987827658653259]\n",
      "[Epoch 3/5] [Batch 775/938] [D loss: 0.5881426930427551] [G loss: 0.9284036159515381]\n",
      "[Epoch 3/5] [Batch 776/938] [D loss: 0.6280491352081299] [G loss: 1.0075178146362305]\n",
      "[Epoch 3/5] [Batch 777/938] [D loss: 0.6468477249145508] [G loss: 0.911194384098053]\n",
      "[Epoch 3/5] [Batch 778/938] [D loss: 0.5636672973632812] [G loss: 0.9597154259681702]\n",
      "[Epoch 3/5] [Batch 779/938] [D loss: 0.6161800026893616] [G loss: 0.8077118396759033]\n",
      "[Epoch 3/5] [Batch 780/938] [D loss: 0.6042653918266296] [G loss: 0.9095178842544556]\n",
      "[Epoch 3/5] [Batch 781/938] [D loss: 0.6365970373153687] [G loss: 1.0422405004501343]\n",
      "[Epoch 3/5] [Batch 782/938] [D loss: 0.626395046710968] [G loss: 0.6368976831436157]\n",
      "[Epoch 3/5] [Batch 783/938] [D loss: 0.6009542942047119] [G loss: 1.2054166793823242]\n",
      "[Epoch 3/5] [Batch 784/938] [D loss: 0.6166068315505981] [G loss: 0.9134194850921631]\n",
      "[Epoch 3/5] [Batch 785/938] [D loss: 0.6475018262863159] [G loss: 0.8236190676689148]\n",
      "[Epoch 3/5] [Batch 786/938] [D loss: 0.6089597940444946] [G loss: 1.2083113193511963]\n",
      "[Epoch 3/5] [Batch 787/938] [D loss: 0.5964359045028687] [G loss: 0.7403609752655029]\n",
      "[Epoch 3/5] [Batch 788/938] [D loss: 0.5958975553512573] [G loss: 0.8433613181114197]\n",
      "[Epoch 3/5] [Batch 789/938] [D loss: 0.6593751907348633] [G loss: 1.0433142185211182]\n",
      "[Epoch 3/5] [Batch 790/938] [D loss: 0.6562111973762512] [G loss: 0.9671280384063721]\n",
      "[Epoch 3/5] [Batch 791/938] [D loss: 0.6197469234466553] [G loss: 0.8083721995353699]\n",
      "[Epoch 3/5] [Batch 792/938] [D loss: 0.6032264828681946] [G loss: 0.831351637840271]\n",
      "[Epoch 3/5] [Batch 793/938] [D loss: 0.5748224258422852] [G loss: 0.9740829467773438]\n",
      "[Epoch 3/5] [Batch 794/938] [D loss: 0.5743790864944458] [G loss: 1.0275206565856934]\n",
      "[Epoch 3/5] [Batch 795/938] [D loss: 0.6026318669319153] [G loss: 0.8264252543449402]\n",
      "[Epoch 3/5] [Batch 796/938] [D loss: 0.5828622579574585] [G loss: 1.0188298225402832]\n",
      "[Epoch 3/5] [Batch 797/938] [D loss: 0.5788030624389648] [G loss: 0.9669214487075806]\n",
      "[Epoch 3/5] [Batch 798/938] [D loss: 0.6208449602127075] [G loss: 0.8921700716018677]\n",
      "[Epoch 3/5] [Batch 799/938] [D loss: 0.599088191986084] [G loss: 1.141158103942871]\n",
      "[Epoch 3/5] [Batch 800/938] [D loss: 0.6584053039550781] [G loss: 0.8649455308914185]\n",
      "[Epoch 3/5] [Batch 801/938] [D loss: 0.5980446338653564] [G loss: 0.7792669534683228]\n",
      "[Epoch 3/5] [Batch 802/938] [D loss: 0.6341214179992676] [G loss: 1.1484298706054688]\n",
      "[Epoch 3/5] [Batch 803/938] [D loss: 0.5369302034378052] [G loss: 0.8146162033081055]\n",
      "[Epoch 3/5] [Batch 804/938] [D loss: 0.6028069257736206] [G loss: 0.807033896446228]\n",
      "[Epoch 3/5] [Batch 805/938] [D loss: 0.5637032985687256] [G loss: 1.013958215713501]\n",
      "[Epoch 3/5] [Batch 806/938] [D loss: 0.6065540313720703] [G loss: 0.8118306994438171]\n",
      "[Epoch 3/5] [Batch 807/938] [D loss: 0.5959998369216919] [G loss: 1.103771448135376]\n",
      "[Epoch 3/5] [Batch 808/938] [D loss: 0.6070773601531982] [G loss: 0.8767727613449097]\n",
      "[Epoch 3/5] [Batch 809/938] [D loss: 0.6342662572860718] [G loss: 0.9715090394020081]\n",
      "[Epoch 3/5] [Batch 810/938] [D loss: 0.6127432584762573] [G loss: 1.0113179683685303]\n",
      "[Epoch 3/5] [Batch 811/938] [D loss: 0.5981847047805786] [G loss: 0.6829599142074585]\n",
      "[Epoch 3/5] [Batch 812/938] [D loss: 0.5796879529953003] [G loss: 1.1296823024749756]\n",
      "[Epoch 3/5] [Batch 813/938] [D loss: 0.6592180728912354] [G loss: 0.995368242263794]\n",
      "[Epoch 3/5] [Batch 814/938] [D loss: 0.644789457321167] [G loss: 0.618652880191803]\n",
      "[Epoch 3/5] [Batch 815/938] [D loss: 0.675550639629364] [G loss: 1.2742085456848145]\n",
      "[Epoch 3/5] [Batch 816/938] [D loss: 0.6448403596878052] [G loss: 0.7201346755027771]\n",
      "[Epoch 3/5] [Batch 817/938] [D loss: 0.5965486168861389] [G loss: 0.886656641960144]\n",
      "[Epoch 3/5] [Batch 818/938] [D loss: 0.5473389029502869] [G loss: 0.9985141754150391]\n",
      "[Epoch 3/5] [Batch 819/938] [D loss: 0.5945636034011841] [G loss: 0.9634301662445068]\n",
      "[Epoch 3/5] [Batch 820/938] [D loss: 0.6063545942306519] [G loss: 0.9926745891571045]\n",
      "[Epoch 3/5] [Batch 821/938] [D loss: 0.6047225594520569] [G loss: 1.012453556060791]\n",
      "[Epoch 3/5] [Batch 822/938] [D loss: 0.5737168788909912] [G loss: 0.8665529489517212]\n",
      "[Epoch 3/5] [Batch 823/938] [D loss: 0.5885787010192871] [G loss: 1.1435980796813965]\n",
      "[Epoch 3/5] [Batch 824/938] [D loss: 0.6623916625976562] [G loss: 0.8359673023223877]\n",
      "[Epoch 3/5] [Batch 825/938] [D loss: 0.6007527112960815] [G loss: 1.0573385953903198]\n",
      "[Epoch 3/5] [Batch 826/938] [D loss: 0.6123486757278442] [G loss: 1.0030642747879028]\n",
      "[Epoch 3/5] [Batch 827/938] [D loss: 0.6452595591545105] [G loss: 0.8517040014266968]\n",
      "[Epoch 3/5] [Batch 828/938] [D loss: 0.6137270927429199] [G loss: 0.9512528777122498]\n",
      "[Epoch 3/5] [Batch 829/938] [D loss: 0.5926234722137451] [G loss: 0.9775190949440002]\n",
      "[Epoch 3/5] [Batch 830/938] [D loss: 0.6155928373336792] [G loss: 0.9666043519973755]\n",
      "[Epoch 3/5] [Batch 831/938] [D loss: 0.5737848281860352] [G loss: 0.979322075843811]\n",
      "[Epoch 3/5] [Batch 832/938] [D loss: 0.5957621932029724] [G loss: 0.831666886806488]\n",
      "[Epoch 3/5] [Batch 833/938] [D loss: 0.5927006006240845] [G loss: 1.2861506938934326]\n",
      "[Epoch 3/5] [Batch 834/938] [D loss: 0.6553514003753662] [G loss: 0.81048583984375]\n",
      "[Epoch 3/5] [Batch 835/938] [D loss: 0.5596288442611694] [G loss: 0.9933233261108398]\n",
      "[Epoch 3/5] [Batch 836/938] [D loss: 0.5658688545227051] [G loss: 0.8952732682228088]\n",
      "[Epoch 3/5] [Batch 837/938] [D loss: 0.6243618726730347] [G loss: 0.9283891916275024]\n",
      "[Epoch 3/5] [Batch 838/938] [D loss: 0.5926042795181274] [G loss: 1.0845057964324951]\n",
      "[Epoch 3/5] [Batch 839/938] [D loss: 0.6509114503860474] [G loss: 1.0362181663513184]\n",
      "[Epoch 3/5] [Batch 840/938] [D loss: 0.6486502885818481] [G loss: 0.7263510227203369]\n",
      "[Epoch 3/5] [Batch 841/938] [D loss: 0.6135579347610474] [G loss: 1.3846501111984253]\n",
      "[Epoch 3/5] [Batch 842/938] [D loss: 0.6326372623443604] [G loss: 0.7682721614837646]\n",
      "[Epoch 3/5] [Batch 843/938] [D loss: 0.6084253787994385] [G loss: 0.9423686265945435]\n",
      "[Epoch 3/5] [Batch 844/938] [D loss: 0.5447975397109985] [G loss: 0.9827920794487]\n",
      "[Epoch 3/5] [Batch 845/938] [D loss: 0.5746423006057739] [G loss: 0.8374781608581543]\n",
      "[Epoch 3/5] [Batch 846/938] [D loss: 0.5982676148414612] [G loss: 0.8876462578773499]\n",
      "[Epoch 3/5] [Batch 847/938] [D loss: 0.6379256248474121] [G loss: 1.5518512725830078]\n",
      "[Epoch 3/5] [Batch 848/938] [D loss: 0.6140969395637512] [G loss: 0.6631974577903748]\n",
      "[Epoch 3/5] [Batch 849/938] [D loss: 0.6736668944358826] [G loss: 1.0449542999267578]\n",
      "[Epoch 3/5] [Batch 850/938] [D loss: 0.5963011980056763] [G loss: 0.9469082355499268]\n",
      "[Epoch 3/5] [Batch 851/938] [D loss: 0.5600350499153137] [G loss: 0.9131710529327393]\n",
      "[Epoch 3/5] [Batch 852/938] [D loss: 0.6008309125900269] [G loss: 0.9106191396713257]\n",
      "[Epoch 3/5] [Batch 853/938] [D loss: 0.5756002068519592] [G loss: 1.0844193696975708]\n",
      "[Epoch 3/5] [Batch 854/938] [D loss: 0.6537964344024658] [G loss: 0.7059979438781738]\n",
      "[Epoch 3/5] [Batch 855/938] [D loss: 0.6112569570541382] [G loss: 0.9131079912185669]\n",
      "[Epoch 3/5] [Batch 856/938] [D loss: 0.6268301010131836] [G loss: 0.9376342296600342]\n",
      "[Epoch 3/5] [Batch 857/938] [D loss: 0.5940257906913757] [G loss: 0.9449050426483154]\n",
      "[Epoch 3/5] [Batch 858/938] [D loss: 0.5985507965087891] [G loss: 0.7247969508171082]\n",
      "[Epoch 3/5] [Batch 859/938] [D loss: 0.6126267313957214] [G loss: 1.060915231704712]\n",
      "[Epoch 3/5] [Batch 860/938] [D loss: 0.6423271298408508] [G loss: 0.7840438485145569]\n",
      "[Epoch 3/5] [Batch 861/938] [D loss: 0.6120138168334961] [G loss: 1.1245511770248413]\n",
      "[Epoch 3/5] [Batch 862/938] [D loss: 0.5569331645965576] [G loss: 1.0070751905441284]\n",
      "[Epoch 3/5] [Batch 863/938] [D loss: 0.6270561814308167] [G loss: 0.9215037226676941]\n",
      "[Epoch 3/5] [Batch 864/938] [D loss: 0.6902430653572083] [G loss: 0.7223620414733887]\n",
      "[Epoch 3/5] [Batch 865/938] [D loss: 0.6176578998565674] [G loss: 0.8815919160842896]\n",
      "[Epoch 3/5] [Batch 866/938] [D loss: 0.6176766157150269] [G loss: 1.1641714572906494]\n",
      "[Epoch 3/5] [Batch 867/938] [D loss: 0.6013463139533997] [G loss: 0.8569071292877197]\n",
      "[Epoch 3/5] [Batch 868/938] [D loss: 0.6339129209518433] [G loss: 0.9586564898490906]\n",
      "[Epoch 3/5] [Batch 869/938] [D loss: 0.6385665535926819] [G loss: 1.0296292304992676]\n",
      "[Epoch 3/5] [Batch 870/938] [D loss: 0.61358642578125] [G loss: 0.9369999170303345]\n",
      "[Epoch 3/5] [Batch 871/938] [D loss: 0.6613233089447021] [G loss: 0.9531834721565247]\n",
      "[Epoch 3/5] [Batch 872/938] [D loss: 0.5910670757293701] [G loss: 0.9086155891418457]\n",
      "[Epoch 3/5] [Batch 873/938] [D loss: 0.605851411819458] [G loss: 0.9050202965736389]\n",
      "[Epoch 3/5] [Batch 874/938] [D loss: 0.647838830947876] [G loss: 0.9023308157920837]\n",
      "[Epoch 3/5] [Batch 875/938] [D loss: 0.6153564453125] [G loss: 0.9317863583564758]\n",
      "[Epoch 3/5] [Batch 876/938] [D loss: 0.6541804075241089] [G loss: 0.9607778787612915]\n",
      "[Epoch 3/5] [Batch 877/938] [D loss: 0.5853350162506104] [G loss: 0.9280118942260742]\n",
      "[Epoch 3/5] [Batch 878/938] [D loss: 0.6228418946266174] [G loss: 0.9032022356987]\n",
      "[Epoch 3/5] [Batch 879/938] [D loss: 0.6460336446762085] [G loss: 0.8843056559562683]\n",
      "[Epoch 3/5] [Batch 880/938] [D loss: 0.5788382887840271] [G loss: 0.8793135285377502]\n",
      "[Epoch 3/5] [Batch 881/938] [D loss: 0.6013280153274536] [G loss: 0.8673194050788879]\n",
      "[Epoch 3/5] [Batch 882/938] [D loss: 0.5487735271453857] [G loss: 0.9595320224761963]\n",
      "[Epoch 3/5] [Batch 883/938] [D loss: 0.6015400290489197] [G loss: 0.9173769354820251]\n",
      "[Epoch 3/5] [Batch 884/938] [D loss: 0.5928183794021606] [G loss: 1.1396698951721191]\n",
      "[Epoch 3/5] [Batch 885/938] [D loss: 0.5848419070243835] [G loss: 0.8441438674926758]\n",
      "[Epoch 3/5] [Batch 886/938] [D loss: 0.6620839834213257] [G loss: 1.026693344116211]\n",
      "[Epoch 3/5] [Batch 887/938] [D loss: 0.6094217896461487] [G loss: 0.7784475684165955]\n",
      "[Epoch 3/5] [Batch 888/938] [D loss: 0.6111797094345093] [G loss: 1.012726902961731]\n",
      "[Epoch 3/5] [Batch 889/938] [D loss: 0.5877418518066406] [G loss: 1.143890142440796]\n",
      "[Epoch 3/5] [Batch 890/938] [D loss: 0.6417554020881653] [G loss: 0.7163181304931641]\n",
      "[Epoch 3/5] [Batch 891/938] [D loss: 0.6264362335205078] [G loss: 1.2899037599563599]\n",
      "[Epoch 3/5] [Batch 892/938] [D loss: 0.599795401096344] [G loss: 0.69959956407547]\n",
      "[Epoch 3/5] [Batch 893/938] [D loss: 0.5863698720932007] [G loss: 0.9424922466278076]\n",
      "[Epoch 3/5] [Batch 894/938] [D loss: 0.6165236830711365] [G loss: 1.143995761871338]\n",
      "[Epoch 3/5] [Batch 895/938] [D loss: 0.6098601222038269] [G loss: 0.7807512283325195]\n",
      "[Epoch 3/5] [Batch 896/938] [D loss: 0.646664023399353] [G loss: 1.0277262926101685]\n",
      "[Epoch 3/5] [Batch 897/938] [D loss: 0.6209639310836792] [G loss: 0.9062814116477966]\n",
      "[Epoch 3/5] [Batch 898/938] [D loss: 0.6049078702926636] [G loss: 0.8079633712768555]\n",
      "[Epoch 3/5] [Batch 899/938] [D loss: 0.5680692791938782] [G loss: 1.1434357166290283]\n",
      "[Epoch 3/5] [Batch 900/938] [D loss: 0.630311906337738] [G loss: 0.8769826292991638]\n",
      "[Epoch 3/5] [Batch 901/938] [D loss: 0.6381393671035767] [G loss: 1.1015983819961548]\n",
      "[Epoch 3/5] [Batch 902/938] [D loss: 0.6026461124420166] [G loss: 0.8717592358589172]\n",
      "[Epoch 3/5] [Batch 903/938] [D loss: 0.6699084639549255] [G loss: 0.8673008680343628]\n",
      "[Epoch 3/5] [Batch 904/938] [D loss: 0.6539430022239685] [G loss: 0.8114606142044067]\n",
      "[Epoch 3/5] [Batch 905/938] [D loss: 0.617368757724762] [G loss: 0.9754823446273804]\n",
      "[Epoch 3/5] [Batch 906/938] [D loss: 0.5897765159606934] [G loss: 0.7814779877662659]\n",
      "[Epoch 3/5] [Batch 907/938] [D loss: 0.5824886560440063] [G loss: 0.8695859313011169]\n",
      "[Epoch 3/5] [Batch 908/938] [D loss: 0.6214016675949097] [G loss: 1.2219332456588745]\n",
      "[Epoch 3/5] [Batch 909/938] [D loss: 0.6148468852043152] [G loss: 0.8176632523536682]\n",
      "[Epoch 3/5] [Batch 910/938] [D loss: 0.6029990911483765] [G loss: 1.073988437652588]\n",
      "[Epoch 3/5] [Batch 911/938] [D loss: 0.6869180202484131] [G loss: 1.121807336807251]\n",
      "[Epoch 3/5] [Batch 912/938] [D loss: 0.6091858744621277] [G loss: 0.6736589074134827]\n",
      "[Epoch 3/5] [Batch 913/938] [D loss: 0.6181831359863281] [G loss: 0.731650173664093]\n",
      "[Epoch 3/5] [Batch 914/938] [D loss: 0.6508086323738098] [G loss: 1.11214017868042]\n",
      "[Epoch 3/5] [Batch 915/938] [D loss: 0.5547281503677368] [G loss: 0.9405673742294312]\n",
      "[Epoch 3/5] [Batch 916/938] [D loss: 0.6484525203704834] [G loss: 0.7862072587013245]\n",
      "[Epoch 3/5] [Batch 917/938] [D loss: 0.5963784456253052] [G loss: 0.9154835939407349]\n",
      "[Epoch 3/5] [Batch 918/938] [D loss: 0.6182692050933838] [G loss: 0.9964659214019775]\n",
      "[Epoch 3/5] [Batch 919/938] [D loss: 0.5855044722557068] [G loss: 0.9466192722320557]\n",
      "[Epoch 3/5] [Batch 920/938] [D loss: 0.7083646059036255] [G loss: 0.7470515966415405]\n",
      "[Epoch 3/5] [Batch 921/938] [D loss: 0.6209082007408142] [G loss: 0.8998066186904907]\n",
      "[Epoch 3/5] [Batch 922/938] [D loss: 0.5936692953109741] [G loss: 0.9213197231292725]\n",
      "[Epoch 3/5] [Batch 923/938] [D loss: 0.6369304656982422] [G loss: 0.9934663772583008]\n",
      "[Epoch 3/5] [Batch 924/938] [D loss: 0.6355611681938171] [G loss: 1.031172752380371]\n",
      "[Epoch 3/5] [Batch 925/938] [D loss: 0.6220422387123108] [G loss: 0.9461323022842407]\n",
      "[Epoch 3/5] [Batch 926/938] [D loss: 0.7003057599067688] [G loss: 0.8852335810661316]\n",
      "[Epoch 3/5] [Batch 927/938] [D loss: 0.6214317083358765] [G loss: 0.7336770296096802]\n",
      "[Epoch 3/5] [Batch 928/938] [D loss: 0.640479326248169] [G loss: 1.2226150035858154]\n",
      "[Epoch 3/5] [Batch 929/938] [D loss: 0.6187107563018799] [G loss: 0.8759995698928833]\n",
      "[Epoch 3/5] [Batch 930/938] [D loss: 0.6322218179702759] [G loss: 0.8010760545730591]\n",
      "[Epoch 3/5] [Batch 931/938] [D loss: 0.610376238822937] [G loss: 0.9948817491531372]\n",
      "[Epoch 3/5] [Batch 932/938] [D loss: 0.5996344089508057] [G loss: 0.8917880654335022]\n",
      "[Epoch 3/5] [Batch 933/938] [D loss: 0.603236734867096] [G loss: 0.9520066380500793]\n",
      "[Epoch 3/5] [Batch 934/938] [D loss: 0.5851988196372986] [G loss: 0.9721235036849976]\n",
      "[Epoch 3/5] [Batch 935/938] [D loss: 0.5943341255187988] [G loss: 0.9980616569519043]\n",
      "[Epoch 3/5] [Batch 936/938] [D loss: 0.6201809644699097] [G loss: 0.9191104173660278]\n",
      "[Epoch 3/5] [Batch 937/938] [D loss: 0.5060387849807739] [G loss: 1.020859718322754]\n",
      "[Epoch 4/5] [Batch 0/938] [D loss: 0.6689660549163818] [G loss: 0.9767587184906006]\n",
      "[Epoch 4/5] [Batch 1/938] [D loss: 0.6370475888252258] [G loss: 0.7442049384117126]\n",
      "[Epoch 4/5] [Batch 2/938] [D loss: 0.5977586507797241] [G loss: 0.9272003769874573]\n",
      "[Epoch 4/5] [Batch 3/938] [D loss: 0.5784124732017517] [G loss: 0.8251882791519165]\n",
      "[Epoch 4/5] [Batch 4/938] [D loss: 0.5995944738388062] [G loss: 0.9493950605392456]\n",
      "[Epoch 4/5] [Batch 5/938] [D loss: 0.631085991859436] [G loss: 1.0659676790237427]\n",
      "[Epoch 4/5] [Batch 6/938] [D loss: 0.6461535692214966] [G loss: 0.7740901112556458]\n",
      "[Epoch 4/5] [Batch 7/938] [D loss: 0.6276867389678955] [G loss: 1.0404266119003296]\n",
      "[Epoch 4/5] [Batch 8/938] [D loss: 0.631048321723938] [G loss: 0.8654641509056091]\n",
      "[Epoch 4/5] [Batch 9/938] [D loss: 0.601152777671814] [G loss: 0.8429756760597229]\n",
      "[Epoch 4/5] [Batch 10/938] [D loss: 0.5964400172233582] [G loss: 0.8224217891693115]\n",
      "[Epoch 4/5] [Batch 11/938] [D loss: 0.5897401571273804] [G loss: 1.1141189336776733]\n",
      "[Epoch 4/5] [Batch 12/938] [D loss: 0.6676328182220459] [G loss: 0.8925076723098755]\n",
      "[Epoch 4/5] [Batch 13/938] [D loss: 0.6353245377540588] [G loss: 0.8510234355926514]\n",
      "[Epoch 4/5] [Batch 14/938] [D loss: 0.631804883480072] [G loss: 1.0727639198303223]\n",
      "[Epoch 4/5] [Batch 15/938] [D loss: 0.5830519199371338] [G loss: 1.0819296836853027]\n",
      "[Epoch 4/5] [Batch 16/938] [D loss: 0.6809822916984558] [G loss: 0.8327287435531616]\n",
      "[Epoch 4/5] [Batch 17/938] [D loss: 0.5855929851531982] [G loss: 0.9873035550117493]\n",
      "[Epoch 4/5] [Batch 18/938] [D loss: 0.613396167755127] [G loss: 0.8243601322174072]\n",
      "[Epoch 4/5] [Batch 19/938] [D loss: 0.6663206815719604] [G loss: 0.8261207938194275]\n",
      "[Epoch 4/5] [Batch 20/938] [D loss: 0.6190060973167419] [G loss: 0.956661581993103]\n",
      "[Epoch 4/5] [Batch 21/938] [D loss: 0.6014071702957153] [G loss: 0.8797740340232849]\n",
      "[Epoch 4/5] [Batch 22/938] [D loss: 0.6453114151954651] [G loss: 0.8997812271118164]\n",
      "[Epoch 4/5] [Batch 23/938] [D loss: 0.6195805072784424] [G loss: 1.0778175592422485]\n",
      "[Epoch 4/5] [Batch 24/938] [D loss: 0.6483407020568848] [G loss: 0.7305590510368347]\n",
      "[Epoch 4/5] [Batch 25/938] [D loss: 0.6169386506080627] [G loss: 0.725803554058075]\n",
      "[Epoch 4/5] [Batch 26/938] [D loss: 0.6140956878662109] [G loss: 1.018751621246338]\n",
      "[Epoch 4/5] [Batch 27/938] [D loss: 0.6003665924072266] [G loss: 0.870844841003418]\n",
      "[Epoch 4/5] [Batch 28/938] [D loss: 0.5856331586837769] [G loss: 0.8393035531044006]\n",
      "[Epoch 4/5] [Batch 29/938] [D loss: 0.6626782417297363] [G loss: 1.0825409889221191]\n",
      "[Epoch 4/5] [Batch 30/938] [D loss: 0.6330676078796387] [G loss: 0.8643556237220764]\n",
      "[Epoch 4/5] [Batch 31/938] [D loss: 0.6561247110366821] [G loss: 0.9736380577087402]\n",
      "[Epoch 4/5] [Batch 32/938] [D loss: 0.5961655974388123] [G loss: 0.824723482131958]\n",
      "[Epoch 4/5] [Batch 33/938] [D loss: 0.6376522779464722] [G loss: 0.8935392498970032]\n",
      "[Epoch 4/5] [Batch 34/938] [D loss: 0.663553774356842] [G loss: 0.9513633251190186]\n",
      "[Epoch 4/5] [Batch 35/938] [D loss: 0.635915219783783] [G loss: 0.6093045473098755]\n",
      "[Epoch 4/5] [Batch 36/938] [D loss: 0.6000896692276001] [G loss: 1.088547706604004]\n",
      "[Epoch 4/5] [Batch 37/938] [D loss: 0.6565931439399719] [G loss: 0.9595608115196228]\n",
      "[Epoch 4/5] [Batch 38/938] [D loss: 0.5983120203018188] [G loss: 0.8513180613517761]\n",
      "[Epoch 4/5] [Batch 39/938] [D loss: 0.6528602838516235] [G loss: 0.8020665645599365]\n",
      "[Epoch 4/5] [Batch 40/938] [D loss: 0.6424649953842163] [G loss: 0.8762741088867188]\n",
      "[Epoch 4/5] [Batch 41/938] [D loss: 0.6419792175292969] [G loss: 0.7200015783309937]\n",
      "[Epoch 4/5] [Batch 42/938] [D loss: 0.5661543607711792] [G loss: 1.095609426498413]\n",
      "[Epoch 4/5] [Batch 43/938] [D loss: 0.6263472437858582] [G loss: 0.9424843788146973]\n",
      "[Epoch 4/5] [Batch 44/938] [D loss: 0.6463981866836548] [G loss: 0.9790995121002197]\n",
      "[Epoch 4/5] [Batch 45/938] [D loss: 0.5834959745407104] [G loss: 0.8886884450912476]\n",
      "[Epoch 4/5] [Batch 46/938] [D loss: 0.6001818180084229] [G loss: 1.029882550239563]\n",
      "[Epoch 4/5] [Batch 47/938] [D loss: 0.6398279666900635] [G loss: 0.8730402588844299]\n",
      "[Epoch 4/5] [Batch 48/938] [D loss: 0.6366798877716064] [G loss: 0.7659872770309448]\n",
      "[Epoch 4/5] [Batch 49/938] [D loss: 0.6219451427459717] [G loss: 0.9769629836082458]\n",
      "[Epoch 4/5] [Batch 50/938] [D loss: 0.629257321357727] [G loss: 0.9172667264938354]\n",
      "[Epoch 4/5] [Batch 51/938] [D loss: 0.6116477847099304] [G loss: 0.8268148303031921]\n",
      "[Epoch 4/5] [Batch 52/938] [D loss: 0.6194931268692017] [G loss: 0.935957670211792]\n",
      "[Epoch 4/5] [Batch 53/938] [D loss: 0.6499150991439819] [G loss: 1.0113638639450073]\n",
      "[Epoch 4/5] [Batch 54/938] [D loss: 0.6338877081871033] [G loss: 0.8568944931030273]\n",
      "[Epoch 4/5] [Batch 55/938] [D loss: 0.6269629001617432] [G loss: 1.051382064819336]\n",
      "[Epoch 4/5] [Batch 56/938] [D loss: 0.6201518774032593] [G loss: 0.7253212928771973]\n",
      "[Epoch 4/5] [Batch 57/938] [D loss: 0.6177738308906555] [G loss: 0.8727995753288269]\n",
      "[Epoch 4/5] [Batch 58/938] [D loss: 0.6223070621490479] [G loss: 1.1044938564300537]\n",
      "[Epoch 4/5] [Batch 59/938] [D loss: 0.637609601020813] [G loss: 0.6905199289321899]\n",
      "[Epoch 4/5] [Batch 60/938] [D loss: 0.6331341862678528] [G loss: 1.0045099258422852]\n",
      "[Epoch 4/5] [Batch 61/938] [D loss: 0.6151825785636902] [G loss: 1.1169360876083374]\n",
      "[Epoch 4/5] [Batch 62/938] [D loss: 0.6526931524276733] [G loss: 0.6923626661300659]\n",
      "[Epoch 4/5] [Batch 63/938] [D loss: 0.597520112991333] [G loss: 0.9093400239944458]\n",
      "[Epoch 4/5] [Batch 64/938] [D loss: 0.6052839756011963] [G loss: 1.0164484977722168]\n",
      "[Epoch 4/5] [Batch 65/938] [D loss: 0.6857817769050598] [G loss: 0.7545204758644104]\n",
      "[Epoch 4/5] [Batch 66/938] [D loss: 0.5980485677719116] [G loss: 0.8061421513557434]\n",
      "[Epoch 4/5] [Batch 67/938] [D loss: 0.653427243232727] [G loss: 1.0492780208587646]\n",
      "[Epoch 4/5] [Batch 68/938] [D loss: 0.6089459657669067] [G loss: 0.6936212182044983]\n",
      "[Epoch 4/5] [Batch 69/938] [D loss: 0.5862837433815002] [G loss: 0.9017773270606995]\n",
      "[Epoch 4/5] [Batch 70/938] [D loss: 0.5983970165252686] [G loss: 1.2532767057418823]\n",
      "[Epoch 4/5] [Batch 71/938] [D loss: 0.6166031956672668] [G loss: 0.7806935906410217]\n",
      "[Epoch 4/5] [Batch 72/938] [D loss: 0.620819628238678] [G loss: 0.9774118661880493]\n",
      "[Epoch 4/5] [Batch 73/938] [D loss: 0.5836145877838135] [G loss: 0.9282267689704895]\n",
      "[Epoch 4/5] [Batch 74/938] [D loss: 0.6171547174453735] [G loss: 1.106143593788147]\n",
      "[Epoch 4/5] [Batch 75/938] [D loss: 0.5810787677764893] [G loss: 0.8616042733192444]\n",
      "[Epoch 4/5] [Batch 76/938] [D loss: 0.5989798307418823] [G loss: 0.9778362512588501]\n",
      "[Epoch 4/5] [Batch 77/938] [D loss: 0.6086812615394592] [G loss: 0.8054652214050293]\n",
      "[Epoch 4/5] [Batch 78/938] [D loss: 0.5641119480133057] [G loss: 0.8497391939163208]\n",
      "[Epoch 4/5] [Batch 79/938] [D loss: 0.6061209440231323] [G loss: 1.2382962703704834]\n",
      "[Epoch 4/5] [Batch 80/938] [D loss: 0.601905107498169] [G loss: 0.7699326276779175]\n",
      "[Epoch 4/5] [Batch 81/938] [D loss: 0.5965721011161804] [G loss: 1.1358202695846558]\n",
      "[Epoch 4/5] [Batch 82/938] [D loss: 0.6072744131088257] [G loss: 1.016202688217163]\n",
      "[Epoch 4/5] [Batch 83/938] [D loss: 0.6223810911178589] [G loss: 0.9944139719009399]\n",
      "[Epoch 4/5] [Batch 84/938] [D loss: 0.6765058636665344] [G loss: 1.0007578134536743]\n",
      "[Epoch 4/5] [Batch 85/938] [D loss: 0.6482518911361694] [G loss: 0.8351237773895264]\n",
      "[Epoch 4/5] [Batch 86/938] [D loss: 0.6094866991043091] [G loss: 1.123718500137329]\n",
      "[Epoch 4/5] [Batch 87/938] [D loss: 0.6062312126159668] [G loss: 0.9375526309013367]\n",
      "[Epoch 4/5] [Batch 88/938] [D loss: 0.6383973360061646] [G loss: 0.8164888620376587]\n",
      "[Epoch 4/5] [Batch 89/938] [D loss: 0.6820144653320312] [G loss: 0.6706204414367676]\n",
      "[Epoch 4/5] [Batch 90/938] [D loss: 0.6602025032043457] [G loss: 1.1016765832901]\n",
      "[Epoch 4/5] [Batch 91/938] [D loss: 0.6325240135192871] [G loss: 0.7342250347137451]\n",
      "[Epoch 4/5] [Batch 92/938] [D loss: 0.6244685649871826] [G loss: 0.7643002271652222]\n",
      "[Epoch 4/5] [Batch 93/938] [D loss: 0.597281813621521] [G loss: 1.0232998132705688]\n",
      "[Epoch 4/5] [Batch 94/938] [D loss: 0.6142836213111877] [G loss: 1.0682119131088257]\n",
      "[Epoch 4/5] [Batch 95/938] [D loss: 0.6360324621200562] [G loss: 0.6869500875473022]\n",
      "[Epoch 4/5] [Batch 96/938] [D loss: 0.6652737855911255] [G loss: 0.9857074022293091]\n",
      "[Epoch 4/5] [Batch 97/938] [D loss: 0.6157820224761963] [G loss: 0.8098673224449158]\n",
      "[Epoch 4/5] [Batch 98/938] [D loss: 0.6546933650970459] [G loss: 0.8073391318321228]\n",
      "[Epoch 4/5] [Batch 99/938] [D loss: 0.664228081703186] [G loss: 0.8631812930107117]\n",
      "[Epoch 4/5] [Batch 100/938] [D loss: 0.6111255884170532] [G loss: 1.0319925546646118]\n",
      "[Epoch 4/5] [Batch 101/938] [D loss: 0.644107460975647] [G loss: 0.9009933471679688]\n",
      "[Epoch 4/5] [Batch 102/938] [D loss: 0.6832244396209717] [G loss: 0.7954612970352173]\n",
      "[Epoch 4/5] [Batch 103/938] [D loss: 0.6353793144226074] [G loss: 0.8747460842132568]\n",
      "[Epoch 4/5] [Batch 104/938] [D loss: 0.6182845234870911] [G loss: 1.0436233282089233]\n",
      "[Epoch 4/5] [Batch 105/938] [D loss: 0.642611026763916] [G loss: 0.8661110997200012]\n",
      "[Epoch 4/5] [Batch 106/938] [D loss: 0.6145405173301697] [G loss: 0.8113195300102234]\n",
      "[Epoch 4/5] [Batch 107/938] [D loss: 0.6084027290344238] [G loss: 0.9329899549484253]\n",
      "[Epoch 4/5] [Batch 108/938] [D loss: 0.6136806011199951] [G loss: 0.9555635452270508]\n",
      "[Epoch 4/5] [Batch 109/938] [D loss: 0.6300154328346252] [G loss: 0.8184384107589722]\n",
      "[Epoch 4/5] [Batch 110/938] [D loss: 0.6057947278022766] [G loss: 0.8704171180725098]\n",
      "[Epoch 4/5] [Batch 111/938] [D loss: 0.5898635983467102] [G loss: 1.02560555934906]\n",
      "[Epoch 4/5] [Batch 112/938] [D loss: 0.59834885597229] [G loss: 0.8886103630065918]\n",
      "[Epoch 4/5] [Batch 113/938] [D loss: 0.6730948686599731] [G loss: 0.8132728338241577]\n",
      "[Epoch 4/5] [Batch 114/938] [D loss: 0.6498130559921265] [G loss: 1.144424557685852]\n",
      "[Epoch 4/5] [Batch 115/938] [D loss: 0.6322484612464905] [G loss: 0.5778881311416626]\n",
      "[Epoch 4/5] [Batch 116/938] [D loss: 0.6438539028167725] [G loss: 1.0745500326156616]\n",
      "[Epoch 4/5] [Batch 117/938] [D loss: 0.6075344085693359] [G loss: 0.8635362982749939]\n",
      "[Epoch 4/5] [Batch 118/938] [D loss: 0.6425151824951172] [G loss: 0.9408889412879944]\n",
      "[Epoch 4/5] [Batch 119/938] [D loss: 0.6706230640411377] [G loss: 0.8911005258560181]\n",
      "[Epoch 4/5] [Batch 120/938] [D loss: 0.6016445159912109] [G loss: 0.9363710284233093]\n",
      "[Epoch 4/5] [Batch 121/938] [D loss: 0.6404797434806824] [G loss: 0.8651654124259949]\n",
      "[Epoch 4/5] [Batch 122/938] [D loss: 0.6419987678527832] [G loss: 0.889764666557312]\n",
      "[Epoch 4/5] [Batch 123/938] [D loss: 0.6019194722175598] [G loss: 0.7957483530044556]\n",
      "[Epoch 4/5] [Batch 124/938] [D loss: 0.5948028564453125] [G loss: 0.9613527655601501]\n",
      "[Epoch 4/5] [Batch 125/938] [D loss: 0.6022006273269653] [G loss: 0.8537376523017883]\n",
      "[Epoch 4/5] [Batch 126/938] [D loss: 0.6299054026603699] [G loss: 0.833084762096405]\n",
      "[Epoch 4/5] [Batch 127/938] [D loss: 0.6104698181152344] [G loss: 0.8665633201599121]\n",
      "[Epoch 4/5] [Batch 128/938] [D loss: 0.6010578870773315] [G loss: 0.9973927736282349]\n",
      "[Epoch 4/5] [Batch 129/938] [D loss: 0.6484102010726929] [G loss: 0.6953538656234741]\n",
      "[Epoch 4/5] [Batch 130/938] [D loss: 0.598774790763855] [G loss: 1.241054654121399]\n",
      "[Epoch 4/5] [Batch 131/938] [D loss: 0.6696449518203735] [G loss: 0.760180652141571]\n",
      "[Epoch 4/5] [Batch 132/938] [D loss: 0.6020363569259644] [G loss: 0.922581672668457]\n",
      "[Epoch 4/5] [Batch 133/938] [D loss: 0.6073729991912842] [G loss: 0.9261302351951599]\n",
      "[Epoch 4/5] [Batch 134/938] [D loss: 0.6115027070045471] [G loss: 1.01810622215271]\n",
      "[Epoch 4/5] [Batch 135/938] [D loss: 0.6217153668403625] [G loss: 0.8694843053817749]\n",
      "[Epoch 4/5] [Batch 136/938] [D loss: 0.599063515663147] [G loss: 0.8577076196670532]\n",
      "[Epoch 4/5] [Batch 137/938] [D loss: 0.6005333662033081] [G loss: 0.9957107305526733]\n",
      "[Epoch 4/5] [Batch 138/938] [D loss: 0.6113042831420898] [G loss: 0.8275772333145142]\n",
      "[Epoch 4/5] [Batch 139/938] [D loss: 0.6035090684890747] [G loss: 1.0135425329208374]\n",
      "[Epoch 4/5] [Batch 140/938] [D loss: 0.650404691696167] [G loss: 0.8753665685653687]\n",
      "[Epoch 4/5] [Batch 141/938] [D loss: 0.6396781206130981] [G loss: 1.1944807767868042]\n",
      "[Epoch 4/5] [Batch 142/938] [D loss: 0.624422013759613] [G loss: 0.8088575005531311]\n",
      "[Epoch 4/5] [Batch 143/938] [D loss: 0.6428825855255127] [G loss: 0.9019043445587158]\n",
      "[Epoch 4/5] [Batch 144/938] [D loss: 0.618445873260498] [G loss: 0.9424839019775391]\n",
      "[Epoch 4/5] [Batch 145/938] [D loss: 0.5846866369247437] [G loss: 0.9507842659950256]\n",
      "[Epoch 4/5] [Batch 146/938] [D loss: 0.5706071853637695] [G loss: 0.9855455160140991]\n",
      "[Epoch 4/5] [Batch 147/938] [D loss: 0.6144908666610718] [G loss: 0.9488929510116577]\n",
      "[Epoch 4/5] [Batch 148/938] [D loss: 0.6342585682868958] [G loss: 1.1351022720336914]\n",
      "[Epoch 4/5] [Batch 149/938] [D loss: 0.6314178705215454] [G loss: 0.6538010239601135]\n",
      "[Epoch 4/5] [Batch 150/938] [D loss: 0.6196967363357544] [G loss: 1.220400333404541]\n",
      "[Epoch 4/5] [Batch 151/938] [D loss: 0.6104288697242737] [G loss: 0.7008962631225586]\n",
      "[Epoch 4/5] [Batch 152/938] [D loss: 0.63816237449646] [G loss: 0.9863253831863403]\n",
      "[Epoch 4/5] [Batch 153/938] [D loss: 0.609444260597229] [G loss: 1.005681037902832]\n",
      "[Epoch 4/5] [Batch 154/938] [D loss: 0.6792969107627869] [G loss: 0.7631509304046631]\n",
      "[Epoch 4/5] [Batch 155/938] [D loss: 0.6196317672729492] [G loss: 1.0028109550476074]\n",
      "[Epoch 4/5] [Batch 156/938] [D loss: 0.6260486841201782] [G loss: 1.003635048866272]\n",
      "[Epoch 4/5] [Batch 157/938] [D loss: 0.6309006214141846] [G loss: 0.6445735692977905]\n",
      "[Epoch 4/5] [Batch 158/938] [D loss: 0.6289796829223633] [G loss: 0.9472612738609314]\n",
      "[Epoch 4/5] [Batch 159/938] [D loss: 0.6288629174232483] [G loss: 0.8665199279785156]\n",
      "[Epoch 4/5] [Batch 160/938] [D loss: 0.6072027683258057] [G loss: 0.7974717020988464]\n",
      "[Epoch 4/5] [Batch 161/938] [D loss: 0.6024183034896851] [G loss: 0.9402125477790833]\n",
      "[Epoch 4/5] [Batch 162/938] [D loss: 0.6221266388893127] [G loss: 0.889973521232605]\n",
      "[Epoch 4/5] [Batch 163/938] [D loss: 0.5891616344451904] [G loss: 0.813348650932312]\n",
      "[Epoch 4/5] [Batch 164/938] [D loss: 0.6363843679428101] [G loss: 1.1036721467971802]\n",
      "[Epoch 4/5] [Batch 165/938] [D loss: 0.5896768569946289] [G loss: 0.9140022993087769]\n",
      "[Epoch 4/5] [Batch 166/938] [D loss: 0.6485458612442017] [G loss: 0.7851476073265076]\n",
      "[Epoch 4/5] [Batch 167/938] [D loss: 0.6198009848594666] [G loss: 0.9686174988746643]\n",
      "[Epoch 4/5] [Batch 168/938] [D loss: 0.6114867329597473] [G loss: 1.1416046619415283]\n",
      "[Epoch 4/5] [Batch 169/938] [D loss: 0.6035568118095398] [G loss: 0.7018551826477051]\n",
      "[Epoch 4/5] [Batch 170/938] [D loss: 0.609978973865509] [G loss: 0.8443717956542969]\n",
      "[Epoch 4/5] [Batch 171/938] [D loss: 0.6223966479301453] [G loss: 1.195717692375183]\n",
      "[Epoch 4/5] [Batch 172/938] [D loss: 0.5674959421157837] [G loss: 0.8579499125480652]\n",
      "[Epoch 4/5] [Batch 173/938] [D loss: 0.5716347694396973] [G loss: 0.7916017174720764]\n",
      "[Epoch 4/5] [Batch 174/938] [D loss: 0.6265699863433838] [G loss: 1.1922874450683594]\n",
      "[Epoch 4/5] [Batch 175/938] [D loss: 0.5954586267471313] [G loss: 0.8548157811164856]\n",
      "[Epoch 4/5] [Batch 176/938] [D loss: 0.6388856172561646] [G loss: 0.7839602828025818]\n",
      "[Epoch 4/5] [Batch 177/938] [D loss: 0.6463468074798584] [G loss: 1.0357478857040405]\n",
      "[Epoch 4/5] [Batch 178/938] [D loss: 0.606410562992096] [G loss: 0.9104359149932861]\n",
      "[Epoch 4/5] [Batch 179/938] [D loss: 0.6022491455078125] [G loss: 0.8032234907150269]\n",
      "[Epoch 4/5] [Batch 180/938] [D loss: 0.6023437976837158] [G loss: 0.8187062740325928]\n",
      "[Epoch 4/5] [Batch 181/938] [D loss: 0.6463843584060669] [G loss: 1.1358249187469482]\n",
      "[Epoch 4/5] [Batch 182/938] [D loss: 0.6290384531021118] [G loss: 0.8479305505752563]\n",
      "[Epoch 4/5] [Batch 183/938] [D loss: 0.5955815315246582] [G loss: 0.9527180194854736]\n",
      "[Epoch 4/5] [Batch 184/938] [D loss: 0.5877739787101746] [G loss: 0.8977673649787903]\n",
      "[Epoch 4/5] [Batch 185/938] [D loss: 0.6165707111358643] [G loss: 0.8414487838745117]\n",
      "[Epoch 4/5] [Batch 186/938] [D loss: 0.5771351456642151] [G loss: 0.9387320280075073]\n",
      "[Epoch 4/5] [Batch 187/938] [D loss: 0.6431030631065369] [G loss: 1.097469687461853]\n",
      "[Epoch 4/5] [Batch 188/938] [D loss: 0.6219446063041687] [G loss: 0.5838314890861511]\n",
      "[Epoch 4/5] [Batch 189/938] [D loss: 0.5616656541824341] [G loss: 1.0471575260162354]\n",
      "[Epoch 4/5] [Batch 190/938] [D loss: 0.6582227945327759] [G loss: 0.9092617630958557]\n",
      "[Epoch 4/5] [Batch 191/938] [D loss: 0.622175395488739] [G loss: 0.8291290998458862]\n",
      "[Epoch 4/5] [Batch 192/938] [D loss: 0.6367577314376831] [G loss: 0.976150393486023]\n",
      "[Epoch 4/5] [Batch 193/938] [D loss: 0.6395267248153687] [G loss: 0.9675843119621277]\n",
      "[Epoch 4/5] [Batch 194/938] [D loss: 0.6222813129425049] [G loss: 0.7972006797790527]\n",
      "[Epoch 4/5] [Batch 195/938] [D loss: 0.6845337152481079] [G loss: 1.2009191513061523]\n",
      "[Epoch 4/5] [Batch 196/938] [D loss: 0.6095969676971436] [G loss: 0.8723970651626587]\n",
      "[Epoch 4/5] [Batch 197/938] [D loss: 0.6200668811798096] [G loss: 1.0675987005233765]\n",
      "[Epoch 4/5] [Batch 198/938] [D loss: 0.6537314653396606] [G loss: 0.8817742466926575]\n",
      "[Epoch 4/5] [Batch 199/938] [D loss: 0.6124430894851685] [G loss: 0.8395344018936157]\n",
      "[Epoch 4/5] [Batch 200/938] [D loss: 0.5951493978500366] [G loss: 0.8044294118881226]\n",
      "[Epoch 4/5] [Batch 201/938] [D loss: 0.6072920560836792] [G loss: 0.9012560844421387]\n",
      "[Epoch 4/5] [Batch 202/938] [D loss: 0.5904857516288757] [G loss: 0.9463400840759277]\n",
      "[Epoch 4/5] [Batch 203/938] [D loss: 0.6039344668388367] [G loss: 0.8016827702522278]\n",
      "[Epoch 4/5] [Batch 204/938] [D loss: 0.6195839643478394] [G loss: 0.9677925109863281]\n",
      "[Epoch 4/5] [Batch 205/938] [D loss: 0.6270667314529419] [G loss: 0.9390277862548828]\n",
      "[Epoch 4/5] [Batch 206/938] [D loss: 0.6150733232498169] [G loss: 0.9229654669761658]\n",
      "[Epoch 4/5] [Batch 207/938] [D loss: 0.6293671727180481] [G loss: 1.0434012413024902]\n",
      "[Epoch 4/5] [Batch 208/938] [D loss: 0.6157612800598145] [G loss: 0.7338235378265381]\n",
      "[Epoch 4/5] [Batch 209/938] [D loss: 0.6476749777793884] [G loss: 0.9951437711715698]\n",
      "[Epoch 4/5] [Batch 210/938] [D loss: 0.6659259796142578] [G loss: 1.04095458984375]\n",
      "[Epoch 4/5] [Batch 211/938] [D loss: 0.6656994223594666] [G loss: 0.8692334294319153]\n",
      "[Epoch 4/5] [Batch 212/938] [D loss: 0.6507219076156616] [G loss: 0.9335881471633911]\n",
      "[Epoch 4/5] [Batch 213/938] [D loss: 0.6454871892929077] [G loss: 1.0047227144241333]\n",
      "[Epoch 4/5] [Batch 214/938] [D loss: 0.5675479173660278] [G loss: 0.9031637907028198]\n",
      "[Epoch 4/5] [Batch 215/938] [D loss: 0.6435552835464478] [G loss: 0.9557366967201233]\n",
      "[Epoch 4/5] [Batch 216/938] [D loss: 0.6027369499206543] [G loss: 0.8507171273231506]\n",
      "[Epoch 4/5] [Batch 217/938] [D loss: 0.6176939010620117] [G loss: 1.0086363554000854]\n",
      "[Epoch 4/5] [Batch 218/938] [D loss: 0.6704877614974976] [G loss: 1.1115163564682007]\n",
      "[Epoch 4/5] [Batch 219/938] [D loss: 0.6356767416000366] [G loss: 0.8436258435249329]\n",
      "[Epoch 4/5] [Batch 220/938] [D loss: 0.6326649785041809] [G loss: 1.0002048015594482]\n",
      "[Epoch 4/5] [Batch 221/938] [D loss: 0.6073616147041321] [G loss: 0.865397036075592]\n",
      "[Epoch 4/5] [Batch 222/938] [D loss: 0.6446080207824707] [G loss: 0.9153400659561157]\n",
      "[Epoch 4/5] [Batch 223/938] [D loss: 0.58504319190979] [G loss: 0.810956597328186]\n",
      "[Epoch 4/5] [Batch 224/938] [D loss: 0.6406279802322388] [G loss: 1.020876407623291]\n",
      "[Epoch 4/5] [Batch 225/938] [D loss: 0.6510045528411865] [G loss: 0.7631993293762207]\n",
      "[Epoch 4/5] [Batch 226/938] [D loss: 0.6097075939178467] [G loss: 1.0449140071868896]\n",
      "[Epoch 4/5] [Batch 227/938] [D loss: 0.6489254832267761] [G loss: 1.0124226808547974]\n",
      "[Epoch 4/5] [Batch 228/938] [D loss: 0.6575607061386108] [G loss: 0.6526618599891663]\n",
      "[Epoch 4/5] [Batch 229/938] [D loss: 0.5687586069107056] [G loss: 1.006388783454895]\n",
      "[Epoch 4/5] [Batch 230/938] [D loss: 0.6086810827255249] [G loss: 0.9378302097320557]\n",
      "[Epoch 4/5] [Batch 231/938] [D loss: 0.6326072216033936] [G loss: 0.8599101305007935]\n",
      "[Epoch 4/5] [Batch 232/938] [D loss: 0.6510543823242188] [G loss: 0.895234227180481]\n",
      "[Epoch 4/5] [Batch 233/938] [D loss: 0.6253963708877563] [G loss: 1.0665262937545776]\n",
      "[Epoch 4/5] [Batch 234/938] [D loss: 0.5883822441101074] [G loss: 0.841896653175354]\n",
      "[Epoch 4/5] [Batch 235/938] [D loss: 0.6009683012962341] [G loss: 0.8782246112823486]\n",
      "[Epoch 4/5] [Batch 236/938] [D loss: 0.5997127890586853] [G loss: 0.9194802641868591]\n",
      "[Epoch 4/5] [Batch 237/938] [D loss: 0.5832138061523438] [G loss: 0.9710981249809265]\n",
      "[Epoch 4/5] [Batch 238/938] [D loss: 0.5670782923698425] [G loss: 1.0173121690750122]\n",
      "[Epoch 4/5] [Batch 239/938] [D loss: 0.6352573037147522] [G loss: 0.6681782007217407]\n",
      "[Epoch 4/5] [Batch 240/938] [D loss: 0.5874940156936646] [G loss: 1.0986700057983398]\n",
      "[Epoch 4/5] [Batch 241/938] [D loss: 0.6019601225852966] [G loss: 1.0257940292358398]\n",
      "[Epoch 4/5] [Batch 242/938] [D loss: 0.6207261681556702] [G loss: 1.038814902305603]\n",
      "[Epoch 4/5] [Batch 243/938] [D loss: 0.6366991996765137] [G loss: 0.71766197681427]\n",
      "[Epoch 4/5] [Batch 244/938] [D loss: 0.6564798355102539] [G loss: 1.0221457481384277]\n",
      "[Epoch 4/5] [Batch 245/938] [D loss: 0.6503609418869019] [G loss: 0.8501954078674316]\n",
      "[Epoch 4/5] [Batch 246/938] [D loss: 0.6722137331962585] [G loss: 0.9512534141540527]\n",
      "[Epoch 4/5] [Batch 247/938] [D loss: 0.5818853378295898] [G loss: 0.8159550428390503]\n",
      "[Epoch 4/5] [Batch 248/938] [D loss: 0.5897157788276672] [G loss: 1.0365782976150513]\n",
      "[Epoch 4/5] [Batch 249/938] [D loss: 0.6329931020736694] [G loss: 0.825465738773346]\n",
      "[Epoch 4/5] [Batch 250/938] [D loss: 0.6008937358856201] [G loss: 1.0067253112792969]\n",
      "[Epoch 4/5] [Batch 251/938] [D loss: 0.5408241748809814] [G loss: 1.1044631004333496]\n",
      "[Epoch 4/5] [Batch 252/938] [D loss: 0.579498291015625] [G loss: 1.0247527360916138]\n",
      "[Epoch 4/5] [Batch 253/938] [D loss: 0.6503194570541382] [G loss: 1.0798909664154053]\n",
      "[Epoch 4/5] [Batch 254/938] [D loss: 0.5832082033157349] [G loss: 0.7857891917228699]\n",
      "[Epoch 4/5] [Batch 255/938] [D loss: 0.6095441579818726] [G loss: 1.2285256385803223]\n",
      "[Epoch 4/5] [Batch 256/938] [D loss: 0.6501137018203735] [G loss: 0.819556474685669]\n",
      "[Epoch 4/5] [Batch 257/938] [D loss: 0.6089723706245422] [G loss: 1.1915903091430664]\n",
      "[Epoch 4/5] [Batch 258/938] [D loss: 0.6235525608062744] [G loss: 0.8869451880455017]\n",
      "[Epoch 4/5] [Batch 259/938] [D loss: 0.6265518665313721] [G loss: 0.7774166464805603]\n",
      "[Epoch 4/5] [Batch 260/938] [D loss: 0.6653758883476257] [G loss: 0.9237223863601685]\n",
      "[Epoch 4/5] [Batch 261/938] [D loss: 0.59283846616745] [G loss: 0.9893476366996765]\n",
      "[Epoch 4/5] [Batch 262/938] [D loss: 0.6207613945007324] [G loss: 0.9209944009780884]\n",
      "[Epoch 4/5] [Batch 263/938] [D loss: 0.5793172121047974] [G loss: 0.9796890020370483]\n",
      "[Epoch 4/5] [Batch 264/938] [D loss: 0.6223032474517822] [G loss: 0.9301534295082092]\n",
      "[Epoch 4/5] [Batch 265/938] [D loss: 0.5316559672355652] [G loss: 0.8816260695457458]\n",
      "[Epoch 4/5] [Batch 266/938] [D loss: 0.629714846611023] [G loss: 0.9855916500091553]\n",
      "[Epoch 4/5] [Batch 267/938] [D loss: 0.6332839727401733] [G loss: 0.8215509653091431]\n",
      "[Epoch 4/5] [Batch 268/938] [D loss: 0.6308687925338745] [G loss: 0.9999059438705444]\n",
      "[Epoch 4/5] [Batch 269/938] [D loss: 0.6354918479919434] [G loss: 0.9776666164398193]\n",
      "[Epoch 4/5] [Batch 270/938] [D loss: 0.6403038501739502] [G loss: 0.7978031635284424]\n",
      "[Epoch 4/5] [Batch 271/938] [D loss: 0.5474287867546082] [G loss: 0.8693210482597351]\n",
      "[Epoch 4/5] [Batch 272/938] [D loss: 0.6239242553710938] [G loss: 0.9113337993621826]\n",
      "[Epoch 4/5] [Batch 273/938] [D loss: 0.577594518661499] [G loss: 0.921860933303833]\n",
      "[Epoch 4/5] [Batch 274/938] [D loss: 0.637844443321228] [G loss: 1.0267469882965088]\n",
      "[Epoch 4/5] [Batch 275/938] [D loss: 0.635611355304718] [G loss: 0.9253109693527222]\n",
      "[Epoch 4/5] [Batch 276/938] [D loss: 0.5975538492202759] [G loss: 1.0109442472457886]\n",
      "[Epoch 4/5] [Batch 277/938] [D loss: 0.6664392948150635] [G loss: 0.9378100633621216]\n",
      "[Epoch 4/5] [Batch 278/938] [D loss: 0.680838406085968] [G loss: 0.9255900382995605]\n",
      "[Epoch 4/5] [Batch 279/938] [D loss: 0.5997346043586731] [G loss: 1.054734706878662]\n",
      "[Epoch 4/5] [Batch 280/938] [D loss: 0.6368464231491089] [G loss: 0.8800950646400452]\n",
      "[Epoch 4/5] [Batch 281/938] [D loss: 0.6055041551589966] [G loss: 0.8658228516578674]\n",
      "[Epoch 4/5] [Batch 282/938] [D loss: 0.629909873008728] [G loss: 0.8684400916099548]\n",
      "[Epoch 4/5] [Batch 283/938] [D loss: 0.604153037071228] [G loss: 1.1561851501464844]\n",
      "[Epoch 4/5] [Batch 284/938] [D loss: 0.6093322038650513] [G loss: 0.8291335701942444]\n",
      "[Epoch 4/5] [Batch 285/938] [D loss: 0.6391992568969727] [G loss: 0.9528448581695557]\n",
      "[Epoch 4/5] [Batch 286/938] [D loss: 0.654525876045227] [G loss: 0.8438846468925476]\n",
      "[Epoch 4/5] [Batch 287/938] [D loss: 0.6261029243469238] [G loss: 1.0226199626922607]\n",
      "[Epoch 4/5] [Batch 288/938] [D loss: 0.6106883883476257] [G loss: 0.8011723756790161]\n",
      "[Epoch 4/5] [Batch 289/938] [D loss: 0.6223162412643433] [G loss: 0.9209804534912109]\n",
      "[Epoch 4/5] [Batch 290/938] [D loss: 0.6060245037078857] [G loss: 0.9092389941215515]\n",
      "[Epoch 4/5] [Batch 291/938] [D loss: 0.6108711361885071] [G loss: 0.9240487813949585]\n",
      "[Epoch 4/5] [Batch 292/938] [D loss: 0.6285408735275269] [G loss: 0.6845546960830688]\n",
      "[Epoch 4/5] [Batch 293/938] [D loss: 0.6232166290283203] [G loss: 0.9455726742744446]\n",
      "[Epoch 4/5] [Batch 294/938] [D loss: 0.6338920593261719] [G loss: 1.0338013172149658]\n",
      "[Epoch 4/5] [Batch 295/938] [D loss: 0.667029857635498] [G loss: 0.7132593393325806]\n",
      "[Epoch 4/5] [Batch 296/938] [D loss: 0.6450831890106201] [G loss: 0.9079870581626892]\n",
      "[Epoch 4/5] [Batch 297/938] [D loss: 0.5951281785964966] [G loss: 0.9346469640731812]\n",
      "[Epoch 4/5] [Batch 298/938] [D loss: 0.5719932317733765] [G loss: 0.9307206869125366]\n",
      "[Epoch 4/5] [Batch 299/938] [D loss: 0.6007418036460876] [G loss: 0.7579700350761414]\n",
      "[Epoch 4/5] [Batch 300/938] [D loss: 0.6753553748130798] [G loss: 1.2211248874664307]\n",
      "[Epoch 4/5] [Batch 301/938] [D loss: 0.6073354482650757] [G loss: 0.6701774597167969]\n",
      "[Epoch 4/5] [Batch 302/938] [D loss: 0.6303175091743469] [G loss: 0.9108726382255554]\n",
      "[Epoch 4/5] [Batch 303/938] [D loss: 0.6545020341873169] [G loss: 1.013185977935791]\n",
      "[Epoch 4/5] [Batch 304/938] [D loss: 0.6273760795593262] [G loss: 0.8696475028991699]\n",
      "[Epoch 4/5] [Batch 305/938] [D loss: 0.6191766262054443] [G loss: 0.706239640712738]\n",
      "[Epoch 4/5] [Batch 306/938] [D loss: 0.5954421162605286] [G loss: 1.0002388954162598]\n",
      "[Epoch 4/5] [Batch 307/938] [D loss: 0.6429003477096558] [G loss: 0.9922455549240112]\n",
      "[Epoch 4/5] [Batch 308/938] [D loss: 0.6335237622261047] [G loss: 0.9883063435554504]\n",
      "[Epoch 4/5] [Batch 309/938] [D loss: 0.5747760534286499] [G loss: 0.7340503931045532]\n",
      "[Epoch 4/5] [Batch 310/938] [D loss: 0.6325525641441345] [G loss: 1.0551109313964844]\n",
      "[Epoch 4/5] [Batch 311/938] [D loss: 0.623394250869751] [G loss: 0.7678531408309937]\n",
      "[Epoch 4/5] [Batch 312/938] [D loss: 0.6419677734375] [G loss: 0.9820138812065125]\n",
      "[Epoch 4/5] [Batch 313/938] [D loss: 0.5916180610656738] [G loss: 0.9044808745384216]\n",
      "[Epoch 4/5] [Batch 314/938] [D loss: 0.627734899520874] [G loss: 0.8919038772583008]\n",
      "[Epoch 4/5] [Batch 315/938] [D loss: 0.6298975348472595] [G loss: 0.8228620886802673]\n",
      "[Epoch 4/5] [Batch 316/938] [D loss: 0.6027548313140869] [G loss: 1.1234478950500488]\n",
      "[Epoch 4/5] [Batch 317/938] [D loss: 0.6270113587379456] [G loss: 0.9348613619804382]\n",
      "[Epoch 4/5] [Batch 318/938] [D loss: 0.6285964846611023] [G loss: 0.9457348585128784]\n",
      "[Epoch 4/5] [Batch 319/938] [D loss: 0.6227827072143555] [G loss: 0.7727617025375366]\n",
      "[Epoch 4/5] [Batch 320/938] [D loss: 0.5840184688568115] [G loss: 0.8612138628959656]\n",
      "[Epoch 4/5] [Batch 321/938] [D loss: 0.5729274153709412] [G loss: 0.9566022753715515]\n",
      "[Epoch 4/5] [Batch 322/938] [D loss: 0.6031913161277771] [G loss: 0.7768997550010681]\n",
      "[Epoch 4/5] [Batch 323/938] [D loss: 0.6177681088447571] [G loss: 1.0992308855056763]\n",
      "[Epoch 4/5] [Batch 324/938] [D loss: 0.5927025675773621] [G loss: 0.8741493225097656]\n",
      "[Epoch 4/5] [Batch 325/938] [D loss: 0.6216827630996704] [G loss: 0.9207184314727783]\n",
      "[Epoch 4/5] [Batch 326/938] [D loss: 0.6903085112571716] [G loss: 1.3069041967391968]\n",
      "[Epoch 4/5] [Batch 327/938] [D loss: 0.7270061373710632] [G loss: 0.43086928129196167]\n",
      "[Epoch 4/5] [Batch 328/938] [D loss: 0.6200366020202637] [G loss: 1.081729769706726]\n",
      "[Epoch 4/5] [Batch 329/938] [D loss: 0.6865699291229248] [G loss: 1.093064546585083]\n",
      "[Epoch 4/5] [Batch 330/938] [D loss: 0.6117101907730103] [G loss: 0.6835471987724304]\n",
      "[Epoch 4/5] [Batch 331/938] [D loss: 0.6392225623130798] [G loss: 0.7530306577682495]\n",
      "[Epoch 4/5] [Batch 332/938] [D loss: 0.6512544751167297] [G loss: 0.8720026612281799]\n",
      "[Epoch 4/5] [Batch 333/938] [D loss: 0.6989585161209106] [G loss: 0.9310917854309082]\n",
      "[Epoch 4/5] [Batch 334/938] [D loss: 0.6167466640472412] [G loss: 0.7932232022285461]\n",
      "[Epoch 4/5] [Batch 335/938] [D loss: 0.5941119194030762] [G loss: 0.7729687690734863]\n",
      "[Epoch 4/5] [Batch 336/938] [D loss: 0.6096654534339905] [G loss: 0.8766158223152161]\n",
      "[Epoch 4/5] [Batch 337/938] [D loss: 0.6094858646392822] [G loss: 0.9148291945457458]\n",
      "[Epoch 4/5] [Batch 338/938] [D loss: 0.6153663396835327] [G loss: 0.8772744536399841]\n",
      "[Epoch 4/5] [Batch 339/938] [D loss: 0.5966318845748901] [G loss: 0.801016092300415]\n",
      "[Epoch 4/5] [Batch 340/938] [D loss: 0.6376475691795349] [G loss: 0.8243550062179565]\n",
      "[Epoch 4/5] [Batch 341/938] [D loss: 0.6485967636108398] [G loss: 0.9244532585144043]\n",
      "[Epoch 4/5] [Batch 342/938] [D loss: 0.6052311658859253] [G loss: 0.8274275660514832]\n",
      "[Epoch 4/5] [Batch 343/938] [D loss: 0.6149383783340454] [G loss: 0.748883843421936]\n",
      "[Epoch 4/5] [Batch 344/938] [D loss: 0.6373217701911926] [G loss: 0.8459381461143494]\n",
      "[Epoch 4/5] [Batch 345/938] [D loss: 0.6017612218856812] [G loss: 1.0763213634490967]\n",
      "[Epoch 4/5] [Batch 346/938] [D loss: 0.5764683485031128] [G loss: 1.051422119140625]\n",
      "[Epoch 4/5] [Batch 347/938] [D loss: 0.6400642395019531] [G loss: 0.922365665435791]\n",
      "[Epoch 4/5] [Batch 348/938] [D loss: 0.6235131025314331] [G loss: 0.8702937364578247]\n",
      "[Epoch 4/5] [Batch 349/938] [D loss: 0.6089708805084229] [G loss: 0.9812529683113098]\n",
      "[Epoch 4/5] [Batch 350/938] [D loss: 0.613326907157898] [G loss: 0.8057251572608948]\n",
      "[Epoch 4/5] [Batch 351/938] [D loss: 0.611635684967041] [G loss: 1.0790588855743408]\n",
      "[Epoch 4/5] [Batch 352/938] [D loss: 0.6422449350357056] [G loss: 0.8106229305267334]\n",
      "[Epoch 4/5] [Batch 353/938] [D loss: 0.6107916831970215] [G loss: 0.812614381313324]\n",
      "[Epoch 4/5] [Batch 354/938] [D loss: 0.6652418375015259] [G loss: 0.9992886781692505]\n",
      "[Epoch 4/5] [Batch 355/938] [D loss: 0.6126307845115662] [G loss: 0.9074859619140625]\n",
      "[Epoch 4/5] [Batch 356/938] [D loss: 0.6335753202438354] [G loss: 0.6793954968452454]\n",
      "[Epoch 4/5] [Batch 357/938] [D loss: 0.5925576090812683] [G loss: 0.9220386743545532]\n",
      "[Epoch 4/5] [Batch 358/938] [D loss: 0.6394340991973877] [G loss: 1.0599452257156372]\n",
      "[Epoch 4/5] [Batch 359/938] [D loss: 0.5998579263687134] [G loss: 0.82015460729599]\n",
      "[Epoch 4/5] [Batch 360/938] [D loss: 0.5632508993148804] [G loss: 0.829798698425293]\n",
      "[Epoch 4/5] [Batch 361/938] [D loss: 0.63861483335495] [G loss: 0.9217901229858398]\n",
      "[Epoch 4/5] [Batch 362/938] [D loss: 0.5793449878692627] [G loss: 0.8567241430282593]\n",
      "[Epoch 4/5] [Batch 363/938] [D loss: 0.6125894784927368] [G loss: 0.8692815899848938]\n",
      "[Epoch 4/5] [Batch 364/938] [D loss: 0.5646305084228516] [G loss: 0.8851679563522339]\n",
      "[Epoch 4/5] [Batch 365/938] [D loss: 0.5749143958091736] [G loss: 0.9295275807380676]\n",
      "[Epoch 4/5] [Batch 366/938] [D loss: 0.6365183591842651] [G loss: 0.906379759311676]\n",
      "[Epoch 4/5] [Batch 367/938] [D loss: 0.6123799085617065] [G loss: 1.1588335037231445]\n",
      "[Epoch 4/5] [Batch 368/938] [D loss: 0.6330970525741577] [G loss: 0.7531300783157349]\n",
      "[Epoch 4/5] [Batch 369/938] [D loss: 0.5954116582870483] [G loss: 0.9741690754890442]\n",
      "[Epoch 4/5] [Batch 370/938] [D loss: 0.6298373937606812] [G loss: 0.9470838904380798]\n",
      "[Epoch 4/5] [Batch 371/938] [D loss: 0.6407902240753174] [G loss: 0.6730377674102783]\n",
      "[Epoch 4/5] [Batch 372/938] [D loss: 0.6548601388931274] [G loss: 1.2566566467285156]\n",
      "[Epoch 4/5] [Batch 373/938] [D loss: 0.5904152393341064] [G loss: 0.8368935585021973]\n",
      "[Epoch 4/5] [Batch 374/938] [D loss: 0.5775209665298462] [G loss: 0.8532055616378784]\n",
      "[Epoch 4/5] [Batch 375/938] [D loss: 0.6595970392227173] [G loss: 0.8619425296783447]\n",
      "[Epoch 4/5] [Batch 376/938] [D loss: 0.5690093040466309] [G loss: 0.9387866854667664]\n",
      "[Epoch 4/5] [Batch 377/938] [D loss: 0.6097166538238525] [G loss: 0.8995909690856934]\n",
      "[Epoch 4/5] [Batch 378/938] [D loss: 0.5938102006912231] [G loss: 0.8517342805862427]\n",
      "[Epoch 4/5] [Batch 379/938] [D loss: 0.601962685585022] [G loss: 1.134625792503357]\n",
      "[Epoch 4/5] [Batch 380/938] [D loss: 0.5769270062446594] [G loss: 0.9641066789627075]\n",
      "[Epoch 4/5] [Batch 381/938] [D loss: 0.6494457125663757] [G loss: 0.7997370362281799]\n",
      "[Epoch 4/5] [Batch 382/938] [D loss: 0.6203339099884033] [G loss: 1.3186510801315308]\n",
      "[Epoch 4/5] [Batch 383/938] [D loss: 0.6662039160728455] [G loss: 0.7316587567329407]\n",
      "[Epoch 4/5] [Batch 384/938] [D loss: 0.6083065271377563] [G loss: 0.8248745203018188]\n",
      "[Epoch 4/5] [Batch 385/938] [D loss: 0.6906746029853821] [G loss: 1.1000138521194458]\n",
      "[Epoch 4/5] [Batch 386/938] [D loss: 0.6387104988098145] [G loss: 0.7116138935089111]\n",
      "[Epoch 4/5] [Batch 387/938] [D loss: 0.6276278495788574] [G loss: 0.769756555557251]\n",
      "[Epoch 4/5] [Batch 388/938] [D loss: 0.6339657306671143] [G loss: 1.1855106353759766]\n",
      "[Epoch 4/5] [Batch 389/938] [D loss: 0.6392483711242676] [G loss: 0.8860668540000916]\n",
      "[Epoch 4/5] [Batch 390/938] [D loss: 0.6181889772415161] [G loss: 0.6568284630775452]\n",
      "[Epoch 4/5] [Batch 391/938] [D loss: 0.5670835375785828] [G loss: 0.9514986872673035]\n",
      "[Epoch 4/5] [Batch 392/938] [D loss: 0.6177449226379395] [G loss: 0.9780850410461426]\n",
      "[Epoch 4/5] [Batch 393/938] [D loss: 0.5806258320808411] [G loss: 0.8251628875732422]\n",
      "[Epoch 4/5] [Batch 394/938] [D loss: 0.5933157205581665] [G loss: 0.8327177166938782]\n",
      "[Epoch 4/5] [Batch 395/938] [D loss: 0.6873939633369446] [G loss: 1.0014959573745728]\n",
      "[Epoch 4/5] [Batch 396/938] [D loss: 0.5912622213363647] [G loss: 0.8474863767623901]\n",
      "[Epoch 4/5] [Batch 397/938] [D loss: 0.6085796356201172] [G loss: 0.8958603143692017]\n",
      "[Epoch 4/5] [Batch 398/938] [D loss: 0.6097344160079956] [G loss: 1.011509656906128]\n",
      "[Epoch 4/5] [Batch 399/938] [D loss: 0.6435388326644897] [G loss: 0.902279257774353]\n",
      "[Epoch 4/5] [Batch 400/938] [D loss: 0.5899094939231873] [G loss: 0.9602543115615845]\n",
      "[Epoch 4/5] [Batch 401/938] [D loss: 0.628790557384491] [G loss: 0.9021007418632507]\n",
      "[Epoch 4/5] [Batch 402/938] [D loss: 0.6803677082061768] [G loss: 1.059285283088684]\n",
      "[Epoch 4/5] [Batch 403/938] [D loss: 0.5968512296676636] [G loss: 0.7256065607070923]\n",
      "[Epoch 4/5] [Batch 404/938] [D loss: 0.6390156745910645] [G loss: 0.7991610765457153]\n",
      "[Epoch 4/5] [Batch 405/938] [D loss: 0.6310214400291443] [G loss: 0.9853739738464355]\n",
      "[Epoch 4/5] [Batch 406/938] [D loss: 0.643871545791626] [G loss: 0.8732441663742065]\n",
      "[Epoch 4/5] [Batch 407/938] [D loss: 0.6710048317909241] [G loss: 0.9050092697143555]\n",
      "[Epoch 4/5] [Batch 408/938] [D loss: 0.6303235292434692] [G loss: 0.9020462036132812]\n",
      "[Epoch 4/5] [Batch 409/938] [D loss: 0.5900514125823975] [G loss: 0.8060096502304077]\n",
      "[Epoch 4/5] [Batch 410/938] [D loss: 0.6154588460922241] [G loss: 0.7718340158462524]\n",
      "[Epoch 4/5] [Batch 411/938] [D loss: 0.5888980627059937] [G loss: 0.8575806021690369]\n",
      "[Epoch 4/5] [Batch 412/938] [D loss: 0.5405095219612122] [G loss: 0.8579959869384766]\n",
      "[Epoch 4/5] [Batch 413/938] [D loss: 0.6594340801239014] [G loss: 0.8960505127906799]\n",
      "[Epoch 4/5] [Batch 414/938] [D loss: 0.6392342448234558] [G loss: 0.742510199546814]\n",
      "[Epoch 4/5] [Batch 415/938] [D loss: 0.6550772190093994] [G loss: 1.2519577741622925]\n",
      "[Epoch 4/5] [Batch 416/938] [D loss: 0.6284971237182617] [G loss: 0.7747647166252136]\n",
      "[Epoch 4/5] [Batch 417/938] [D loss: 0.648099422454834] [G loss: 0.7887779474258423]\n",
      "[Epoch 4/5] [Batch 418/938] [D loss: 0.6246415376663208] [G loss: 1.0477027893066406]\n",
      "[Epoch 4/5] [Batch 419/938] [D loss: 0.6006689667701721] [G loss: 0.941791832447052]\n",
      "[Epoch 4/5] [Batch 420/938] [D loss: 0.641617476940155] [G loss: 0.7497370839118958]\n",
      "[Epoch 4/5] [Batch 421/938] [D loss: 0.6290655136108398] [G loss: 0.9671965837478638]\n",
      "[Epoch 4/5] [Batch 422/938] [D loss: 0.6330820322036743] [G loss: 0.9657732248306274]\n",
      "[Epoch 4/5] [Batch 423/938] [D loss: 0.6557394862174988] [G loss: 0.8533127903938293]\n",
      "[Epoch 4/5] [Batch 424/938] [D loss: 0.645007848739624] [G loss: 0.9022161960601807]\n",
      "[Epoch 4/5] [Batch 425/938] [D loss: 0.6220319271087646] [G loss: 1.0013983249664307]\n",
      "[Epoch 4/5] [Batch 426/938] [D loss: 0.5775421857833862] [G loss: 0.749838650226593]\n",
      "[Epoch 4/5] [Batch 427/938] [D loss: 0.5904179215431213] [G loss: 1.097801685333252]\n",
      "[Epoch 4/5] [Batch 428/938] [D loss: 0.6014773845672607] [G loss: 0.7804995775222778]\n",
      "[Epoch 4/5] [Batch 429/938] [D loss: 0.664307713508606] [G loss: 0.8656893372535706]\n",
      "[Epoch 4/5] [Batch 430/938] [D loss: 0.6364278197288513] [G loss: 0.9288522601127625]\n",
      "[Epoch 4/5] [Batch 431/938] [D loss: 0.6459918022155762] [G loss: 0.8753600120544434]\n",
      "[Epoch 4/5] [Batch 432/938] [D loss: 0.652121901512146] [G loss: 1.006508708000183]\n",
      "[Epoch 4/5] [Batch 433/938] [D loss: 0.6447939872741699] [G loss: 0.9454678297042847]\n",
      "[Epoch 4/5] [Batch 434/938] [D loss: 0.635689914226532] [G loss: 0.8811410665512085]\n",
      "[Epoch 4/5] [Batch 435/938] [D loss: 0.6331033706665039] [G loss: 0.8070610761642456]\n",
      "[Epoch 4/5] [Batch 436/938] [D loss: 0.6680963039398193] [G loss: 1.183754801750183]\n",
      "[Epoch 4/5] [Batch 437/938] [D loss: 0.6731131076812744] [G loss: 0.7995851635932922]\n",
      "[Epoch 4/5] [Batch 438/938] [D loss: 0.65268874168396] [G loss: 0.8017492890357971]\n",
      "[Epoch 4/5] [Batch 439/938] [D loss: 0.582931637763977] [G loss: 1.0444637537002563]\n",
      "[Epoch 4/5] [Batch 440/938] [D loss: 0.6266975402832031] [G loss: 0.8670575618743896]\n",
      "[Epoch 4/5] [Batch 441/938] [D loss: 0.5916880965232849] [G loss: 0.7908344268798828]\n",
      "[Epoch 4/5] [Batch 442/938] [D loss: 0.5910439491271973] [G loss: 0.9338917136192322]\n",
      "[Epoch 4/5] [Batch 443/938] [D loss: 0.602712869644165] [G loss: 0.8906573057174683]\n",
      "[Epoch 4/5] [Batch 444/938] [D loss: 0.6568912267684937] [G loss: 1.024202585220337]\n",
      "[Epoch 4/5] [Batch 445/938] [D loss: 0.6465615034103394] [G loss: 0.7723886370658875]\n",
      "[Epoch 4/5] [Batch 446/938] [D loss: 0.6644207835197449] [G loss: 1.1744877099990845]\n",
      "[Epoch 4/5] [Batch 447/938] [D loss: 0.5709984302520752] [G loss: 0.8439900279045105]\n",
      "[Epoch 4/5] [Batch 448/938] [D loss: 0.617090106010437] [G loss: 0.8276035785675049]\n",
      "[Epoch 4/5] [Batch 449/938] [D loss: 0.6384705305099487] [G loss: 0.8638123273849487]\n",
      "[Epoch 4/5] [Batch 450/938] [D loss: 0.6262797713279724] [G loss: 0.843696117401123]\n",
      "[Epoch 4/5] [Batch 451/938] [D loss: 0.5979916453361511] [G loss: 1.0549417734146118]\n",
      "[Epoch 4/5] [Batch 452/938] [D loss: 0.6808070540428162] [G loss: 0.708314836025238]\n",
      "[Epoch 4/5] [Batch 453/938] [D loss: 0.6420853137969971] [G loss: 0.875850260257721]\n",
      "[Epoch 4/5] [Batch 454/938] [D loss: 0.6191160082817078] [G loss: 1.1871775388717651]\n",
      "[Epoch 4/5] [Batch 455/938] [D loss: 0.6535125970840454] [G loss: 0.7568017244338989]\n",
      "[Epoch 4/5] [Batch 456/938] [D loss: 0.6124107837677002] [G loss: 0.793218731880188]\n",
      "[Epoch 4/5] [Batch 457/938] [D loss: 0.6390189528465271] [G loss: 0.9160990118980408]\n",
      "[Epoch 4/5] [Batch 458/938] [D loss: 0.628058910369873] [G loss: 0.9989616870880127]\n",
      "[Epoch 4/5] [Batch 459/938] [D loss: 0.571539044380188] [G loss: 0.8292603492736816]\n",
      "[Epoch 4/5] [Batch 460/938] [D loss: 0.5835915803909302] [G loss: 0.8650200366973877]\n",
      "[Epoch 4/5] [Batch 461/938] [D loss: 0.633419394493103] [G loss: 0.9710788726806641]\n",
      "[Epoch 4/5] [Batch 462/938] [D loss: 0.6236881017684937] [G loss: 1.02193021774292]\n",
      "[Epoch 4/5] [Batch 463/938] [D loss: 0.604243814945221] [G loss: 0.8340506553649902]\n",
      "[Epoch 4/5] [Batch 464/938] [D loss: 0.6042190194129944] [G loss: 0.7711447477340698]\n",
      "[Epoch 4/5] [Batch 465/938] [D loss: 0.6241514682769775] [G loss: 0.8722634315490723]\n",
      "[Epoch 4/5] [Batch 466/938] [D loss: 0.6314533352851868] [G loss: 1.0651955604553223]\n",
      "[Epoch 4/5] [Batch 467/938] [D loss: 0.6339923143386841] [G loss: 0.9945762753486633]\n",
      "[Epoch 4/5] [Batch 468/938] [D loss: 0.5753174424171448] [G loss: 0.7803956866264343]\n",
      "[Epoch 4/5] [Batch 469/938] [D loss: 0.6640434265136719] [G loss: 1.002921223640442]\n",
      "[Epoch 4/5] [Batch 470/938] [D loss: 0.5936927199363708] [G loss: 1.1311635971069336]\n",
      "[Epoch 4/5] [Batch 471/938] [D loss: 0.6117878556251526] [G loss: 0.7580935955047607]\n",
      "[Epoch 4/5] [Batch 472/938] [D loss: 0.6169047355651855] [G loss: 0.8252245187759399]\n",
      "[Epoch 4/5] [Batch 473/938] [D loss: 0.5756539106369019] [G loss: 0.9787590503692627]\n",
      "[Epoch 4/5] [Batch 474/938] [D loss: 0.6337591409683228] [G loss: 0.773076057434082]\n",
      "[Epoch 4/5] [Batch 475/938] [D loss: 0.5788271427154541] [G loss: 0.9983918070793152]\n",
      "[Epoch 4/5] [Batch 476/938] [D loss: 0.6081328392028809] [G loss: 0.8886271715164185]\n",
      "[Epoch 4/5] [Batch 477/938] [D loss: 0.6296757459640503] [G loss: 0.9183123111724854]\n",
      "[Epoch 4/5] [Batch 478/938] [D loss: 0.6169382929801941] [G loss: 0.8594016432762146]\n",
      "[Epoch 4/5] [Batch 479/938] [D loss: 0.6528257727622986] [G loss: 1.0491687059402466]\n",
      "[Epoch 4/5] [Batch 480/938] [D loss: 0.5957831144332886] [G loss: 0.8491301536560059]\n",
      "[Epoch 4/5] [Batch 481/938] [D loss: 0.6320969462394714] [G loss: 0.8037439584732056]\n",
      "[Epoch 4/5] [Batch 482/938] [D loss: 0.6983096599578857] [G loss: 0.800888180732727]\n",
      "[Epoch 4/5] [Batch 483/938] [D loss: 0.5834673643112183] [G loss: 0.9850495457649231]\n",
      "[Epoch 4/5] [Batch 484/938] [D loss: 0.6146637201309204] [G loss: 0.7692751288414001]\n",
      "[Epoch 4/5] [Batch 485/938] [D loss: 0.6221697330474854] [G loss: 0.9417608976364136]\n",
      "[Epoch 4/5] [Batch 486/938] [D loss: 0.5799959301948547] [G loss: 0.9505667090415955]\n",
      "[Epoch 4/5] [Batch 487/938] [D loss: 0.5814501047134399] [G loss: 0.9464532732963562]\n",
      "[Epoch 4/5] [Batch 488/938] [D loss: 0.674493670463562] [G loss: 0.8607889413833618]\n",
      "[Epoch 4/5] [Batch 489/938] [D loss: 0.6358810663223267] [G loss: 0.997437059879303]\n",
      "[Epoch 4/5] [Batch 490/938] [D loss: 0.6331882476806641] [G loss: 0.9795824885368347]\n",
      "[Epoch 4/5] [Batch 491/938] [D loss: 0.6609827876091003] [G loss: 0.6661866903305054]\n",
      "[Epoch 4/5] [Batch 492/938] [D loss: 0.6080536842346191] [G loss: 0.9651616811752319]\n",
      "[Epoch 4/5] [Batch 493/938] [D loss: 0.5871213674545288] [G loss: 1.0860272645950317]\n",
      "[Epoch 4/5] [Batch 494/938] [D loss: 0.6484755277633667] [G loss: 0.8088889122009277]\n",
      "[Epoch 4/5] [Batch 495/938] [D loss: 0.6426999568939209] [G loss: 0.8352592587471008]\n",
      "[Epoch 4/5] [Batch 496/938] [D loss: 0.6155309677124023] [G loss: 0.9241040349006653]\n",
      "[Epoch 4/5] [Batch 497/938] [D loss: 0.6284598112106323] [G loss: 0.912558913230896]\n",
      "[Epoch 4/5] [Batch 498/938] [D loss: 0.597178041934967] [G loss: 0.9128684997558594]\n",
      "[Epoch 4/5] [Batch 499/938] [D loss: 0.6025787591934204] [G loss: 0.8072049617767334]\n",
      "[Epoch 4/5] [Batch 500/938] [D loss: 0.6108802556991577] [G loss: 1.035500168800354]\n",
      "[Epoch 4/5] [Batch 501/938] [D loss: 0.6088550090789795] [G loss: 0.9695440530776978]\n",
      "[Epoch 4/5] [Batch 502/938] [D loss: 0.6260671615600586] [G loss: 0.8282114863395691]\n",
      "[Epoch 4/5] [Batch 503/938] [D loss: 0.6274505257606506] [G loss: 0.9089660048484802]\n",
      "[Epoch 4/5] [Batch 504/938] [D loss: 0.6109898090362549] [G loss: 1.0054562091827393]\n",
      "[Epoch 4/5] [Batch 505/938] [D loss: 0.6314369440078735] [G loss: 0.8243982195854187]\n",
      "[Epoch 4/5] [Batch 506/938] [D loss: 0.5711759924888611] [G loss: 0.9328434467315674]\n",
      "[Epoch 4/5] [Batch 507/938] [D loss: 0.6273468136787415] [G loss: 0.9275740385055542]\n",
      "[Epoch 4/5] [Batch 508/938] [D loss: 0.641268789768219] [G loss: 0.8822818398475647]\n",
      "[Epoch 4/5] [Batch 509/938] [D loss: 0.6102795600891113] [G loss: 1.0277433395385742]\n",
      "[Epoch 4/5] [Batch 510/938] [D loss: 0.595113217830658] [G loss: 0.771370530128479]\n",
      "[Epoch 4/5] [Batch 511/938] [D loss: 0.6006351709365845] [G loss: 0.7919553518295288]\n",
      "[Epoch 4/5] [Batch 512/938] [D loss: 0.6595950126647949] [G loss: 1.1149117946624756]\n",
      "[Epoch 4/5] [Batch 513/938] [D loss: 0.5764642357826233] [G loss: 0.8095237016677856]\n",
      "[Epoch 4/5] [Batch 514/938] [D loss: 0.6321830749511719] [G loss: 0.8708012104034424]\n",
      "[Epoch 4/5] [Batch 515/938] [D loss: 0.6213967800140381] [G loss: 0.8261553645133972]\n",
      "[Epoch 4/5] [Batch 516/938] [D loss: 0.6301459670066833] [G loss: 1.0386667251586914]\n",
      "[Epoch 4/5] [Batch 517/938] [D loss: 0.6527577042579651] [G loss: 0.8741750717163086]\n",
      "[Epoch 4/5] [Batch 518/938] [D loss: 0.6470645666122437] [G loss: 0.8941106200218201]\n",
      "[Epoch 4/5] [Batch 519/938] [D loss: 0.6295756101608276] [G loss: 1.1345317363739014]\n",
      "[Epoch 4/5] [Batch 520/938] [D loss: 0.6108783483505249] [G loss: 0.8160037398338318]\n",
      "[Epoch 4/5] [Batch 521/938] [D loss: 0.6585857272148132] [G loss: 0.8433002233505249]\n",
      "[Epoch 4/5] [Batch 522/938] [D loss: 0.5988390445709229] [G loss: 0.8665354251861572]\n",
      "[Epoch 4/5] [Batch 523/938] [D loss: 0.6737833023071289] [G loss: 1.1522504091262817]\n",
      "[Epoch 4/5] [Batch 524/938] [D loss: 0.6323985457420349] [G loss: 0.672175407409668]\n",
      "[Epoch 4/5] [Batch 525/938] [D loss: 0.6122227907180786] [G loss: 0.9322497248649597]\n",
      "[Epoch 4/5] [Batch 526/938] [D loss: 0.6106177568435669] [G loss: 0.9656146168708801]\n",
      "[Epoch 4/5] [Batch 527/938] [D loss: 0.6962485909461975] [G loss: 0.819309651851654]\n",
      "[Epoch 4/5] [Batch 528/938] [D loss: 0.6243618726730347] [G loss: 0.9271364212036133]\n",
      "[Epoch 4/5] [Batch 529/938] [D loss: 0.6153826713562012] [G loss: 1.1121820211410522]\n",
      "[Epoch 4/5] [Batch 530/938] [D loss: 0.6095033288002014] [G loss: 1.091284990310669]\n",
      "[Epoch 4/5] [Batch 531/938] [D loss: 0.6328082084655762] [G loss: 0.6957237124443054]\n",
      "[Epoch 4/5] [Batch 532/938] [D loss: 0.63897705078125] [G loss: 0.9671545624732971]\n",
      "[Epoch 4/5] [Batch 533/938] [D loss: 0.601370632648468] [G loss: 0.9723088145256042]\n",
      "[Epoch 4/5] [Batch 534/938] [D loss: 0.6086544990539551] [G loss: 0.7743407487869263]\n",
      "[Epoch 4/5] [Batch 535/938] [D loss: 0.5950753688812256] [G loss: 0.932900071144104]\n",
      "[Epoch 4/5] [Batch 536/938] [D loss: 0.62361741065979] [G loss: 0.7453016638755798]\n",
      "[Epoch 4/5] [Batch 537/938] [D loss: 0.6107112169265747] [G loss: 0.9487756490707397]\n",
      "[Epoch 4/5] [Batch 538/938] [D loss: 0.5736702084541321] [G loss: 0.9386383891105652]\n",
      "[Epoch 4/5] [Batch 539/938] [D loss: 0.5850487351417542] [G loss: 1.085971474647522]\n",
      "[Epoch 4/5] [Batch 540/938] [D loss: 0.6009527444839478] [G loss: 0.7684204578399658]\n",
      "[Epoch 4/5] [Batch 541/938] [D loss: 0.5883288383483887] [G loss: 0.8963415622711182]\n",
      "[Epoch 4/5] [Batch 542/938] [D loss: 0.6517210602760315] [G loss: 1.0737651586532593]\n",
      "[Epoch 4/5] [Batch 543/938] [D loss: 0.6116453409194946] [G loss: 0.7453513145446777]\n",
      "[Epoch 4/5] [Batch 544/938] [D loss: 0.654444694519043] [G loss: 0.9427478909492493]\n",
      "[Epoch 4/5] [Batch 545/938] [D loss: 0.5816541314125061] [G loss: 0.9161933660507202]\n",
      "[Epoch 4/5] [Batch 546/938] [D loss: 0.5581786632537842] [G loss: 0.8781253099441528]\n",
      "[Epoch 4/5] [Batch 547/938] [D loss: 0.6283535957336426] [G loss: 0.9291538596153259]\n",
      "[Epoch 4/5] [Batch 548/938] [D loss: 0.5920581221580505] [G loss: 1.0356041193008423]\n",
      "[Epoch 4/5] [Batch 549/938] [D loss: 0.7314950227737427] [G loss: 0.820703387260437]\n",
      "[Epoch 4/5] [Batch 550/938] [D loss: 0.6209118366241455] [G loss: 1.248595952987671]\n",
      "[Epoch 4/5] [Batch 551/938] [D loss: 0.6401693820953369] [G loss: 0.7084611654281616]\n",
      "[Epoch 4/5] [Batch 552/938] [D loss: 0.5815807580947876] [G loss: 0.8980730772018433]\n",
      "[Epoch 4/5] [Batch 553/938] [D loss: 0.5993073582649231] [G loss: 1.1412497758865356]\n",
      "[Epoch 4/5] [Batch 554/938] [D loss: 0.6176158785820007] [G loss: 0.6982560753822327]\n",
      "[Epoch 4/5] [Batch 555/938] [D loss: 0.6190574169158936] [G loss: 0.9515483379364014]\n",
      "[Epoch 4/5] [Batch 556/938] [D loss: 0.6547033786773682] [G loss: 0.9987407922744751]\n",
      "[Epoch 4/5] [Batch 557/938] [D loss: 0.6164951920509338] [G loss: 0.7731558680534363]\n",
      "[Epoch 4/5] [Batch 558/938] [D loss: 0.6386013627052307] [G loss: 0.7127524018287659]\n",
      "[Epoch 4/5] [Batch 559/938] [D loss: 0.6592629551887512] [G loss: 0.970458984375]\n",
      "[Epoch 4/5] [Batch 560/938] [D loss: 0.6395076513290405] [G loss: 0.9209520220756531]\n",
      "[Epoch 4/5] [Batch 561/938] [D loss: 0.614230751991272] [G loss: 0.8544931411743164]\n",
      "[Epoch 4/5] [Batch 562/938] [D loss: 0.6049454212188721] [G loss: 0.8986069560050964]\n",
      "[Epoch 4/5] [Batch 563/938] [D loss: 0.6086164712905884] [G loss: 0.958162248134613]\n",
      "[Epoch 4/5] [Batch 564/938] [D loss: 0.616945743560791] [G loss: 0.8242677450180054]\n",
      "[Epoch 4/5] [Batch 565/938] [D loss: 0.6110045909881592] [G loss: 0.9919964075088501]\n",
      "[Epoch 4/5] [Batch 566/938] [D loss: 0.5978572368621826] [G loss: 0.9886086583137512]\n",
      "[Epoch 4/5] [Batch 567/938] [D loss: 0.6365975141525269] [G loss: 0.7648323178291321]\n",
      "[Epoch 4/5] [Batch 568/938] [D loss: 0.5983749628067017] [G loss: 1.0689303874969482]\n",
      "[Epoch 4/5] [Batch 569/938] [D loss: 0.6612247824668884] [G loss: 0.7888802289962769]\n",
      "[Epoch 4/5] [Batch 570/938] [D loss: 0.6376371383666992] [G loss: 0.8620254993438721]\n",
      "[Epoch 4/5] [Batch 571/938] [D loss: 0.6048280000686646] [G loss: 0.881208598613739]\n",
      "[Epoch 4/5] [Batch 572/938] [D loss: 0.6035939455032349] [G loss: 0.9170244932174683]\n",
      "[Epoch 4/5] [Batch 573/938] [D loss: 0.6509256362915039] [G loss: 0.9608498811721802]\n",
      "[Epoch 4/5] [Batch 574/938] [D loss: 0.6379607915878296] [G loss: 0.8023752570152283]\n",
      "[Epoch 4/5] [Batch 575/938] [D loss: 0.5958304405212402] [G loss: 0.9805289506912231]\n",
      "[Epoch 4/5] [Batch 576/938] [D loss: 0.5525835752487183] [G loss: 0.9754169583320618]\n",
      "[Epoch 4/5] [Batch 577/938] [D loss: 0.6291546821594238] [G loss: 0.6687147617340088]\n",
      "[Epoch 4/5] [Batch 578/938] [D loss: 0.6501333713531494] [G loss: 1.1712348461151123]\n",
      "[Epoch 4/5] [Batch 579/938] [D loss: 0.7271076440811157] [G loss: 0.6776936650276184]\n",
      "[Epoch 4/5] [Batch 580/938] [D loss: 0.6316467523574829] [G loss: 0.9489831328392029]\n",
      "[Epoch 4/5] [Batch 581/938] [D loss: 0.6200204491615295] [G loss: 0.9509321451187134]\n",
      "[Epoch 4/5] [Batch 582/938] [D loss: 0.5897635221481323] [G loss: 0.7919435501098633]\n",
      "[Epoch 4/5] [Batch 583/938] [D loss: 0.6172407865524292] [G loss: 0.9006645083427429]\n",
      "[Epoch 4/5] [Batch 584/938] [D loss: 0.5696223974227905] [G loss: 1.0074559450149536]\n",
      "[Epoch 4/5] [Batch 585/938] [D loss: 0.6299788951873779] [G loss: 0.8171262145042419]\n",
      "[Epoch 4/5] [Batch 586/938] [D loss: 0.6113122701644897] [G loss: 0.9410538673400879]\n",
      "[Epoch 4/5] [Batch 587/938] [D loss: 0.6331863403320312] [G loss: 1.017101526260376]\n",
      "[Epoch 4/5] [Batch 588/938] [D loss: 0.6342483162879944] [G loss: 0.8254515528678894]\n",
      "[Epoch 4/5] [Batch 589/938] [D loss: 0.6636388301849365] [G loss: 0.8984932899475098]\n",
      "[Epoch 4/5] [Batch 590/938] [D loss: 0.6355519890785217] [G loss: 0.8159012794494629]\n",
      "[Epoch 4/5] [Batch 591/938] [D loss: 0.6185117363929749] [G loss: 0.8376510143280029]\n",
      "[Epoch 4/5] [Batch 592/938] [D loss: 0.6038463115692139] [G loss: 0.8782429695129395]\n",
      "[Epoch 4/5] [Batch 593/938] [D loss: 0.6412904262542725] [G loss: 0.9661973714828491]\n",
      "[Epoch 4/5] [Batch 594/938] [D loss: 0.6325240135192871] [G loss: 1.0500987768173218]\n",
      "[Epoch 4/5] [Batch 595/938] [D loss: 0.6961439847946167] [G loss: 0.8130836486816406]\n",
      "[Epoch 4/5] [Batch 596/938] [D loss: 0.6013933420181274] [G loss: 0.9718412160873413]\n",
      "[Epoch 4/5] [Batch 597/938] [D loss: 0.6130886077880859] [G loss: 0.9043344855308533]\n",
      "[Epoch 4/5] [Batch 598/938] [D loss: 0.5952032804489136] [G loss: 0.8401291370391846]\n",
      "[Epoch 4/5] [Batch 599/938] [D loss: 0.6251189708709717] [G loss: 0.9385824203491211]\n",
      "[Epoch 4/5] [Batch 600/938] [D loss: 0.6606804728507996] [G loss: 0.9130228757858276]\n",
      "[Epoch 4/5] [Batch 601/938] [D loss: 0.6876980662345886] [G loss: 0.7862541675567627]\n",
      "[Epoch 4/5] [Batch 602/938] [D loss: 0.66202712059021] [G loss: 0.8829842805862427]\n",
      "[Epoch 4/5] [Batch 603/938] [D loss: 0.59085613489151] [G loss: 0.9741188287734985]\n",
      "[Epoch 4/5] [Batch 604/938] [D loss: 0.6063185334205627] [G loss: 0.8752429485321045]\n",
      "[Epoch 4/5] [Batch 605/938] [D loss: 0.6889052391052246] [G loss: 0.8880990147590637]\n",
      "[Epoch 4/5] [Batch 606/938] [D loss: 0.6781044006347656] [G loss: 0.8052613139152527]\n",
      "[Epoch 4/5] [Batch 607/938] [D loss: 0.6483669281005859] [G loss: 0.8990055918693542]\n",
      "[Epoch 4/5] [Batch 608/938] [D loss: 0.6183547973632812] [G loss: 1.0179286003112793]\n",
      "[Epoch 4/5] [Batch 609/938] [D loss: 0.6331017017364502] [G loss: 0.7992438077926636]\n",
      "[Epoch 4/5] [Batch 610/938] [D loss: 0.6060963273048401] [G loss: 0.8785300254821777]\n",
      "[Epoch 4/5] [Batch 611/938] [D loss: 0.6272845268249512] [G loss: 1.0732916593551636]\n",
      "[Epoch 4/5] [Batch 612/938] [D loss: 0.6534212827682495] [G loss: 0.744689404964447]\n",
      "[Epoch 4/5] [Batch 613/938] [D loss: 0.624839186668396] [G loss: 0.8612253069877625]\n",
      "[Epoch 4/5] [Batch 614/938] [D loss: 0.639005720615387] [G loss: 0.9077368378639221]\n",
      "[Epoch 4/5] [Batch 615/938] [D loss: 0.6327991485595703] [G loss: 0.863336980342865]\n",
      "[Epoch 4/5] [Batch 616/938] [D loss: 0.598524808883667] [G loss: 0.8334568738937378]\n",
      "[Epoch 4/5] [Batch 617/938] [D loss: 0.6585796475410461] [G loss: 0.8917467594146729]\n",
      "[Epoch 4/5] [Batch 618/938] [D loss: 0.6195166707038879] [G loss: 1.1967072486877441]\n",
      "[Epoch 4/5] [Batch 619/938] [D loss: 0.6711979508399963] [G loss: 0.6917362213134766]\n",
      "[Epoch 4/5] [Batch 620/938] [D loss: 0.6628005504608154] [G loss: 1.0305017232894897]\n",
      "[Epoch 4/5] [Batch 621/938] [D loss: 0.6018298864364624] [G loss: 0.8707512021064758]\n",
      "[Epoch 4/5] [Batch 622/938] [D loss: 0.6357424259185791] [G loss: 0.8015184998512268]\n",
      "[Epoch 4/5] [Batch 623/938] [D loss: 0.6308492422103882] [G loss: 0.9972930550575256]\n",
      "[Epoch 4/5] [Batch 624/938] [D loss: 0.6249067187309265] [G loss: 0.9209308624267578]\n",
      "[Epoch 4/5] [Batch 625/938] [D loss: 0.6187625527381897] [G loss: 0.89836585521698]\n",
      "[Epoch 4/5] [Batch 626/938] [D loss: 0.6404728889465332] [G loss: 0.9929337501525879]\n",
      "[Epoch 4/5] [Batch 627/938] [D loss: 0.581825315952301] [G loss: 0.9566226005554199]\n",
      "[Epoch 4/5] [Batch 628/938] [D loss: 0.5850822925567627] [G loss: 0.7386329770088196]\n",
      "[Epoch 4/5] [Batch 629/938] [D loss: 0.5986230373382568] [G loss: 0.8880603909492493]\n",
      "[Epoch 4/5] [Batch 630/938] [D loss: 0.6333378553390503] [G loss: 1.0567913055419922]\n",
      "[Epoch 4/5] [Batch 631/938] [D loss: 0.6464512944221497] [G loss: 0.7551253437995911]\n",
      "[Epoch 4/5] [Batch 632/938] [D loss: 0.6559994220733643] [G loss: 0.7999558448791504]\n",
      "[Epoch 4/5] [Batch 633/938] [D loss: 0.655460000038147] [G loss: 1.0722225904464722]\n",
      "[Epoch 4/5] [Batch 634/938] [D loss: 0.6292716264724731] [G loss: 0.7293376326560974]\n",
      "[Epoch 4/5] [Batch 635/938] [D loss: 0.5908602476119995] [G loss: 0.9371253252029419]\n",
      "[Epoch 4/5] [Batch 636/938] [D loss: 0.620322585105896] [G loss: 0.9149989485740662]\n",
      "[Epoch 4/5] [Batch 637/938] [D loss: 0.6028751134872437] [G loss: 0.9773431420326233]\n",
      "[Epoch 4/5] [Batch 638/938] [D loss: 0.6921255588531494] [G loss: 0.880513072013855]\n",
      "[Epoch 4/5] [Batch 639/938] [D loss: 0.6424502730369568] [G loss: 1.027670979499817]\n",
      "[Epoch 4/5] [Batch 640/938] [D loss: 0.60155189037323] [G loss: 0.7926228642463684]\n",
      "[Epoch 4/5] [Batch 641/938] [D loss: 0.6325336694717407] [G loss: 0.978073239326477]\n",
      "[Epoch 4/5] [Batch 642/938] [D loss: 0.6420736312866211] [G loss: 0.9195722341537476]\n",
      "[Epoch 4/5] [Batch 643/938] [D loss: 0.588657021522522] [G loss: 0.8280801773071289]\n",
      "[Epoch 4/5] [Batch 644/938] [D loss: 0.6205743551254272] [G loss: 0.7678524255752563]\n",
      "[Epoch 4/5] [Batch 645/938] [D loss: 0.6543941497802734] [G loss: 0.985476553440094]\n",
      "[Epoch 4/5] [Batch 646/938] [D loss: 0.6430703401565552] [G loss: 0.7760776877403259]\n",
      "[Epoch 4/5] [Batch 647/938] [D loss: 0.6771256923675537] [G loss: 1.0312724113464355]\n",
      "[Epoch 4/5] [Batch 648/938] [D loss: 0.5742828249931335] [G loss: 0.8267673254013062]\n",
      "[Epoch 4/5] [Batch 649/938] [D loss: 0.6275545358657837] [G loss: 0.8603150248527527]\n",
      "[Epoch 4/5] [Batch 650/938] [D loss: 0.6980197429656982] [G loss: 0.9131583571434021]\n",
      "[Epoch 4/5] [Batch 651/938] [D loss: 0.6674474477767944] [G loss: 0.9349576234817505]\n",
      "[Epoch 4/5] [Batch 652/938] [D loss: 0.679735541343689] [G loss: 0.7465869784355164]\n",
      "[Epoch 4/5] [Batch 653/938] [D loss: 0.6868445873260498] [G loss: 0.9303508996963501]\n",
      "[Epoch 4/5] [Batch 654/938] [D loss: 0.6277488470077515] [G loss: 0.9232557415962219]\n",
      "[Epoch 4/5] [Batch 655/938] [D loss: 0.6377711296081543] [G loss: 0.8505860567092896]\n",
      "[Epoch 4/5] [Batch 656/938] [D loss: 0.6063823699951172] [G loss: 0.8873756527900696]\n",
      "[Epoch 4/5] [Batch 657/938] [D loss: 0.6317625045776367] [G loss: 0.8890435695648193]\n",
      "[Epoch 4/5] [Batch 658/938] [D loss: 0.6028494834899902] [G loss: 0.8896706700325012]\n",
      "[Epoch 4/5] [Batch 659/938] [D loss: 0.627768337726593] [G loss: 0.8694058656692505]\n",
      "[Epoch 4/5] [Batch 660/938] [D loss: 0.6459828615188599] [G loss: 0.7902530431747437]\n",
      "[Epoch 4/5] [Batch 661/938] [D loss: 0.6831002235412598] [G loss: 0.9237594604492188]\n",
      "[Epoch 4/5] [Batch 662/938] [D loss: 0.6519309282302856] [G loss: 0.8290253281593323]\n",
      "[Epoch 4/5] [Batch 663/938] [D loss: 0.6563811898231506] [G loss: 0.783259391784668]\n",
      "[Epoch 4/5] [Batch 664/938] [D loss: 0.6494531631469727] [G loss: 0.941394567489624]\n",
      "[Epoch 4/5] [Batch 665/938] [D loss: 0.6328036785125732] [G loss: 0.8850847482681274]\n",
      "[Epoch 4/5] [Batch 666/938] [D loss: 0.6457723379135132] [G loss: 0.877720296382904]\n",
      "[Epoch 4/5] [Batch 667/938] [D loss: 0.631761908531189] [G loss: 0.7842840552330017]\n",
      "[Epoch 4/5] [Batch 668/938] [D loss: 0.6163715124130249] [G loss: 0.9244871139526367]\n",
      "[Epoch 4/5] [Batch 669/938] [D loss: 0.6128747463226318] [G loss: 0.785866379737854]\n",
      "[Epoch 4/5] [Batch 670/938] [D loss: 0.6403957009315491] [G loss: 0.8565183281898499]\n",
      "[Epoch 4/5] [Batch 671/938] [D loss: 0.6162294149398804] [G loss: 0.9144695401191711]\n",
      "[Epoch 4/5] [Batch 672/938] [D loss: 0.6064889430999756] [G loss: 0.9066076278686523]\n",
      "[Epoch 4/5] [Batch 673/938] [D loss: 0.6415712833404541] [G loss: 0.8271996974945068]\n",
      "[Epoch 4/5] [Batch 674/938] [D loss: 0.6056783199310303] [G loss: 0.826650857925415]\n",
      "[Epoch 4/5] [Batch 675/938] [D loss: 0.6272971630096436] [G loss: 0.8816705346107483]\n",
      "[Epoch 4/5] [Batch 676/938] [D loss: 0.7144825458526611] [G loss: 0.9379227161407471]\n",
      "[Epoch 4/5] [Batch 677/938] [D loss: 0.5982990264892578] [G loss: 0.894957423210144]\n",
      "[Epoch 4/5] [Batch 678/938] [D loss: 0.6270921230316162] [G loss: 0.8243362903594971]\n",
      "[Epoch 4/5] [Batch 679/938] [D loss: 0.6026514768600464] [G loss: 0.9180665016174316]\n",
      "[Epoch 4/5] [Batch 680/938] [D loss: 0.6130955219268799] [G loss: 0.8078049421310425]\n",
      "[Epoch 4/5] [Batch 681/938] [D loss: 0.6296093463897705] [G loss: 0.9463111758232117]\n",
      "[Epoch 4/5] [Batch 682/938] [D loss: 0.5867418050765991] [G loss: 0.8515801429748535]\n",
      "[Epoch 4/5] [Batch 683/938] [D loss: 0.6306279897689819] [G loss: 0.9173043966293335]\n",
      "[Epoch 4/5] [Batch 684/938] [D loss: 0.6336753368377686] [G loss: 1.0561856031417847]\n",
      "[Epoch 4/5] [Batch 685/938] [D loss: 0.6384356021881104] [G loss: 0.775351345539093]\n",
      "[Epoch 4/5] [Batch 686/938] [D loss: 0.628105878829956] [G loss: 0.8210539817810059]\n",
      "[Epoch 4/5] [Batch 687/938] [D loss: 0.6272855401039124] [G loss: 1.0139788389205933]\n",
      "[Epoch 4/5] [Batch 688/938] [D loss: 0.6263925433158875] [G loss: 0.7749274969100952]\n",
      "[Epoch 4/5] [Batch 689/938] [D loss: 0.5912392735481262] [G loss: 0.8985090851783752]\n",
      "[Epoch 4/5] [Batch 690/938] [D loss: 0.6355233192443848] [G loss: 0.8768692016601562]\n",
      "[Epoch 4/5] [Batch 691/938] [D loss: 0.6294460296630859] [G loss: 1.0083038806915283]\n",
      "[Epoch 4/5] [Batch 692/938] [D loss: 0.6083000898361206] [G loss: 0.6791307926177979]\n",
      "[Epoch 4/5] [Batch 693/938] [D loss: 0.65528404712677] [G loss: 1.1957314014434814]\n",
      "[Epoch 4/5] [Batch 694/938] [D loss: 0.5600032210350037] [G loss: 0.8988515734672546]\n",
      "[Epoch 4/5] [Batch 695/938] [D loss: 0.5978617072105408] [G loss: 0.902630090713501]\n",
      "[Epoch 4/5] [Batch 696/938] [D loss: 0.6495904922485352] [G loss: 0.7685840129852295]\n",
      "[Epoch 4/5] [Batch 697/938] [D loss: 0.6607764363288879] [G loss: 0.8943026065826416]\n",
      "[Epoch 4/5] [Batch 698/938] [D loss: 0.6408015489578247] [G loss: 0.8861837387084961]\n",
      "[Epoch 4/5] [Batch 699/938] [D loss: 0.6278458833694458] [G loss: 0.8966774940490723]\n",
      "[Epoch 4/5] [Batch 700/938] [D loss: 0.6545926332473755] [G loss: 0.8185282945632935]\n",
      "[Epoch 4/5] [Batch 701/938] [D loss: 0.6059330701828003] [G loss: 0.9975094199180603]\n",
      "[Epoch 4/5] [Batch 702/938] [D loss: 0.6265014410018921] [G loss: 0.8855417370796204]\n",
      "[Epoch 4/5] [Batch 703/938] [D loss: 0.6082596182823181] [G loss: 0.8158086538314819]\n",
      "[Epoch 4/5] [Batch 704/938] [D loss: 0.625159740447998] [G loss: 0.955903947353363]\n",
      "[Epoch 4/5] [Batch 705/938] [D loss: 0.6305203437805176] [G loss: 0.9094825387001038]\n",
      "[Epoch 4/5] [Batch 706/938] [D loss: 0.6860328912734985] [G loss: 0.7663676738739014]\n",
      "[Epoch 4/5] [Batch 707/938] [D loss: 0.5930888652801514] [G loss: 0.9230865240097046]\n",
      "[Epoch 4/5] [Batch 708/938] [D loss: 0.6067813634872437] [G loss: 0.8128553628921509]\n",
      "[Epoch 4/5] [Batch 709/938] [D loss: 0.6189231276512146] [G loss: 0.9638625979423523]\n",
      "[Epoch 4/5] [Batch 710/938] [D loss: 0.6579041481018066] [G loss: 0.8146371245384216]\n",
      "[Epoch 4/5] [Batch 711/938] [D loss: 0.6159546375274658] [G loss: 0.9901158809661865]\n",
      "[Epoch 4/5] [Batch 712/938] [D loss: 0.603249192237854] [G loss: 0.8412538766860962]\n",
      "[Epoch 4/5] [Batch 713/938] [D loss: 0.6519802808761597] [G loss: 0.8693506121635437]\n",
      "[Epoch 4/5] [Batch 714/938] [D loss: 0.6262742877006531] [G loss: 0.9534431099891663]\n",
      "[Epoch 4/5] [Batch 715/938] [D loss: 0.6386008858680725] [G loss: 0.8224721550941467]\n",
      "[Epoch 4/5] [Batch 716/938] [D loss: 0.6459962129592896] [G loss: 0.9892085790634155]\n",
      "[Epoch 4/5] [Batch 717/938] [D loss: 0.6499254703521729] [G loss: 0.7839140295982361]\n",
      "[Epoch 4/5] [Batch 718/938] [D loss: 0.6479941606521606] [G loss: 1.0355329513549805]\n",
      "[Epoch 4/5] [Batch 719/938] [D loss: 0.6412708759307861] [G loss: 0.764693021774292]\n",
      "[Epoch 4/5] [Batch 720/938] [D loss: 0.6452051997184753] [G loss: 0.8271249532699585]\n",
      "[Epoch 4/5] [Batch 721/938] [D loss: 0.5990502238273621] [G loss: 0.7414724826812744]\n",
      "[Epoch 4/5] [Batch 722/938] [D loss: 0.6395351886749268] [G loss: 0.8793500065803528]\n",
      "[Epoch 4/5] [Batch 723/938] [D loss: 0.5699347853660583] [G loss: 0.8778043389320374]\n",
      "[Epoch 4/5] [Batch 724/938] [D loss: 0.6285110712051392] [G loss: 0.8173898458480835]\n",
      "[Epoch 4/5] [Batch 725/938] [D loss: 0.6046321392059326] [G loss: 0.9781933426856995]\n",
      "[Epoch 4/5] [Batch 726/938] [D loss: 0.6407560110092163] [G loss: 0.8928202390670776]\n",
      "[Epoch 4/5] [Batch 727/938] [D loss: 0.6398999691009521] [G loss: 0.8089767098426819]\n",
      "[Epoch 4/5] [Batch 728/938] [D loss: 0.6563236117362976] [G loss: 1.0460569858551025]\n",
      "[Epoch 4/5] [Batch 729/938] [D loss: 0.5977673530578613] [G loss: 0.708734929561615]\n",
      "[Epoch 4/5] [Batch 730/938] [D loss: 0.632758378982544] [G loss: 0.7909318208694458]\n",
      "[Epoch 4/5] [Batch 731/938] [D loss: 0.6401736736297607] [G loss: 0.997390627861023]\n",
      "[Epoch 4/5] [Batch 732/938] [D loss: 0.69171142578125] [G loss: 0.8628897666931152]\n",
      "[Epoch 4/5] [Batch 733/938] [D loss: 0.6991162300109863] [G loss: 0.5925195813179016]\n",
      "[Epoch 4/5] [Batch 734/938] [D loss: 0.6238465309143066] [G loss: 0.8918187618255615]\n",
      "[Epoch 4/5] [Batch 735/938] [D loss: 0.6907564997673035] [G loss: 1.1015328168869019]\n",
      "[Epoch 4/5] [Batch 736/938] [D loss: 0.6113798022270203] [G loss: 0.7408096194267273]\n",
      "[Epoch 4/5] [Batch 737/938] [D loss: 0.6580159664154053] [G loss: 0.6988357305526733]\n",
      "[Epoch 4/5] [Batch 738/938] [D loss: 0.6379889249801636] [G loss: 0.8485638499259949]\n",
      "[Epoch 4/5] [Batch 739/938] [D loss: 0.6488335132598877] [G loss: 0.8604111671447754]\n",
      "[Epoch 4/5] [Batch 740/938] [D loss: 0.6465710401535034] [G loss: 0.818523645401001]\n",
      "[Epoch 4/5] [Batch 741/938] [D loss: 0.6351639032363892] [G loss: 0.8444077968597412]\n",
      "[Epoch 4/5] [Batch 742/938] [D loss: 0.6097738742828369] [G loss: 0.853173017501831]\n",
      "[Epoch 4/5] [Batch 743/938] [D loss: 0.682660698890686] [G loss: 0.7913311719894409]\n",
      "[Epoch 4/5] [Batch 744/938] [D loss: 0.6388348340988159] [G loss: 0.7885976433753967]\n",
      "[Epoch 4/5] [Batch 745/938] [D loss: 0.6588395833969116] [G loss: 0.982561469078064]\n",
      "[Epoch 4/5] [Batch 746/938] [D loss: 0.6589032411575317] [G loss: 0.7392441630363464]\n",
      "[Epoch 4/5] [Batch 747/938] [D loss: 0.6411606669425964] [G loss: 0.7967038750648499]\n",
      "[Epoch 4/5] [Batch 748/938] [D loss: 0.6100377440452576] [G loss: 0.9257675409317017]\n",
      "[Epoch 4/5] [Batch 749/938] [D loss: 0.644827127456665] [G loss: 0.7815192937850952]\n",
      "[Epoch 4/5] [Batch 750/938] [D loss: 0.6320416927337646] [G loss: 0.9107600450515747]\n",
      "[Epoch 4/5] [Batch 751/938] [D loss: 0.6287705898284912] [G loss: 0.7484148740768433]\n",
      "[Epoch 4/5] [Batch 752/938] [D loss: 0.6393617987632751] [G loss: 0.9040548205375671]\n",
      "[Epoch 4/5] [Batch 753/938] [D loss: 0.6400586366653442] [G loss: 0.9883367419242859]\n",
      "[Epoch 4/5] [Batch 754/938] [D loss: 0.6306794881820679] [G loss: 0.7583868503570557]\n",
      "[Epoch 4/5] [Batch 755/938] [D loss: 0.6438186764717102] [G loss: 0.9727423787117004]\n",
      "[Epoch 4/5] [Batch 756/938] [D loss: 0.6442614793777466] [G loss: 0.7782281041145325]\n",
      "[Epoch 4/5] [Batch 757/938] [D loss: 0.6437771916389465] [G loss: 0.8098549246788025]\n",
      "[Epoch 4/5] [Batch 758/938] [D loss: 0.6040542721748352] [G loss: 0.8359360694885254]\n",
      "[Epoch 4/5] [Batch 759/938] [D loss: 0.600976824760437] [G loss: 1.038157343864441]\n",
      "[Epoch 4/5] [Batch 760/938] [D loss: 0.5980148315429688] [G loss: 0.8433060646057129]\n",
      "[Epoch 4/5] [Batch 761/938] [D loss: 0.6434844732284546] [G loss: 0.9719191789627075]\n",
      "[Epoch 4/5] [Batch 762/938] [D loss: 0.6757858991622925] [G loss: 0.684729278087616]\n",
      "[Epoch 4/5] [Batch 763/938] [D loss: 0.6087184548377991] [G loss: 0.9825652241706848]\n",
      "[Epoch 4/5] [Batch 764/938] [D loss: 0.6462990045547485] [G loss: 0.7974228262901306]\n",
      "[Epoch 4/5] [Batch 765/938] [D loss: 0.618384599685669] [G loss: 0.7501111030578613]\n",
      "[Epoch 4/5] [Batch 766/938] [D loss: 0.6040275692939758] [G loss: 0.8124508261680603]\n",
      "[Epoch 4/5] [Batch 767/938] [D loss: 0.6754735708236694] [G loss: 0.7936319708824158]\n",
      "[Epoch 4/5] [Batch 768/938] [D loss: 0.5999823808670044] [G loss: 0.9732645750045776]\n",
      "[Epoch 4/5] [Batch 769/938] [D loss: 0.6008343696594238] [G loss: 0.8002896308898926]\n",
      "[Epoch 4/5] [Batch 770/938] [D loss: 0.6769183874130249] [G loss: 0.7075318694114685]\n",
      "[Epoch 4/5] [Batch 771/938] [D loss: 0.6143929958343506] [G loss: 0.8398187160491943]\n",
      "[Epoch 4/5] [Batch 772/938] [D loss: 0.6312934756278992] [G loss: 1.017888069152832]\n",
      "[Epoch 4/5] [Batch 773/938] [D loss: 0.5893608927726746] [G loss: 0.7935298681259155]\n",
      "[Epoch 4/5] [Batch 774/938] [D loss: 0.6077882647514343] [G loss: 0.9387868046760559]\n",
      "[Epoch 4/5] [Batch 775/938] [D loss: 0.6485269665718079] [G loss: 1.076249361038208]\n",
      "[Epoch 4/5] [Batch 776/938] [D loss: 0.6763935089111328] [G loss: 0.6859342455863953]\n",
      "[Epoch 4/5] [Batch 777/938] [D loss: 0.5972005724906921] [G loss: 0.8530485033988953]\n",
      "[Epoch 4/5] [Batch 778/938] [D loss: 0.6122431755065918] [G loss: 1.0808844566345215]\n",
      "[Epoch 4/5] [Batch 779/938] [D loss: 0.5648475885391235] [G loss: 0.9406299591064453]\n",
      "[Epoch 4/5] [Batch 780/938] [D loss: 0.6165147423744202] [G loss: 0.7338055372238159]\n",
      "[Epoch 4/5] [Batch 781/938] [D loss: 0.6780940294265747] [G loss: 0.9730217456817627]\n",
      "[Epoch 4/5] [Batch 782/938] [D loss: 0.6154007911682129] [G loss: 0.9755300283432007]\n",
      "[Epoch 4/5] [Batch 783/938] [D loss: 0.5674226880073547] [G loss: 0.7923113107681274]\n",
      "[Epoch 4/5] [Batch 784/938] [D loss: 0.6276076436042786] [G loss: 0.9236075282096863]\n",
      "[Epoch 4/5] [Batch 785/938] [D loss: 0.6039234399795532] [G loss: 0.8681296110153198]\n",
      "[Epoch 4/5] [Batch 786/938] [D loss: 0.636817455291748] [G loss: 1.0841271877288818]\n",
      "[Epoch 4/5] [Batch 787/938] [D loss: 0.6410180330276489] [G loss: 0.7379809617996216]\n",
      "[Epoch 4/5] [Batch 788/938] [D loss: 0.6537978649139404] [G loss: 0.8783811330795288]\n",
      "[Epoch 4/5] [Batch 789/938] [D loss: 0.6232276558876038] [G loss: 0.9539828896522522]\n",
      "[Epoch 4/5] [Batch 790/938] [D loss: 0.5796067118644714] [G loss: 0.8171036243438721]\n",
      "[Epoch 4/5] [Batch 791/938] [D loss: 0.6989811658859253] [G loss: 0.7952912449836731]\n",
      "[Epoch 4/5] [Batch 792/938] [D loss: 0.6093580722808838] [G loss: 0.8798829317092896]\n",
      "[Epoch 4/5] [Batch 793/938] [D loss: 0.5661344528198242] [G loss: 0.9837521910667419]\n",
      "[Epoch 4/5] [Batch 794/938] [D loss: 0.6431565284729004] [G loss: 0.8741780519485474]\n",
      "[Epoch 4/5] [Batch 795/938] [D loss: 0.6003282070159912] [G loss: 0.8772686123847961]\n",
      "[Epoch 4/5] [Batch 796/938] [D loss: 0.6355530023574829] [G loss: 0.9254489541053772]\n",
      "[Epoch 4/5] [Batch 797/938] [D loss: 0.6322386860847473] [G loss: 0.8735959529876709]\n",
      "[Epoch 4/5] [Batch 798/938] [D loss: 0.6819283962249756] [G loss: 0.975150465965271]\n",
      "[Epoch 4/5] [Batch 799/938] [D loss: 0.5870675444602966] [G loss: 0.9035670161247253]\n",
      "[Epoch 4/5] [Batch 800/938] [D loss: 0.6688957810401917] [G loss: 0.6017258763313293]\n",
      "[Epoch 4/5] [Batch 801/938] [D loss: 0.6614710688591003] [G loss: 1.116402506828308]\n",
      "[Epoch 4/5] [Batch 802/938] [D loss: 0.6088600158691406] [G loss: 0.8903228640556335]\n",
      "[Epoch 4/5] [Batch 803/938] [D loss: 0.5964821577072144] [G loss: 0.7633427381515503]\n",
      "[Epoch 4/5] [Batch 804/938] [D loss: 0.612094521522522] [G loss: 0.7964919209480286]\n",
      "[Epoch 4/5] [Batch 805/938] [D loss: 0.5606750249862671] [G loss: 0.7912111282348633]\n",
      "[Epoch 4/5] [Batch 806/938] [D loss: 0.558804988861084] [G loss: 0.9514912366867065]\n",
      "[Epoch 4/5] [Batch 807/938] [D loss: 0.5844215154647827] [G loss: 0.8902852535247803]\n",
      "[Epoch 4/5] [Batch 808/938] [D loss: 0.6533949375152588] [G loss: 1.019059419631958]\n",
      "[Epoch 4/5] [Batch 809/938] [D loss: 0.6302181482315063] [G loss: 0.7771568298339844]\n",
      "[Epoch 4/5] [Batch 810/938] [D loss: 0.6584306955337524] [G loss: 1.1161084175109863]\n",
      "[Epoch 4/5] [Batch 811/938] [D loss: 0.6545141339302063] [G loss: 0.6750813126564026]\n",
      "[Epoch 4/5] [Batch 812/938] [D loss: 0.6069821715354919] [G loss: 0.9124839305877686]\n",
      "[Epoch 4/5] [Batch 813/938] [D loss: 0.6544250249862671] [G loss: 0.9294738173484802]\n",
      "[Epoch 4/5] [Batch 814/938] [D loss: 0.5995355248451233] [G loss: 0.777921199798584]\n",
      "[Epoch 4/5] [Batch 815/938] [D loss: 0.6531882286071777] [G loss: 0.8315861225128174]\n",
      "[Epoch 4/5] [Batch 816/938] [D loss: 0.6115778684616089] [G loss: 0.8550260663032532]\n",
      "[Epoch 4/5] [Batch 817/938] [D loss: 0.6106071472167969] [G loss: 0.869716465473175]\n",
      "[Epoch 4/5] [Batch 818/938] [D loss: 0.5735578536987305] [G loss: 0.8486300110816956]\n",
      "[Epoch 4/5] [Batch 819/938] [D loss: 0.5642009973526001] [G loss: 0.9118622541427612]\n",
      "[Epoch 4/5] [Batch 820/938] [D loss: 0.5934371948242188] [G loss: 0.9462151527404785]\n",
      "[Epoch 4/5] [Batch 821/938] [D loss: 0.585387110710144] [G loss: 0.749583899974823]\n",
      "[Epoch 4/5] [Batch 822/938] [D loss: 0.5615415573120117] [G loss: 0.9690335988998413]\n",
      "[Epoch 4/5] [Batch 823/938] [D loss: 0.6493107080459595] [G loss: 1.002600073814392]\n",
      "[Epoch 4/5] [Batch 824/938] [D loss: 0.6149493455886841] [G loss: 0.817908763885498]\n",
      "[Epoch 4/5] [Batch 825/938] [D loss: 0.5944907665252686] [G loss: 0.6927537322044373]\n",
      "[Epoch 4/5] [Batch 826/938] [D loss: 0.6032962799072266] [G loss: 1.141369342803955]\n",
      "[Epoch 4/5] [Batch 827/938] [D loss: 0.6253235936164856] [G loss: 0.7191276550292969]\n",
      "[Epoch 4/5] [Batch 828/938] [D loss: 0.6999914646148682] [G loss: 1.5096471309661865]\n",
      "[Epoch 4/5] [Batch 829/938] [D loss: 0.6450671553611755] [G loss: 0.6709034442901611]\n",
      "[Epoch 4/5] [Batch 830/938] [D loss: 0.609254002571106] [G loss: 0.755861759185791]\n",
      "[Epoch 4/5] [Batch 831/938] [D loss: 0.6462128162384033] [G loss: 0.9672011137008667]\n",
      "[Epoch 4/5] [Batch 832/938] [D loss: 0.6392925977706909] [G loss: 0.9641129374504089]\n",
      "[Epoch 4/5] [Batch 833/938] [D loss: 0.6341791749000549] [G loss: 0.7096697688102722]\n",
      "[Epoch 4/5] [Batch 834/938] [D loss: 0.6177761554718018] [G loss: 1.0803579092025757]\n",
      "[Epoch 4/5] [Batch 835/938] [D loss: 0.585248589515686] [G loss: 0.8434367179870605]\n",
      "[Epoch 4/5] [Batch 836/938] [D loss: 0.633928120136261] [G loss: 0.7983425855636597]\n",
      "[Epoch 4/5] [Batch 837/938] [D loss: 0.6031532287597656] [G loss: 0.8216534852981567]\n",
      "[Epoch 4/5] [Batch 838/938] [D loss: 0.6295502185821533] [G loss: 0.9397234320640564]\n",
      "[Epoch 4/5] [Batch 839/938] [D loss: 0.6144213676452637] [G loss: 0.8742300868034363]\n",
      "[Epoch 4/5] [Batch 840/938] [D loss: 0.6337412595748901] [G loss: 0.935293436050415]\n",
      "[Epoch 4/5] [Batch 841/938] [D loss: 0.5967491269111633] [G loss: 0.9186974763870239]\n",
      "[Epoch 4/5] [Batch 842/938] [D loss: 0.635725736618042] [G loss: 0.6488735675811768]\n",
      "[Epoch 4/5] [Batch 843/938] [D loss: 0.5824815034866333] [G loss: 0.8830152750015259]\n",
      "[Epoch 4/5] [Batch 844/938] [D loss: 0.6141349077224731] [G loss: 0.9871063232421875]\n",
      "[Epoch 4/5] [Batch 845/938] [D loss: 0.606410801410675] [G loss: 0.8296791911125183]\n",
      "[Epoch 4/5] [Batch 846/938] [D loss: 0.6400564908981323] [G loss: 0.8485714793205261]\n",
      "[Epoch 4/5] [Batch 847/938] [D loss: 0.6429394483566284] [G loss: 1.0596709251403809]\n",
      "[Epoch 4/5] [Batch 848/938] [D loss: 0.6404749155044556] [G loss: 0.774352490901947]\n",
      "[Epoch 4/5] [Batch 849/938] [D loss: 0.681257426738739] [G loss: 0.9489759206771851]\n",
      "[Epoch 4/5] [Batch 850/938] [D loss: 0.6353583335876465] [G loss: 0.8409723043441772]\n",
      "[Epoch 4/5] [Batch 851/938] [D loss: 0.6362294554710388] [G loss: 0.9452556371688843]\n",
      "[Epoch 4/5] [Batch 852/938] [D loss: 0.657665491104126] [G loss: 0.9026874899864197]\n",
      "[Epoch 4/5] [Batch 853/938] [D loss: 0.6154779195785522] [G loss: 0.936779260635376]\n",
      "[Epoch 4/5] [Batch 854/938] [D loss: 0.6250524520874023] [G loss: 0.8425325751304626]\n",
      "[Epoch 4/5] [Batch 855/938] [D loss: 0.6778411865234375] [G loss: 0.938828706741333]\n",
      "[Epoch 4/5] [Batch 856/938] [D loss: 0.5964266061782837] [G loss: 0.83351069688797]\n",
      "[Epoch 4/5] [Batch 857/938] [D loss: 0.653336763381958] [G loss: 0.9015262722969055]\n",
      "[Epoch 4/5] [Batch 858/938] [D loss: 0.6223715543746948] [G loss: 0.8036456108093262]\n",
      "[Epoch 4/5] [Batch 859/938] [D loss: 0.6131296157836914] [G loss: 0.905189573764801]\n",
      "[Epoch 4/5] [Batch 860/938] [D loss: 0.6499791145324707] [G loss: 0.8365291357040405]\n",
      "[Epoch 4/5] [Batch 861/938] [D loss: 0.5955030918121338] [G loss: 0.9587167501449585]\n",
      "[Epoch 4/5] [Batch 862/938] [D loss: 0.6207177639007568] [G loss: 0.8744108080863953]\n",
      "[Epoch 4/5] [Batch 863/938] [D loss: 0.6203848123550415] [G loss: 0.786173939704895]\n",
      "[Epoch 4/5] [Batch 864/938] [D loss: 0.6272857189178467] [G loss: 0.9519383311271667]\n",
      "[Epoch 4/5] [Batch 865/938] [D loss: 0.6325215101242065] [G loss: 0.8135702013969421]\n",
      "[Epoch 4/5] [Batch 866/938] [D loss: 0.6259962916374207] [G loss: 0.9439861178398132]\n",
      "[Epoch 4/5] [Batch 867/938] [D loss: 0.5741186141967773] [G loss: 1.0313665866851807]\n",
      "[Epoch 4/5] [Batch 868/938] [D loss: 0.6238290071487427] [G loss: 0.737458348274231]\n",
      "[Epoch 4/5] [Batch 869/938] [D loss: 0.5974831581115723] [G loss: 0.8991221189498901]\n",
      "[Epoch 4/5] [Batch 870/938] [D loss: 0.6188378930091858] [G loss: 0.7870709300041199]\n",
      "[Epoch 4/5] [Batch 871/938] [D loss: 0.6157585978507996] [G loss: 1.024107575416565]\n",
      "[Epoch 4/5] [Batch 872/938] [D loss: 0.6750205159187317] [G loss: 0.9658856987953186]\n",
      "[Epoch 4/5] [Batch 873/938] [D loss: 0.6223743557929993] [G loss: 0.9327682256698608]\n",
      "[Epoch 4/5] [Batch 874/938] [D loss: 0.6369791030883789] [G loss: 0.8299779295921326]\n",
      "[Epoch 4/5] [Batch 875/938] [D loss: 0.6470599174499512] [G loss: 1.1178981065750122]\n",
      "[Epoch 4/5] [Batch 876/938] [D loss: 0.6654627323150635] [G loss: 0.6692430973052979]\n",
      "[Epoch 4/5] [Batch 877/938] [D loss: 0.7317766547203064] [G loss: 1.2212748527526855]\n",
      "[Epoch 4/5] [Batch 878/938] [D loss: 0.6426615715026855] [G loss: 0.6324992179870605]\n",
      "[Epoch 4/5] [Batch 879/938] [D loss: 0.6021953225135803] [G loss: 0.930171549320221]\n",
      "[Epoch 4/5] [Batch 880/938] [D loss: 0.6427836418151855] [G loss: 0.9627984762191772]\n",
      "[Epoch 4/5] [Batch 881/938] [D loss: 0.6039652824401855] [G loss: 0.8926223516464233]\n",
      "[Epoch 4/5] [Batch 882/938] [D loss: 0.6567413806915283] [G loss: 0.7907503843307495]\n",
      "[Epoch 4/5] [Batch 883/938] [D loss: 0.6620182991027832] [G loss: 0.9996767044067383]\n",
      "[Epoch 4/5] [Batch 884/938] [D loss: 0.6581497192382812] [G loss: 0.7084697484970093]\n",
      "[Epoch 4/5] [Batch 885/938] [D loss: 0.6551392078399658] [G loss: 0.6627503633499146]\n",
      "[Epoch 4/5] [Batch 886/938] [D loss: 0.6201475858688354] [G loss: 0.9587075710296631]\n",
      "[Epoch 4/5] [Batch 887/938] [D loss: 0.7128965854644775] [G loss: 0.969333827495575]\n",
      "[Epoch 4/5] [Batch 888/938] [D loss: 0.6823054552078247] [G loss: 0.7140907645225525]\n",
      "[Epoch 4/5] [Batch 889/938] [D loss: 0.6382508277893066] [G loss: 0.7902094125747681]\n",
      "[Epoch 4/5] [Batch 890/938] [D loss: 0.6100360751152039] [G loss: 0.9340678453445435]\n",
      "[Epoch 4/5] [Batch 891/938] [D loss: 0.6656900644302368] [G loss: 0.8200854063034058]\n",
      "[Epoch 4/5] [Batch 892/938] [D loss: 0.5903271436691284] [G loss: 0.8936281800270081]\n",
      "[Epoch 4/5] [Batch 893/938] [D loss: 0.5940544009208679] [G loss: 0.7661986351013184]\n",
      "[Epoch 4/5] [Batch 894/938] [D loss: 0.6303304433822632] [G loss: 0.795056939125061]\n",
      "[Epoch 4/5] [Batch 895/938] [D loss: 0.6858795881271362] [G loss: 1.0320773124694824]\n",
      "[Epoch 4/5] [Batch 896/938] [D loss: 0.6281114220619202] [G loss: 0.8402553796768188]\n",
      "[Epoch 4/5] [Batch 897/938] [D loss: 0.5914997458457947] [G loss: 0.7368425726890564]\n",
      "[Epoch 4/5] [Batch 898/938] [D loss: 0.623080313205719] [G loss: 1.0008012056350708]\n",
      "[Epoch 4/5] [Batch 899/938] [D loss: 0.5867953300476074] [G loss: 0.9030423760414124]\n",
      "[Epoch 4/5] [Batch 900/938] [D loss: 0.6331744194030762] [G loss: 0.8136427998542786]\n",
      "[Epoch 4/5] [Batch 901/938] [D loss: 0.6254584193229675] [G loss: 0.9673076868057251]\n",
      "[Epoch 4/5] [Batch 902/938] [D loss: 0.6007429361343384] [G loss: 0.9343660473823547]\n",
      "[Epoch 4/5] [Batch 903/938] [D loss: 0.6350557804107666] [G loss: 1.0282548666000366]\n",
      "[Epoch 4/5] [Batch 904/938] [D loss: 0.6490814089775085] [G loss: 0.8786544799804688]\n",
      "[Epoch 4/5] [Batch 905/938] [D loss: 0.6382512450218201] [G loss: 0.9637169241905212]\n",
      "[Epoch 4/5] [Batch 906/938] [D loss: 0.6362521648406982] [G loss: 0.7483837008476257]\n",
      "[Epoch 4/5] [Batch 907/938] [D loss: 0.6461705565452576] [G loss: 1.0240780115127563]\n",
      "[Epoch 4/5] [Batch 908/938] [D loss: 0.6108781099319458] [G loss: 0.8827511072158813]\n",
      "[Epoch 4/5] [Batch 909/938] [D loss: 0.6791051626205444] [G loss: 0.806792676448822]\n",
      "[Epoch 4/5] [Batch 910/938] [D loss: 0.6314899921417236] [G loss: 0.8640698194503784]\n",
      "[Epoch 4/5] [Batch 911/938] [D loss: 0.6156401634216309] [G loss: 1.0020431280136108]\n",
      "[Epoch 4/5] [Batch 912/938] [D loss: 0.6304123401641846] [G loss: 0.904553234577179]\n",
      "[Epoch 4/5] [Batch 913/938] [D loss: 0.6222743391990662] [G loss: 0.7012880444526672]\n",
      "[Epoch 4/5] [Batch 914/938] [D loss: 0.5962960124015808] [G loss: 0.9735709428787231]\n",
      "[Epoch 4/5] [Batch 915/938] [D loss: 0.6525897979736328] [G loss: 0.9083801507949829]\n",
      "[Epoch 4/5] [Batch 916/938] [D loss: 0.6547290086746216] [G loss: 0.8437697291374207]\n",
      "[Epoch 4/5] [Batch 917/938] [D loss: 0.6406034231185913] [G loss: 0.9406059980392456]\n",
      "[Epoch 4/5] [Batch 918/938] [D loss: 0.6183456778526306] [G loss: 0.9708051681518555]\n",
      "[Epoch 4/5] [Batch 919/938] [D loss: 0.6453618407249451] [G loss: 0.7595874071121216]\n",
      "[Epoch 4/5] [Batch 920/938] [D loss: 0.6558560132980347] [G loss: 0.8121427893638611]\n",
      "[Epoch 4/5] [Batch 921/938] [D loss: 0.6223320364952087] [G loss: 1.1301040649414062]\n",
      "[Epoch 4/5] [Batch 922/938] [D loss: 0.5640458464622498] [G loss: 0.7629733681678772]\n",
      "[Epoch 4/5] [Batch 923/938] [D loss: 0.6327676773071289] [G loss: 0.8524156808853149]\n",
      "[Epoch 4/5] [Batch 924/938] [D loss: 0.6120154857635498] [G loss: 0.7749829888343811]\n",
      "[Epoch 4/5] [Batch 925/938] [D loss: 0.6049231886863708] [G loss: 0.9568367004394531]\n",
      "[Epoch 4/5] [Batch 926/938] [D loss: 0.6324563026428223] [G loss: 0.9055830240249634]\n",
      "[Epoch 4/5] [Batch 927/938] [D loss: 0.6624705791473389] [G loss: 0.754538357257843]\n",
      "[Epoch 4/5] [Batch 928/938] [D loss: 0.6395244002342224] [G loss: 1.0972363948822021]\n",
      "[Epoch 4/5] [Batch 929/938] [D loss: 0.6596962213516235] [G loss: 0.7521721124649048]\n",
      "[Epoch 4/5] [Batch 930/938] [D loss: 0.5898799896240234] [G loss: 0.7626122236251831]\n",
      "[Epoch 4/5] [Batch 931/938] [D loss: 0.6091666221618652] [G loss: 0.856363832950592]\n",
      "[Epoch 4/5] [Batch 932/938] [D loss: 0.58523029088974] [G loss: 0.8795275688171387]\n",
      "[Epoch 4/5] [Batch 933/938] [D loss: 0.6832544803619385] [G loss: 0.8341132402420044]\n",
      "[Epoch 4/5] [Batch 934/938] [D loss: 0.6557636857032776] [G loss: 0.8897374868392944]\n",
      "[Epoch 4/5] [Batch 935/938] [D loss: 0.6745777726173401] [G loss: 0.8502663969993591]\n",
      "[Epoch 4/5] [Batch 936/938] [D loss: 0.6045455932617188] [G loss: 0.8903087377548218]\n",
      "[Epoch 4/5] [Batch 937/938] [D loss: 0.5692887306213379] [G loss: 1.1456588506698608]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32768x512 and 784x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Measure discriminator's ability to classify real from generated samples\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m criterion(\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m)\u001b[49m, valid)\n\u001b[1;32m    152\u001b[0m fake_loss \u001b[38;5;241m=\u001b[39m criterion(discriminator(gen_imgs\u001b[38;5;241m.\u001b[39mdetach()), fake)\n\u001b[1;32m    153\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (real_loss \u001b[38;5;241m+\u001b[39m fake_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 46\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32768x512 and 784x1024)"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "num_epochs = 5\n",
    "latent_dim = 100\n",
    "img_shape = (1, 28, 28)\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    ")\n",
    "\n",
    "mnist = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(mnist, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers (generator와 discriminator는 이전에 정의되어 있어야 합니다)\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((imgs.size(0), 1), requires_grad=False).to(device)\n",
    "        fake = torch.zeros((imgs.size(0), 1), requires_grad=False).to(device)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.to(device)\n",
    "        real_imgs = real_imgs.view(real_imgs.size(0), -1)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        gen_imgs = gen_imgs.view(gen_imgs.size(0), -1)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn((imgs.size(0), latent_dim)).to(device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_dir, gt_dir, transform=None):\n",
    "        self.input_dir = input_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.input_images = sorted(os.listdir(input_dir))\n",
    "        self.gt_images = sorted(os.listdir(gt_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = Image.open(\n",
    "            os.path.join(self.input_dir, self.input_images[idx])\n",
    "        ).convert(\"L\")\n",
    "        gt_image = Image.open(os.path.join(self.gt_dir, self.gt_images[idx])).convert(\n",
    "            \"L\"\n",
    "        )\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            gt_image = self.transform(gt_image)\n",
    "\n",
    "        return input_image, gt_image\n",
    "\n",
    "\n",
    "# Directories\n",
    "input_dir = \"./train_input\"\n",
    "gt_dir = \"./train_gt\"\n",
    "\n",
    "# Data loading\n",
    "custom_dataset = CustomDataset(input_dir, gt_dir, transform=transform)\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop for custom dataset\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_imgs, gt_imgs) in enumerate(custom_dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((input_imgs.size(0), 1), requires_grad=False).to(device)\n",
    "        fake = torch.zeros((input_imgs.size(0), 1), requires_grad=False).to(device)\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn((input_imgs.size(0), latent_dim)).to(device)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = gt_imgs.to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(custom_dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# 잠재 공간에서 무작위 노이즈 생성\n",
    "z = torch.randn(64, latent_dim).to(device)\n",
    "\n",
    "# 생성자를 사용하여 이미지 생성\n",
    "generator.eval()  # 평가 모드로 전환\n",
    "with torch.no_grad():\n",
    "    gen_imgs = generator(z)\n",
    "\n",
    "# 이미지 형태 조정 및 정규화 해제\n",
    "gen_imgs = gen_imgs.view(gen_imgs.size(0), *img_shape)\n",
    "gen_imgs = gen_imgs * 0.5 + 0.5  # [-1,1] 범위를 [0,1]로 변환\n",
    "\n",
    "# 이미지 시각화\n",
    "grid = vutils.make_grid(gen_imgs.cpu(), nrow=8, normalize=True)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
