# -*- coding: utf-8 -*-
"""MAT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lc-BFuZl8Rj86M8LCIg9tdHNSCltP6L3
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip -qq /content/drive/MyDrive/MLProject/train_dataset.zip -d /content/dataset/

import os
print(len(os.listdir('/content/drive/MyDrive/MLProject/MAT/masks')))
print(len(os.listdir('/content/dataset/train_dataset/train_input')))

"""## MASK 생성"""

import os
import numpy as np
from PIL import Image
from glob import glob

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms


class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1):
        super(UNet, self).__init__()

        def CBR(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            )

        # 인코더
        self.enc1 = CBR(in_channels, 64)
        self.pool1 = nn.MaxPool2d(2)

        self.enc2 = CBR(64, 128)
        self.pool2 = nn.MaxPool2d(2)

        self.enc3 = CBR(128, 256)
        self.pool3 = nn.MaxPool2d(2)

        self.enc4 = CBR(256, 512)
        self.pool4 = nn.MaxPool2d(2)

        # 보틀넥
        self.bottleneck = CBR(512, 1024)

        # 디코더
        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.dec4 = CBR(1024, 512)

        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec3 = CBR(512, 256)

        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec2 = CBR(256, 128)

        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec1 = CBR(128, 64)

        # 출력 레이어
        self.conv_last = nn.Conv2d(64, out_channels, kernel_size=1)

    def forward(self, x):
        # 인코더
        enc1 = self.enc1(x)
        pool1 = self.pool1(enc1)

        enc2 = self.enc2(pool1)
        pool2 = self.pool2(enc2)

        enc3 = self.enc3(pool2)
        pool3 = self.pool3(enc3)

        enc4 = self.enc4(pool3)
        pool4 = self.pool4(enc4)

        # 보틀넥
        bottleneck = self.bottleneck(pool4)

        # 디코더
        up4 = self.upconv4(bottleneck)
        up4 = torch.cat([up4, enc4], dim=1)
        dec4 = self.dec4(up4)

        up3 = self.upconv3(dec4)
        up3 = torch.cat([up3, enc3], dim=1)
        dec3 = self.dec3(up3)

        up2 = self.upconv2(dec3)
        up2 = torch.cat([up2, enc2], dim=1)
        dec2 = self.dec2(up2)

        up1 = self.upconv1(dec2)
        up1 = torch.cat([up1, enc1], dim=1)
        dec1 = self.dec1(up1)

        # 출력
        out = self.conv_last(dec1)
        return out

# 7. 디바이스 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 모델 초기화
model = UNet(in_channels=1, out_channels=1)
model = model.to(device)

# 데이터 경로 설정
train_x_path = '/content/dataset/train_dataset/train_input'
label_y_path = '/content/dataset/train_dataset/train_gt'

# 데이터셋 클래스 정의
class SegmentationDataset(Dataset):
    def __init__(self, input_dir, gt_dir, transforms=None):
        self.input_dir = input_dir
        self.gt_dir = gt_dir
        self.transforms = transforms

        # 이미지 파일 목록 가져오기
        self.input_images = sorted(glob(os.path.join(self.input_dir, '*')))
        self.gt_images = sorted(glob(os.path.join(self.gt_dir, '*')))

    def __len__(self):
        return len(self.input_images)

    def __getitem__(self, idx):
        # 이미지 경로
        input_image_path = self.input_images[idx]
        gt_image_path = self.gt_images[idx]

        # 이미지 로드 및 흑백으로 변환
        input_image = Image.open(input_image_path).convert('L')
        gt_image = Image.open(gt_image_path).convert('L')  # 컬러 이미지를 흑백으로 변환

        # numpy 배열로 변환
        input_array = np.array(input_image)
        gt_array = np.array(gt_image)

        # 세그멘테이션 마스크 생성
        # 마스킹된 영역: 0, 나머지 영역: 1
        mask = (input_array == gt_array).astype(np.float32)

        # 변환 적용
        if self.transforms:
            input_image = self.transforms['input'](input_image)
            mask = self.transforms['mask'](mask)

        return input_image, mask

# 데이터 변환 설정
input_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

mask_transform = transforms.Compose([
    transforms.Lambda(lambda x: torch.from_numpy(x).unsqueeze(0))
])

# 데이터셋 및 데이터로더 생성
train_dataset = SegmentationDataset(
    input_dir=train_x_path,
    gt_dir=label_y_path,
    transforms={'input': input_transform, 'mask': mask_transform}
)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# U-Net 모델 정의 (이전과 동일)

# 디바이스 설정 및 모델 초기화 (이전과 동일)

# 손실 함수 및 옵티마이저 설정
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# 학습 루프 수정
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for batch_idx, (input_image, mask) in enumerate(train_loader):
        input_image = input_image.to(device)
        mask = mask.to(device)

        optimizer.zero_grad()

        # 모델 추론
        outputs = model(input_image)

        # 손실 함수 계산
        loss = criterion(outputs, mask)

        # 손실 값 및 마스크 값 확인
        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')

        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    epoch_loss = running_loss / len(train_loader)
    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_loss:.4f}')

    # 모델 저장 (선택 사항)
    torch.save(model.state_dict(), f'/content/drive/MyDrive/MLProject/MAT/maskGen/unet_epoch_{epoch+1}.pth')

import gc
gc.collect()
torch.cuda.empty_cache()

# 필요한 라이브러리 임포트
import os
import matplotlib.pyplot as plt
from PIL import Image

# 디바이스 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 모델 로드 (이미 학습된 모델이어야 함)
model = UNet(in_channels=1, out_channels=1)
model.load_state_dict(torch.load('/content/drive/MyDrive/MLProject/MAT/maskGen/unet_epoch_15.pth'))
model = model.to(device)

# 추론 함수 정의 (이전과 동일)
def inference(model, image_path, device):
    model.eval()
    with torch.no_grad():
        # 이미지 로드 및 전처리
        input_image = Image.open(image_path).convert('L')

        # 이미지 크기가 512x512로 고정되어 있으므로 변환 생략
        input_tensor = transforms.ToTensor()(input_image)
        input_tensor = transforms.Normalize(mean=[0.5], std=[0.5])(input_tensor)
        input_tensor = input_tensor.unsqueeze(0).to(device)

        # 모델 추론
        output = model(input_tensor)
        output = torch.sigmoid(output)
        output = output.squeeze().cpu().numpy()

        # 바이너리 마스크 생성
        threshold = 0.5
        binary_mask = (output > threshold).astype(np.uint8)

        # 마스크 이미지 생성
        mask_image = Image.fromarray(binary_mask * 255)

        return mask_image

# 필요한 라이브러리 임포트
import os
import matplotlib.pyplot as plt
from PIL import Image
# 디바이스 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 모델 로드 (이미 학습된 모델이어야 함)
model = UNet(in_channels=1, out_channels=1)
model.load_state_dict(torch.load('/content/drive/MyDrive/MLProject/MAT/maskGen/unet_epoch_15.pth'))
model = model.to(device)
def inferenceInverted(model, image_path, device):
    model.eval()
    with torch.no_grad():
        # 이미지 로드 및 전처리
        input_image = Image.open(image_path).convert('L')

        # 이미지 크기가 512x512로 고정되어 있으므로 변환 생략
        input_tensor = transforms.ToTensor()(input_image)
        input_tensor = transforms.Normalize(mean=[0.5], std=[0.5])(input_tensor)
        input_tensor = input_tensor.unsqueeze(0).to(device)

        # 모델 추론
        output = model(input_tensor)
        output = torch.sigmoid(output)
        output = output.squeeze().cpu().numpy()

        # 바이너리 마스크 생성 및 반전
        threshold = 0.5
        binary_mask = (output > threshold).astype(np.uint8)
        inverted_mask = 1 - binary_mask  # 마스크 반전

        # 마스크 이미지 생성
        mask_image = Image.fromarray(inverted_mask * 255)

        return mask_image

from tqdm import tqdm
# 마스크 이미지를 저장할 디렉토리 설정
output_dir = '/content/drive/MyDrive/MLProject/MAT/masks'

# 디렉토리가 존재하지 않으면 생성
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# 입력 이미지 디렉토리 설정
input_dir = '/content/dataset/train_dataset/train_input'
i=0
# 추론 및 결과 시각화 및 저장
for img_name in tqdm(os.listdir(input_dir)):
    #print(f"Processing: {img_name}")
    test_image_path = os.path.join(input_dir, img_name)
    predicted_mask = inferenceInverted(model, test_image_path, device)

    # 마스크 이미지 저장 경로 설정
    mask_save_path = os.path.join(output_dir, f"mask_{img_name}")

    # 마스크 이미지 저장
    predicted_mask.save(mask_save_path)

    # 원본 이미지 로드
    original_image = Image.open(test_image_path).convert('L')

    # 결과 시각화
    if i < 5:
      plt.figure(figsize=(10, 5))

      plt.subplot(1, 2, 1)
      plt.imshow(original_image, cmap='gray')
      plt.title('Input Image')
      plt.axis('off')

      plt.subplot(1, 2, 2)
      plt.imshow(predicted_mask, cmap='gray')
      plt.title('Predicted Mask')
      plt.axis('off')

      plt.show()
      i = i + 1

"""## MAT 복원"""

#!git  clone https://github.com/fenglinglwb/MAT.git /content/drive/MyDrive/MLProject/MAT

!pip install -r /content/drive/MyDrive/MLProject/MAT/requirements.txt

!python /content/drive/MyDrive/MLProject/MAT/generate_image.py --network /content/drive/MyDrive/MLProject/MAT/CelebA-HQ_512.pkl --dpath /content/dataset/train_dataset/train_input --outdir /content/drive/MyDrive/MLProject/MAT/sample_ouput --mpath /content/drive/MyDrive/MLProject/MAT/masks